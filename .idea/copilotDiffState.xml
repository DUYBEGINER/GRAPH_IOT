<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/CNN/cnn_cicids2018_full_pipeline.ipynb">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/CNN/cnn_cicids2018_full_pipeline.ipynb" />
              <option name="updatedContent" value="{&#10; &quot;cells&quot;: [&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;#  PHÁT HIỆN LƯU LƯỢNG MẠNG IOT BẤT THƯỜNG BẰNG CNN\n&quot;,&#10;    &quot;## Binary Classification: Benign vs Attack\n&quot;,&#10;    &quot;### Dataset: CSE-CIC-IDS2018\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;---\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;**Các bước thực hiện:**\n&quot;,&#10;    &quot;1. **Bước 1:** Clean dữ liệu (loại bỏ cột ID, xử lý NaN/Inf, loại duplicate)\n&quot;,&#10;    &quot;2. **Bước 2:** Cân bằng dữ liệu và chuẩn bị training (70% Benign, 30% Attack)\n&quot;,&#10;    &quot;3. **Bước 3:** Train mô hình CNN\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;**Kiến trúc CNN:**\n&quot;,&#10;    &quot;- 5 lớp Conv1D với MaxPooling\n&quot;,&#10;    &quot;- BatchNormalization + Dropout\n&quot;,&#10;    &quot;- Output: Dense(1, sigmoid) cho binary classification&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;##  IMPORT THƯ VIỆN&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;import os\n&quot;,&#10;    &quot;import numpy as np\n&quot;,&#10;    &quot;import pandas as pd\n&quot;,&#10;    &quot;import pickle\n&quot;,&#10;    &quot;import json\n&quot;,&#10;    &quot;import gc\n&quot;,&#10;    &quot;from pathlib import Path\n&quot;,&#10;    &quot;from datetime import datetime\n&quot;,&#10;    &quot;import warnings\n&quot;,&#10;    &quot;warnings.filterwarnings('ignore')\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Sklearn\n&quot;,&#10;    &quot;from sklearn.preprocessing import StandardScaler\n&quot;,&#10;    &quot;from sklearn.model_selection import train_test_split\n&quot;,&#10;    &quot;from sklearn.utils.class_weight import compute_class_weight\n&quot;,&#10;    &quot;from sklearn.metrics import confusion_matrix, classification_report\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# TensorFlow/Keras\n&quot;,&#10;    &quot;import tensorflow as tf\n&quot;,&#10;    &quot;from tensorflow import keras\n&quot;,&#10;    &quot;from tensorflow.keras.models import Sequential\n&quot;,&#10;    &quot;from tensorflow.keras.layers import (\n&quot;,&#10;    &quot;    Conv1D, MaxPooling1D, Flatten, Dense,\n&quot;,&#10;    &quot;    Dropout, BatchNormalization, Input\n&quot;,&#10;    &quot;)\n&quot;,&#10;    &quot;from tensorflow.keras.callbacks import (\n&quot;,&#10;    &quot;    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n&quot;,&#10;    &quot;)\n&quot;,&#10;    &quot;from tensorflow.keras.metrics import Precision, Recall\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Visualization\n&quot;,&#10;    &quot;import matplotlib.pyplot as plt\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Progress bar\n&quot;,&#10;    &quot;from tqdm import tqdm\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;TensorFlow version: {tf.__version__}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;GPU available: {tf.config.list_physical_devices('GPU')}\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## ⚙️ CẤU HÌNH&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# ============================================================================\n&quot;,&#10;    &quot;# CẤU HÌNH ĐƯỜNG DẪN\n&quot;,&#10;    &quot;# ============================================================================\n&quot;,&#10;    &quot;IS_KAGGLE = os.path.exists('/kaggle/input')\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;if IS_KAGGLE:\n&quot;,&#10;    &quot;    # Đường dẫn Kaggle - THAY ĐỔI TÊN DATASET NẾU CẦN\n&quot;,&#10;    &quot;    DATA_DIR = \&quot;/kaggle/input/cicids2018\&quot;  # Thay đổi theo tên dataset của bạn\n&quot;,&#10;    &quot;    OUTPUT_DIR = \&quot;/kaggle/working\&quot;\n&quot;,&#10;    &quot;    print(\&quot; Đang chạy trên KAGGLE\&quot;)\n&quot;,&#10;    &quot;else:\n&quot;,&#10;    &quot;    # Đường dẫn Local\n&quot;,&#10;    &quot;    DATA_DIR = r\&quot;D:\\PROJECT\\Machine Learning\\IOT\\CICIDS2018-CSV\&quot;\n&quot;,&#10;    &quot;    OUTPUT_DIR = r\&quot;D:\\PROJECT\\Machine Learning\\IOT\\CNN\&quot;\n&quot;,&#10;    &quot;    print(\&quot; Đang chạy trên LOCAL\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Tạo các thư mục con\n&quot;,&#10;    &quot;CLEANED_DATA_DIR = os.path.join(OUTPUT_DIR, \&quot;cleaned_data\&quot;)\n&quot;,&#10;    &quot;TRAINING_DATA_DIR = os.path.join(OUTPUT_DIR, \&quot;training_data\&quot;)\n&quot;,&#10;    &quot;MODEL_DIR = os.path.join(OUTPUT_DIR, \&quot;models\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;for d in [CLEANED_DATA_DIR, TRAINING_DATA_DIR, MODEL_DIR]:\n&quot;,&#10;    &quot;    os.makedirs(d, exist_ok=True)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# ============================================================================\n&quot;,&#10;    &quot;# CẤU HÌNH XỬ LÝ DỮ LIỆU\n&quot;,&#10;    &quot;# ============================================================================\n&quot;,&#10;    &quot;CHUNK_SIZE = 300000        # Số dòng mỗi chunk khi đọc CSV\n&quot;,&#10;    &quot;RANDOM_STATE = 42          # Random seed\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# ============================================================================\n&quot;,&#10;    &quot;# CẤU HÌNH CÂN BẰNG DỮ LIỆU\n&quot;,&#10;    &quot;# ============================================================================\n&quot;,&#10;    &quot;TOTAL_SAMPLES = 3000000    # Tổng số mẫu mong muốn (3 triệu)\n&quot;,&#10;    &quot;BENIGN_RATIO = 0.70        # 70% Benign\n&quot;,&#10;    &quot;ATTACK_RATIO = 0.30        # 30% Attack\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;TARGET_BENIGN = int(TOTAL_SAMPLES * BENIGN_RATIO)  # 2,100,000\n&quot;,&#10;    &quot;TARGET_ATTACK = int(TOTAL_SAMPLES * ATTACK_RATIO)  # 900,000\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Tỷ lệ chia train/val/test\n&quot;,&#10;    &quot;TEST_SIZE = 0.20   # 20% test\n&quot;,&#10;    &quot;VAL_SIZE = 0.10    # 10% validation\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# ============================================================================\n&quot;,&#10;    &quot;# CẤU HÌNH TRAINING\n&quot;,&#10;    &quot;# ============================================================================\n&quot;,&#10;    &quot;BATCH_SIZE = 256\n&quot;,&#10;    &quot;EPOCHS = 50\n&quot;,&#10;    &quot;LEARNING_RATE = 0.001\n&quot;,&#10;    &quot;DROPOUT_RATE = 0.5\n&quot;,&#10;    &quot;PATIENCE = 10\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# ============================================================================\n&quot;,&#10;    &quot;# CÁC CỘT CẦN LOẠI BỎ\n&quot;,&#10;    &quot;# ============================================================================\n&quot;,&#10;    &quot;COLUMNS_TO_DROP = [\n&quot;,&#10;    &quot;    'Flow ID', 'Src IP', 'Dst IP', 'Src Port', 'Dst Port', 'Timestamp'\n&quot;,&#10;    &quot;]\n&quot;,&#10;    &quot;LABEL_COLUMN = 'Label'\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;\\n CẤU HÌNH:\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Tổng mẫu: {TOTAL_SAMPLES:,}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Benign: {TARGET_BENIGN:,} ({BENIGN_RATIO*100:.0f}%)\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Attack: {TARGET_ATTACK:,} ({ATTACK_RATIO*100:.0f}%)\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;---\n&quot;,&#10;    &quot;#  BƯỚC 1: CLEAN DỮ LIỆU\n&quot;,&#10;    &quot;---&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def get_csv_files(data_dir):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Lấy danh sách các file CSV\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    data_dir = Path(data_dir)\n&quot;,&#10;    &quot;    csv_files = list(data_dir.glob(\&quot;*_TrafficForML_CICFlowMeter.csv\&quot;))\n&quot;,&#10;    &quot;    if not csv_files:\n&quot;,&#10;    &quot;        csv_files = [f for f in data_dir.glob(\&quot;*.csv\&quot;) if not f.name.endswith('.zip')]\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    if not csv_files:\n&quot;,&#10;    &quot;        raise FileNotFoundError(f\&quot;Không tìm thấy file CSV trong {data_dir}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;\\n Tìm thấy {len(csv_files)} file CSV:\&quot;)\n&quot;,&#10;    &quot;    for f in sorted(csv_files):\n&quot;,&#10;    &quot;        print(f\&quot;   - {f.name}\&quot;)\n&quot;,&#10;    &quot;    return sorted(csv_files)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;csv_files = get_csv_files(DATA_DIR)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def first_pass_collect_info(csv_files, chunk_size=CHUNK_SIZE):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    Lần đọc đầu tiên: Thu thập thông tin về columns và tính mode\n&quot;,&#10;    &quot;    Mục đích:\n&quot;,&#10;    &quot;    - Xác định các cột có variance = 0\n&quot;,&#10;    &quot;    - Tính mode của từng cột để thay thế NaN/Inf\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    print(\&quot;\\n\&quot; + \&quot;=\&quot;*80)\n&quot;,&#10;    &quot;    print(\&quot; BƯỚC 1.1: THU THẬP THÔNG TIN TỪ DỮ LIỆU\&quot;)\n&quot;,&#10;    &quot;    print(\&quot;=\&quot;*80)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    all_columns = None\n&quot;,&#10;    &quot;    column_value_counts = {}  # Để tính mode\n&quot;,&#10;    &quot;    column_min_max = {}  # Để kiểm tra variance\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    for csv_file in csv_files:\n&quot;,&#10;    &quot;        print(f\&quot;\\n   Đang scan: {csv_file.name}\&quot;)\n&quot;,&#10;    &quot;        chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size,\n&quot;,&#10;    &quot;                                    low_memory=False, encoding='utf-8')\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        for chunk in tqdm(chunk_iterator, desc=\&quot;   Chunks\&quot;):\n&quot;,&#10;    &quot;            # Chuẩn hóa tên cột\n&quot;,&#10;    &quot;            chunk.columns = chunk.columns.str.strip()\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Loại bỏ cột identification\n&quot;,&#10;    &quot;            cols_to_drop = [c for c in COLUMNS_TO_DROP if c in chunk.columns]\n&quot;,&#10;    &quot;            chunk = chunk.drop(columns=cols_to_drop)\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Chuyển sang numeric\n&quot;,&#10;    &quot;            feature_cols = [c for c in chunk.columns if c != LABEL_COLUMN]\n&quot;,&#10;    &quot;            for col in feature_cols:\n&quot;,&#10;    &quot;                if chunk[col].dtype == 'object':\n&quot;,&#10;    &quot;                    chunk[col] = pd.to_numeric(chunk[col], errors='coerce')\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            if all_columns is None:\n&quot;,&#10;    &quot;                all_columns = feature_cols\n&quot;,&#10;    &quot;                for col in all_columns:\n&quot;,&#10;    &quot;                    column_value_counts[col] = {}\n&quot;,&#10;    &quot;                    column_min_max[col] = {'min': np.inf, 'max': -np.inf}\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Thu thập thông tin cho mỗi cột\n&quot;,&#10;    &quot;            for col in all_columns:\n&quot;,&#10;    &quot;                if col in chunk.columns:\n&quot;,&#10;    &quot;                    col_data = chunk[col].replace([np.inf, -np.inf], np.nan)\n&quot;,&#10;    &quot;                    valid_data = col_data.dropna()\n&quot;,&#10;    &quot;                    \n&quot;,&#10;    &quot;                    if len(valid_data) &gt; 0:\n&quot;,&#10;    &quot;                        # Cập nhật min/max\n&quot;,&#10;    &quot;                        column_min_max[col]['min'] = min(column_min_max[col]['min'], valid_data.min())\n&quot;,&#10;    &quot;                        column_min_max[col]['max'] = max(column_min_max[col]['max'], valid_data.max())\n&quot;,&#10;    &quot;                        \n&quot;,&#10;    &quot;                        # Thu thập value counts cho mode\n&quot;,&#10;    &quot;                        vc = valid_data.value_counts().head(10).to_dict()\n&quot;,&#10;    &quot;                        for val, count in vc.items():\n&quot;,&#10;    &quot;                            if val not in column_value_counts[col]:\n&quot;,&#10;    &quot;                                column_value_counts[col][val] = 0\n&quot;,&#10;    &quot;                            column_value_counts[col][val] += count\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            gc.collect()\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Xác định zero-variance columns\n&quot;,&#10;    &quot;    zero_variance_cols = []\n&quot;,&#10;    &quot;    for col in all_columns:\n&quot;,&#10;    &quot;        if column_min_max[col]['min'] == column_min_max[col]['max']:\n&quot;,&#10;    &quot;            zero_variance_cols.append(col)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Tính mode cho mỗi cột\n&quot;,&#10;    &quot;    column_modes = {}\n&quot;,&#10;    &quot;    for col in all_columns:\n&quot;,&#10;    &quot;        if col not in zero_variance_cols:\n&quot;,&#10;    &quot;            if column_value_counts[col]:\n&quot;,&#10;    &quot;                mode_val = max(column_value_counts[col], key=column_value_counts[col].get)\n&quot;,&#10;    &quot;                column_modes[col] = mode_val\n&quot;,&#10;    &quot;            else:\n&quot;,&#10;    &quot;                column_modes[col] = 0\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;\\n   ✅ Số cột zero-variance sẽ loại bỏ: {len(zero_variance_cols)}\&quot;)\n&quot;,&#10;    &quot;    if zero_variance_cols:\n&quot;,&#10;    &quot;        print(f\&quot;      Các cột: {zero_variance_cols}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   ✅ Số cột sẽ giữ lại: {len(all_columns) - len(zero_variance_cols)}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    return all_columns, zero_variance_cols, column_modes\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;all_columns, zero_variance_cols, column_modes = first_pass_collect_info(csv_files)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def clean_all_files(csv_files, zero_variance_cols, column_modes, chunk_size=CHUNK_SIZE):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    Clean tất cả các file CSV\n&quot;,&#10;    &quot;    - Loại bỏ cột identification\n&quot;,&#10;    &quot;    - Loại bỏ zero-variance columns\n&quot;,&#10;    &quot;    - Xử lý NaN/Inf bằng mode\n&quot;,&#10;    &quot;    - Chuyển nhãn sang binary\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    print(\&quot;\\n\&quot; + \&quot;=\&quot;*80)\n&quot;,&#10;    &quot;    print(\&quot; BƯỚC 1.2: CLEAN DỮ LIỆU\&quot;)\n&quot;,&#10;    &quot;    print(\&quot;=\&quot;*80)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    all_dataframes = []\n&quot;,&#10;    &quot;    stats = {\n&quot;,&#10;    &quot;        'total_rows_read': 0,\n&quot;,&#10;    &quot;        'nan_replaced': 0,\n&quot;,&#10;    &quot;        'inf_replaced': 0\n&quot;,&#10;    &quot;    }\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    for csv_file in csv_files:\n&quot;,&#10;    &quot;        print(f\&quot;\\n Đang xử lý: {csv_file.name}\&quot;)\n&quot;,&#10;    &quot;        processed_chunks = []\n&quot;,&#10;    &quot;        chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size,\n&quot;,&#10;    &quot;                                     low_memory=False, encoding='utf-8')\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        for chunk in tqdm(chunk_iterator, desc=\&quot;   Chunks\&quot;):\n&quot;,&#10;    &quot;            stats['total_rows_read'] += len(chunk)\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Chuẩn hóa tên cột\n&quot;,&#10;    &quot;            chunk.columns = chunk.columns.str.strip()\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Loại bỏ cột identification\n&quot;,&#10;    &quot;            cols_to_drop = [c for c in COLUMNS_TO_DROP if c in chunk.columns]\n&quot;,&#10;    &quot;            chunk = chunk.drop(columns=cols_to_drop)\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Chuyển sang numeric\n&quot;,&#10;    &quot;            feature_cols = [c for c in chunk.columns if c != LABEL_COLUMN]\n&quot;,&#10;    &quot;            for col in feature_cols:\n&quot;,&#10;    &quot;                if chunk[col].dtype == 'object':\n&quot;,&#10;    &quot;                    chunk[col] = pd.to_numeric(chunk[col], errors='coerce')\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Chuyển đổi nhãn sang binary\n&quot;,&#10;    &quot;            chunk[LABEL_COLUMN] = chunk[LABEL_COLUMN].astype(str).str.strip().str.lower()\n&quot;,&#10;    &quot;            chunk = chunk[chunk[LABEL_COLUMN] != 'label']  # Loại header lẫn vào\n&quot;,&#10;    &quot;            chunk['binary_label'] = (chunk[LABEL_COLUMN] != 'benign').astype(int)\n&quot;,&#10;    &quot;            chunk = chunk.drop(columns=[LABEL_COLUMN])\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Loại bỏ zero-variance columns\n&quot;,&#10;    &quot;            cols_zv = [c for c in zero_variance_cols if c in chunk.columns]\n&quot;,&#10;    &quot;            chunk = chunk.drop(columns=cols_zv)\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Xử lý NaN/Inf bằng mode\n&quot;,&#10;    &quot;            feature_cols = [c for c in chunk.columns if c != 'binary_label']\n&quot;,&#10;    &quot;            for col in feature_cols:\n&quot;,&#10;    &quot;                if col in column_modes:\n&quot;,&#10;    &quot;                    mode_val = column_modes[col]\n&quot;,&#10;    &quot;                    \n&quot;,&#10;    &quot;                    # Đếm inf và nan\n&quot;,&#10;    &quot;                    inf_mask = np.isinf(chunk[col])\n&quot;,&#10;    &quot;                    nan_mask = chunk[col].isna()\n&quot;,&#10;    &quot;                    stats['inf_replaced'] += inf_mask.sum()\n&quot;,&#10;    &quot;                    stats['nan_replaced'] += nan_mask.sum()\n&quot;,&#10;    &quot;                    \n&quot;,&#10;    &quot;                    # Thay thế\n&quot;,&#10;    &quot;                    chunk[col] = chunk[col].replace([np.inf, -np.inf], np.nan)\n&quot;,&#10;    &quot;                    chunk[col] = chunk[col].fillna(mode_val)\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            processed_chunks.append(chunk)\n&quot;,&#10;    &quot;            gc.collect()\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        if processed_chunks:\n&quot;,&#10;    &quot;            df_file = pd.concat(processed_chunks, ignore_index=True)\n&quot;,&#10;    &quot;            all_dataframes.append(df_file)\n&quot;,&#10;    &quot;            print(f\&quot;   ✅ Đã xử lý: {len(df_file):,} mẫu\&quot;)\n&quot;,&#10;    &quot;            del processed_chunks, df_file\n&quot;,&#10;    &quot;            gc.collect()\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Gộp tất cả\n&quot;,&#10;    &quot;    print(\&quot;\\n   Đang gộp dữ liệu...\&quot;)\n&quot;,&#10;    &quot;    df_combined = pd.concat(all_dataframes, ignore_index=True)\n&quot;,&#10;    &quot;    del all_dataframes\n&quot;,&#10;    &quot;    gc.collect()\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;   Tổng số mẫu sau khi gộp: {len(df_combined):,}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Loại bỏ duplicate\n&quot;,&#10;    &quot;    print(\&quot;   Đang loại bỏ duplicate...\&quot;)\n&quot;,&#10;    &quot;    rows_before = len(df_combined)\n&quot;,&#10;    &quot;    df_combined = df_combined.drop_duplicates()\n&quot;,&#10;    &quot;    rows_after = len(df_combined)\n&quot;,&#10;    &quot;    duplicates_removed = rows_before - rows_after\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;   Số mẫu sau khi loại duplicate: {len(df_combined):,}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   Số duplicate đã loại: {duplicates_removed:,}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Thống kê\n&quot;,&#10;    &quot;    benign_count = (df_combined['binary_label'] == 0).sum()\n&quot;,&#10;    &quot;    attack_count = (df_combined['binary_label'] == 1).sum()\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;\\n    PHÂN BỐ NHÃN SAU CLEAN:\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   Benign: {benign_count:,} ({benign_count/len(df_combined)*100:.1f}%)\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   Attack: {attack_count:,} ({attack_count/len(df_combined)*100:.1f}%)\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    return df_combined, stats\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;df_cleaned, clean_stats = clean_all_files(csv_files, zero_variance_cols, column_modes)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Lưu dữ liệu đã clean\n&quot;,&#10;    &quot;print(\&quot;\\n ĐANG LƯU DỮ LIỆU ĐÃ CLEAN...\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Lưu parquet\n&quot;,&#10;    &quot;parquet_path = os.path.join(CLEANED_DATA_DIR, 'cleaned_data.parquet')\n&quot;,&#10;    &quot;df_cleaned.to_parquet(parquet_path, index=False)\n&quot;,&#10;    &quot;print(f\&quot;   ✅ Đã lưu: {parquet_path}\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Lưu feature names\n&quot;,&#10;    &quot;feature_names = [c for c in df_cleaned.columns if c != 'binary_label']\n&quot;,&#10;    &quot;with open(os.path.join(CLEANED_DATA_DIR, 'feature_names.txt'), 'w') as f:\n&quot;,&#10;    &quot;    for name in feature_names:\n&quot;,&#10;    &quot;        f.write(name + '\\n')\n&quot;,&#10;    &quot;print(f\&quot;   ✅ Đã lưu: feature_names.txt ({len(feature_names)} features)\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Lưu column modes\n&quot;,&#10;    &quot;with open(os.path.join(CLEANED_DATA_DIR, 'column_modes.pkl'), 'wb') as f:\n&quot;,&#10;    &quot;    pickle.dump(column_modes, f)\n&quot;,&#10;    &quot;print(f\&quot;   ✅ Đã lưu: column_modes.pkl\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;\\n✅ HOÀN THÀNH BƯỚC 1 - CLEAN DỮ LIỆU\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;---\n&quot;,&#10;    &quot;# ⚖️ BƯỚC 2: CÂN BẰNG VÀ CHUẨN BỊ DỮ LIỆU TRAINING\n&quot;,&#10;    &quot;---&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def balanced_sample(df, target_benign, target_attack):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    Sample dữ liệu với tỷ lệ cân bằng mong muốn\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    print(\&quot;\\n\&quot; + \&quot;=\&quot;*80)\n&quot;,&#10;    &quot;    print(\&quot;⚖️ ĐANG CÂN BẰNG DỮ LIỆU\&quot;)\n&quot;,&#10;    &quot;    print(\&quot;=\&quot;*80)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Tách theo class\n&quot;,&#10;    &quot;    df_benign = df[df['binary_label'] == 0]\n&quot;,&#10;    &quot;    df_attack = df[df['binary_label'] == 1]\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    n_benign = len(df_benign)\n&quot;,&#10;    &quot;    n_attack = len(df_attack)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;\\n   Dữ liệu gốc:\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   - Benign: {n_benign:,}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   - Attack: {n_attack:,}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;\\n   Target mong muốn:\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   - Benign: {target_benign:,} ({BENIGN_RATIO*100:.0f}%)\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   - Attack: {target_attack:,} ({ATTACK_RATIO*100:.0f}%)\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Xác định số lượng thực tế\n&quot;,&#10;    &quot;    actual_attack = min(target_attack, n_attack)\n&quot;,&#10;    &quot;    actual_benign = int(actual_attack * (BENIGN_RATIO / ATTACK_RATIO))\n&quot;,&#10;    &quot;    actual_benign = min(actual_benign, n_benign)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Điều chỉnh nếu cần\n&quot;,&#10;    &quot;    if actual_benign &lt; int(actual_attack * (BENIGN_RATIO / ATTACK_RATIO)):\n&quot;,&#10;    &quot;        actual_attack = int(actual_benign * (ATTACK_RATIO / BENIGN_RATIO))\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;\\n   Số lượng sẽ lấy:\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   - Benign: {actual_benign:,}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   - Attack: {actual_attack:,}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Random sample\n&quot;,&#10;    &quot;    df_benign_sampled = df_benign.sample(n=actual_benign, random_state=RANDOM_STATE)\n&quot;,&#10;    &quot;    df_attack_sampled = df_attack.sample(n=actual_attack, random_state=RANDOM_STATE)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Gộp và shuffle\n&quot;,&#10;    &quot;    df_balanced = pd.concat([df_benign_sampled, df_attack_sampled], ignore_index=True)\n&quot;,&#10;    &quot;    df_balanced = df_balanced.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    total = len(df_balanced)\n&quot;,&#10;    &quot;    print(f\&quot;\\n   ✅ Kết quả:\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   - Benign: {actual_benign:,} ({actual_benign/total*100:.1f}%)\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   - Attack: {actual_attack:,} ({actual_attack/total*100:.1f}%)\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   - Tổng: {total:,}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    del df_benign, df_attack, df_benign_sampled, df_attack_sampled\n&quot;,&#10;    &quot;    gc.collect()\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    return df_balanced\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;df_balanced = balanced_sample(df_cleaned, TARGET_BENIGN, TARGET_ATTACK)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Giải phóng bộ nhớ\n&quot;,&#10;    &quot;del df_cleaned\n&quot;,&#10;    &quot;gc.collect()&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Tách features và labels\n&quot;,&#10;    &quot;X = df_balanced.drop(columns=['binary_label']).values\n&quot;,&#10;    &quot;y = df_balanced['binary_label'].values\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;del df_balanced\n&quot;,&#10;    &quot;gc.collect()\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;\\n Shape:\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   X: {X.shape}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   y: {y.shape}\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Áp dụng Log Transform: log_e(1+x)\n&quot;,&#10;    &quot;print(\&quot;\\n ĐANG ÁP DỤNG LOG TRANSFORM: log_e(1+x)...\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Đảm bảo không có giá trị âm\n&quot;,&#10;    &quot;min_val = X.min()\n&quot;,&#10;    &quot;if min_val &lt; 0:\n&quot;,&#10;    &quot;    print(f\&quot;   ⚠️ Phát hiện giá trị âm (min={min_val:.4f}), đang shift...\&quot;)\n&quot;,&#10;    &quot;    X = X - min_val\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Áp dụng log(1+x)\n&quot;,&#10;    &quot;X = np.log1p(X)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;   ✅ Log transform hoàn tất\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;      Range: [{X.min():.4f}, {X.max():.4f}]\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Chuẩn hóa bằng StandardScaler\n&quot;,&#10;    &quot;print(\&quot;\\n ĐANG CHUẨN HÓA BẰNG STANDARDSCALER...\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;scaler = StandardScaler()\n&quot;,&#10;    &quot;X = scaler.fit_transform(X)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;   ✅ StandardScaler hoàn tất\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;      Mean: {X.mean():.6f}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;      Std:  {X.std():.6f}\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Reshape cho CNN 1D\n&quot;,&#10;    &quot;print(\&quot;\\n ĐANG RESHAPE CHO CNN 1D...\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;X = X.reshape(X.shape[0], X.shape[1], 1)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;   ✅ Shape: {X.shape}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;      (samples, features, channels)\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Chia train/val/test\n&quot;,&#10;    &quot;print(\&quot;\\n ĐANG CHIA DỮ LIỆU TRAIN/VAL/TEST...\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Chia train+val / test\n&quot;,&#10;    &quot;X_temp, X_test, y_temp, y_test = train_test_split(\n&quot;,&#10;    &quot;    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n&quot;,&#10;    &quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Chia train / val\n&quot;,&#10;    &quot;val_ratio = VAL_SIZE / (1 - TEST_SIZE)\n&quot;,&#10;    &quot;X_train, X_val, y_train, y_val = train_test_split(\n&quot;,&#10;    &quot;    X_temp, y_temp, test_size=val_ratio, random_state=RANDOM_STATE, stratify=y_temp\n&quot;,&#10;    &quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;del X, y, X_temp, y_temp\n&quot;,&#10;    &quot;gc.collect()\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;\\n    KẾT QUẢ CHIA DỮ LIỆU:\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   {'='*60}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   {'Set':&lt;10} {'Samples':&gt;12} {'Benign':&gt;12} {'Attack':&gt;12} {'Attack%':&gt;10}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   {'-'*60}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   {'Train':&lt;10} {len(X_train):&gt;12,} {(y_train==0).sum():&gt;12,} {(y_train==1).sum():&gt;12,} {(y_train==1).sum()/len(y_train)*100:&gt;9.1f}%\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   {'Val':&lt;10} {len(X_val):&gt;12,} {(y_val==0).sum():&gt;12,} {(y_val==1).sum():&gt;12,} {(y_val==1).sum()/len(y_val)*100:&gt;9.1f}%\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   {'Test':&lt;10} {len(X_test):&gt;12,} {(y_test==0).sum():&gt;12,} {(y_test==1).sum():&gt;12,} {(y_test==1).sum()/len(y_test)*100:&gt;9.1f}%\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   {'-'*60}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   {'Total':&lt;10} {len(X_train)+len(X_val)+len(X_test):&gt;12,}\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Tính class weights\n&quot;,&#10;    &quot;print(\&quot;\\n⚖️ ĐANG TÍNH CLASS WEIGHTS...\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;classes = np.unique(y_train)\n&quot;,&#10;    &quot;weights = compute_class_weight('balanced', classes=classes, y=y_train)\n&quot;,&#10;    &quot;class_weights = dict(zip(classes, weights))\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;   Class 0 (Benign): {class_weights[0]:.4f}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Class 1 (Attack): {class_weights[1]:.4f}\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Lưu dữ liệu training\n&quot;,&#10;    &quot;print(\&quot;\\n ĐANG LƯU DỮ LIỆU TRAINING...\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;np.save(os.path.join(TRAINING_DATA_DIR, 'X_train.npy'), X_train)\n&quot;,&#10;    &quot;np.save(os.path.join(TRAINING_DATA_DIR, 'X_val.npy'), X_val)\n&quot;,&#10;    &quot;np.save(os.path.join(TRAINING_DATA_DIR, 'X_test.npy'), X_test)\n&quot;,&#10;    &quot;np.save(os.path.join(TRAINING_DATA_DIR, 'y_train.npy'), y_train)\n&quot;,&#10;    &quot;np.save(os.path.join(TRAINING_DATA_DIR, 'y_val.npy'), y_val)\n&quot;,&#10;    &quot;np.save(os.path.join(TRAINING_DATA_DIR, 'y_test.npy'), y_test)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;with open(os.path.join(TRAINING_DATA_DIR, 'scaler.pkl'), 'wb') as f:\n&quot;,&#10;    &quot;    pickle.dump(scaler, f)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;with open(os.path.join(TRAINING_DATA_DIR, 'class_weights.pkl'), 'wb') as f:\n&quot;,&#10;    &quot;    pickle.dump(class_weights, f)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;   ✅ Đã lưu tất cả dữ liệu training\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;\\n✅ HOÀN THÀNH BƯỚC 2 - CHUẨN BỊ DỮ LIỆU\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;---\n&quot;,&#10;    &quot;#  BƯỚC 3: XÂY DỰNG VÀ TRAIN MÔ HÌNH CNN\n&quot;,&#10;    &quot;---&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def build_cnn_model(input_shape):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    Xây dựng mô hình CNN cho phân loại binary\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    Kiến trúc:\n&quot;,&#10;    &quot;    - 5 lớp Conv1D với MaxPooling\n&quot;,&#10;    &quot;    - BatchNormalization + Dropout trước Flatten\n&quot;,&#10;    &quot;    - Output: Dense(1, sigmoid)\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    print(\&quot;\\n\&quot; + \&quot;=\&quot;*80)\n&quot;,&#10;    &quot;    print(\&quot;️ ĐANG XÂY DỰNG MÔ HÌNH CNN\&quot;)\n&quot;,&#10;    &quot;    print(\&quot;=\&quot;*80)\n&quot;,&#10;    &quot;    print(f\&quot;   Input shape: {input_shape}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    model = Sequential(name='CNN_Binary_Classification')\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Input\n&quot;,&#10;    &quot;    model.add(Input(shape=input_shape))\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Conv Block 1: 32 filters\n&quot;,&#10;    &quot;    model.add(Conv1D(32, kernel_size=2, activation='relu', padding='same', name='conv1d_1'))\n&quot;,&#10;    &quot;    model.add(MaxPooling1D(pool_size=2, name='maxpool_1'))\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Conv Block 2: 32 filters\n&quot;,&#10;    &quot;    model.add(Conv1D(32, kernel_size=2, activation='relu', padding='same', name='conv1d_2'))\n&quot;,&#10;    &quot;    model.add(MaxPooling1D(pool_size=2, name='maxpool_2'))\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Conv Block 3: 64 filters\n&quot;,&#10;    &quot;    model.add(Conv1D(64, kernel_size=2, activation='relu', padding='same', name='conv1d_3'))\n&quot;,&#10;    &quot;    model.add(MaxPooling1D(pool_size=2, name='maxpool_3'))\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Conv Block 4: 64 filters\n&quot;,&#10;    &quot;    model.add(Conv1D(64, kernel_size=2, activation='relu', padding='same', name='conv1d_4'))\n&quot;,&#10;    &quot;    model.add(MaxPooling1D(pool_size=2, name='maxpool_4'))\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Conv Block 5: 64 filters\n&quot;,&#10;    &quot;    model.add(Conv1D(64, kernel_size=2, activation='relu', padding='same', name='conv1d_5'))\n&quot;,&#10;    &quot;    model.add(MaxPooling1D(pool_size=2, name='maxpool_5'))\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Regularization\n&quot;,&#10;    &quot;    model.add(BatchNormalization(name='batch_norm'))\n&quot;,&#10;    &quot;    model.add(Dropout(DROPOUT_RATE, name='dropout'))\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Flatten và Output\n&quot;,&#10;    &quot;    model.add(Flatten(name='flatten'))\n&quot;,&#10;    &quot;    model.add(Dense(1, activation='sigmoid', name='output'))\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Compile\n&quot;,&#10;    &quot;    model.compile(\n&quot;,&#10;    &quot;        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n&quot;,&#10;    &quot;        loss='binary_crossentropy',\n&quot;,&#10;    &quot;        metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n&quot;,&#10;    &quot;    )\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(\&quot;\\n    KIẾN TRÚC MÔ HÌNH:\&quot;)\n&quot;,&#10;    &quot;    model.summary()\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    return model\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Xây dựng model\n&quot;,&#10;    &quot;input_shape = (X_train.shape[1], X_train.shape[2])\n&quot;,&#10;    &quot;model = build_cnn_model(input_shape)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Tạo callbacks\n&quot;,&#10;    &quot;print(\&quot;\\n ĐANG CẤU HÌNH CALLBACKS...\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;callbacks = [\n&quot;,&#10;    &quot;    # Early Stopping\n&quot;,&#10;    &quot;    EarlyStopping(\n&quot;,&#10;    &quot;        monitor='val_loss',\n&quot;,&#10;    &quot;        patience=PATIENCE,\n&quot;,&#10;    &quot;        verbose=1,\n&quot;,&#10;    &quot;        mode='min',\n&quot;,&#10;    &quot;        restore_best_weights=True\n&quot;,&#10;    &quot;    ),\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Model Checkpoint\n&quot;,&#10;    &quot;    ModelCheckpoint(\n&quot;,&#10;    &quot;        filepath=os.path.join(MODEL_DIR, 'best_model.keras'),\n&quot;,&#10;    &quot;        monitor='val_loss',\n&quot;,&#10;    &quot;        verbose=1,\n&quot;,&#10;    &quot;        save_best_only=True,\n&quot;,&#10;    &quot;        mode='min'\n&quot;,&#10;    &quot;    ),\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Reduce LR\n&quot;,&#10;    &quot;    ReduceLROnPlateau(\n&quot;,&#10;    &quot;        monitor='val_loss',\n&quot;,&#10;    &quot;        factor=0.5,\n&quot;,&#10;    &quot;        patience=5,\n&quot;,&#10;    &quot;        min_lr=1e-7,\n&quot;,&#10;    &quot;        verbose=1\n&quot;,&#10;    &quot;    )\n&quot;,&#10;    &quot;]\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;   ✅ EarlyStopping: patience=10\&quot;)\n&quot;,&#10;    &quot;print(\&quot;   ✅ ModelCheckpoint: save best model\&quot;)\n&quot;,&#10;    &quot;print(\&quot;   ✅ ReduceLROnPlateau: factor=0.5, patience=5\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Train model\n&quot;,&#10;    &quot;print(\&quot;\\n\&quot; + \&quot;=\&quot;*80)\n&quot;,&#10;    &quot;print(\&quot; BẮT ĐẦU HUẤN LUYỆN MÔ HÌNH\&quot;)\n&quot;,&#10;    &quot;print(\&quot;=\&quot;*80)\n&quot;,&#10;    &quot;print(f\&quot;   Epochs: {EPOCHS}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Batch size: {BATCH_SIZE}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Learning rate: {LEARNING_RATE}\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;start_time = datetime.now()\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;history = model.fit(\n&quot;,&#10;    &quot;    X_train, y_train,\n&quot;,&#10;    &quot;    batch_size=BATCH_SIZE,\n&quot;,&#10;    &quot;    epochs=EPOCHS,\n&quot;,&#10;    &quot;    validation_data=(X_val, y_val),\n&quot;,&#10;    &quot;    class_weight=class_weights,\n&quot;,&#10;    &quot;    callbacks=callbacks,\n&quot;,&#10;    &quot;    verbose=1\n&quot;,&#10;    &quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;end_time = datetime.now()\n&quot;,&#10;    &quot;training_time = (end_time - start_time).total_seconds()\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;\\n   ⏱️ Thời gian training: {training_time/60:.2f} phút\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;    Best val_loss: {min(history.history['val_loss']):.4f}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;    Best val_accuracy: {max(history.history['val_accuracy']):.4f}\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Đánh giá trên test set\n&quot;,&#10;    &quot;print(\&quot;\\n\&quot; + \&quot;=\&quot;*80)\n&quot;,&#10;    &quot;print(\&quot; ĐÁNH GIÁ MÔ HÌNH TRÊN TEST SET\&quot;)\n&quot;,&#10;    &quot;print(\&quot;=\&quot;*80)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;loss, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=1)\n&quot;,&#10;    &quot;f1_score = 2 * (precision * recall) / (precision + recall + 1e-7)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;\\n    KẾT QUẢ:\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   {'='*40}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Loss:      {loss:.4f}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Precision: {precision:.4f}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Recall:    {recall:.4f}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   F1-Score:  {f1_score:.4f}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   {'='*40}\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Confusion Matrix và Classification Report\n&quot;,&#10;    &quot;y_pred_prob = model.predict(X_test, verbose=0)\n&quot;,&#10;    &quot;y_pred = (y_pred_prob &gt; 0.5).astype(int).flatten()\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;\\n CONFUSION MATRIX:\&quot;)\n&quot;,&#10;    &quot;cm = confusion_matrix(y_test, y_pred)\n&quot;,&#10;    &quot;print(f\&quot;                 Predicted\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;                 Benign  Attack\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Actual Benign  {cm[0,0]:&gt;6}  {cm[0,1]:&gt;6}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Actual Attack  {cm[1,0]:&gt;6}  {cm[1,1]:&gt;6}\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;\\n CLASSIFICATION REPORT:\&quot;)\n&quot;,&#10;    &quot;print(classification_report(y_test, y_pred, target_names=['Benign', 'Attack']))&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Vẽ biểu đồ training history\n&quot;,&#10;    &quot;fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Loss\n&quot;,&#10;    &quot;axes[0, 0].plot(history.history['loss'], label='Train Loss')\n&quot;,&#10;    &quot;axes[0, 0].plot(history.history['val_loss'], label='Val Loss')\n&quot;,&#10;    &quot;axes[0, 0].set_title('Model Loss')\n&quot;,&#10;    &quot;axes[0, 0].set_xlabel('Epoch')\n&quot;,&#10;    &quot;axes[0, 0].set_ylabel('Loss')\n&quot;,&#10;    &quot;axes[0, 0].legend()\n&quot;,&#10;    &quot;axes[0, 0].grid(True)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Accuracy\n&quot;,&#10;    &quot;axes[0, 1].plot(history.history['accuracy'], label='Train Accuracy')\n&quot;,&#10;    &quot;axes[0, 1].plot(history.history['val_accuracy'], label='Val Accuracy')\n&quot;,&#10;    &quot;axes[0, 1].set_title('Model Accuracy')\n&quot;,&#10;    &quot;axes[0, 1].set_xlabel('Epoch')\n&quot;,&#10;    &quot;axes[0, 1].set_ylabel('Accuracy')\n&quot;,&#10;    &quot;axes[0, 1].legend()\n&quot;,&#10;    &quot;axes[0, 1].grid(True)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Precision\n&quot;,&#10;    &quot;axes[1, 0].plot(history.history['precision'], label='Train Precision')\n&quot;,&#10;    &quot;axes[1, 0].plot(history.history['val_precision'], label='Val Precision')\n&quot;,&#10;    &quot;axes[1, 0].set_title('Model Precision')\n&quot;,&#10;    &quot;axes[1, 0].set_xlabel('Epoch')\n&quot;,&#10;    &quot;axes[1, 0].set_ylabel('Precision')\n&quot;,&#10;    &quot;axes[1, 0].legend()\n&quot;,&#10;    &quot;axes[1, 0].grid(True)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Recall\n&quot;,&#10;    &quot;axes[1, 1].plot(history.history['recall'], label='Train Recall')\n&quot;,&#10;    &quot;axes[1, 1].plot(history.history['val_recall'], label='Val Recall')\n&quot;,&#10;    &quot;axes[1, 1].set_title('Model Recall')\n&quot;,&#10;    &quot;axes[1, 1].set_xlabel('Epoch')\n&quot;,&#10;    &quot;axes[1, 1].set_ylabel('Recall')\n&quot;,&#10;    &quot;axes[1, 1].legend()\n&quot;,&#10;    &quot;axes[1, 1].grid(True)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;plt.tight_layout()\n&quot;,&#10;    &quot;plt.savefig(os.path.join(MODEL_DIR, 'training_history.png'), dpi=150)\n&quot;,&#10;    &quot;plt.show()\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;   ✅ Đã lưu training_history.png\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Lưu model và kết quả\n&quot;,&#10;    &quot;print(\&quot;\\n ĐANG LƯU MODEL VÀ KẾT QUẢ...\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Lưu model\n&quot;,&#10;    &quot;model.save(os.path.join(MODEL_DIR, 'final_model.keras'))\n&quot;,&#10;    &quot;print(\&quot;   ✅ final_model.keras\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Lưu weights\n&quot;,&#10;    &quot;model.save_weights(os.path.join(MODEL_DIR, 'model_weights.h5'))\n&quot;,&#10;    &quot;print(\&quot;   ✅ model_weights.h5\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Lưu history\n&quot;,&#10;    &quot;history_dict = {key: [float(v) for v in values] for key, values in history.history.items()}\n&quot;,&#10;    &quot;with open(os.path.join(MODEL_DIR, 'training_history.json'), 'w') as f:\n&quot;,&#10;    &quot;    json.dump(history_dict, f, indent=4)\n&quot;,&#10;    &quot;print(\&quot;   ✅ training_history.json\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Lưu kết quả\n&quot;,&#10;    &quot;results = {\n&quot;,&#10;    &quot;    'test_loss': float(loss),\n&quot;,&#10;    &quot;    'test_accuracy': float(accuracy),\n&quot;,&#10;    &quot;    'test_precision': float(precision),\n&quot;,&#10;    &quot;    'test_recall': float(recall),\n&quot;,&#10;    &quot;    'test_f1_score': float(f1_score),\n&quot;,&#10;    &quot;    'training_time_minutes': training_time / 60,\n&quot;,&#10;    &quot;    'epochs_trained': len(history.history['loss']),\n&quot;,&#10;    &quot;    'confusion_matrix': cm.tolist()\n&quot;,&#10;    &quot;}\n&quot;,&#10;    &quot;with open(os.path.join(MODEL_DIR, 'evaluation_results.json'), 'w') as f:\n&quot;,&#10;    &quot;    json.dump(results, f, indent=4)\n&quot;,&#10;    &quot;print(\&quot;   ✅ evaluation_results.json\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;\\n Tất cả file được lưu tại: {MODEL_DIR}\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Tóm tắt kết quả\n&quot;,&#10;    &quot;print(\&quot;\\n\&quot; + \&quot;=\&quot;*80)\n&quot;,&#10;    &quot;print(\&quot;✅ HOÀN THÀNH HUẤN LUYỆN MÔ HÌNH CNN!\&quot;)\n&quot;,&#10;    &quot;print(\&quot;=\&quot;*80)\n&quot;,&#10;    &quot;print(f\&quot;\\n    KẾT QUẢ CUỐI CÙNG:\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   {'='*40}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Test Accuracy:  {accuracy*100:.2f}%\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Test Precision: {precision*100:.2f}%\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Test Recall:    {recall*100:.2f}%\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Test F1-Score:  {f1_score*100:.2f}%\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Training Time:  {training_time/60:.2f} phút\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   {'='*40}\&quot;)\n&quot;,&#10;    &quot;print(\&quot;\\n   Mô hình đã được lưu và sẵn sàng sử dụng!\&quot;)&quot;&#10;   ]&#10;  }&#10; ],&#10; &quot;metadata&quot;: {&#10;  &quot;kernelspec&quot;: {&#10;   &quot;display_name&quot;: &quot;Python 3&quot;,&#10;   &quot;language&quot;: &quot;python&quot;,&#10;   &quot;name&quot;: &quot;python3&quot;&#10;  },&#10;  &quot;language_info&quot;: {&#10;   &quot;codemirror_mode&quot;: {&#10;    &quot;name&quot;: &quot;ipython&quot;,&#10;    &quot;version&quot;: 3&#10;   },&#10;   &quot;file_extension&quot;: &quot;.py&quot;,&#10;   &quot;mimetype&quot;: &quot;text/x-python&quot;,&#10;   &quot;name&quot;: &quot;python&quot;,&#10;   &quot;nbconvert_exporter&quot;: &quot;python&quot;,&#10;   &quot;pygments_lexer&quot;: &quot;ipython3&quot;,&#10;   &quot;version&quot;: &quot;3.10.0&quot;&#10;  }&#10; },&#10; &quot;nbformat&quot;: 4,&#10; &quot;nbformat_minor&quot;: 4&#10;}&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/CNN/prepare_folds.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/CNN/prepare_folds.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;=============================================================================&#10;CHIA DỮ LIỆU THEO K-FOLD CHO MÔ HÌNH CNN&#10;Phát hiện lưu lượng mạng IoT bất thường (Binary Classification)&#10;=============================================================================&#10;Mô tả:&#10;    - Đọc dữ liệu đã được tiền xử lý&#10;    - Chia dữ liệu theo K-Fold Cross Validation&#10;    - Hỗ trợ Stratified K-Fold để giữ tỷ lệ class&#10;    - Lưu các fold để sử dụng khi training&#10;=============================================================================&#10;&quot;&quot;&quot;&#10;&#10;import os&#10;import numpy as np&#10;import pickle&#10;from sklearn.model_selection import StratifiedKFold, train_test_split&#10;from tqdm import tqdm&#10;import warnings&#10;&#10;warnings.filterwarnings('ignore')&#10;&#10;&#10;# =============================================================================&#10;# CẤU HÌNH ĐƯỜNG DẪN&#10;# =============================================================================&#10;&#10;def get_paths():&#10;    &quot;&quot;&quot;&#10;    Tự động xác định đường dẫn dựa trên môi trường (Kaggle/Local)&#10;    &quot;&quot;&quot;&#10;    # Kiểm tra nếu đang chạy trên Kaggle&#10;    if os.path.exists('/kaggle/input'):&#10;        INPUT_DIR = '/kaggle/working/processed_cnn'&#10;        OUTPUT_DIR = '/kaggle/working/folds'&#10;        print(&quot; Đang chạy trên KAGGLE&quot;)&#10;    else:&#10;        # Đường dẫn Local&#10;        BASE_DIR = os.path.dirname(os.path.abspath(__file__))&#10;        INPUT_DIR = os.path.join(BASE_DIR, 'processed_data')&#10;        OUTPUT_DIR = os.path.join(BASE_DIR, 'folds')&#10;        print(&quot; Đang chạy trên LOCAL&quot;)&#10;    &#10;    # Tạo thư mục output nếu chưa tồn tại&#10;    os.makedirs(OUTPUT_DIR, exist_ok=True)&#10;    &#10;    print(f&quot; Thư mục input: {INPUT_DIR}&quot;)&#10;    print(f&quot; Thư mục output: {OUTPUT_DIR}&quot;)&#10;    &#10;    return INPUT_DIR, OUTPUT_DIR&#10;&#10;&#10;# =============================================================================&#10;# HÀM CHIA DỮ LIỆU&#10;# =============================================================================&#10;&#10;def load_processed_data(input_dir):&#10;    &quot;&quot;&quot;&#10;    Đọc dữ liệu đã được tiền xử lý&#10;    &quot;&quot;&quot;&#10;    print(&quot;\n Đang đọc dữ liệu đã xử lý...&quot;)&#10;    &#10;    # Đọc features và labels&#10;    X = np.load(os.path.join(input_dir, 'X_processed.npy'))&#10;    y = np.load(os.path.join(input_dir, 'y_binary.npy'))&#10;    &#10;    # Đọc metadata&#10;    with open(os.path.join(input_dir, 'metadata.pkl'), 'rb') as f:&#10;        metadata = pickle.load(f)&#10;    &#10;    print(f&quot;✅ Đã đọc dữ liệu:&quot;)&#10;    print(f&quot;   - X shape: {X.shape}&quot;)&#10;    print(f&quot;   - y shape: {y.shape}&quot;)&#10;    print(f&quot;   - Số mẫu Benign (0): {np.sum(y == 0):,}&quot;)&#10;    print(f&quot;   - Số mẫu Attack (1): {np.sum(y == 1):,}&quot;)&#10;    &#10;    return X, y, metadata&#10;&#10;&#10;def reshape_for_cnn(X, method='1d'):&#10;    &quot;&quot;&quot;&#10;    Reshape dữ liệu cho CNN&#10;    &#10;    Parameters:&#10;    -----------&#10;    X : numpy array&#10;        Dữ liệu features (n_samples, n_features)&#10;    method : str&#10;        '1d' - Reshape thành (samples, features, 1) cho Conv1D&#10;        '2d' - Reshape thành hình vuông cho Conv2D&#10;    &#10;    Returns:&#10;    --------&#10;    X_reshaped : numpy array&#10;        Dữ liệu đã reshape&#10;    &quot;&quot;&quot;&#10;    n_samples, n_features = X.shape&#10;    &#10;    if method == '1d':&#10;        # Reshape cho Conv1D: (samples, features, 1)&#10;        X_reshaped = X.reshape(n_samples, n_features, 1)&#10;        &#10;    elif method == '2d':&#10;        # Tìm kích thước hình vuông gần nhất&#10;        sqrt_features = int(np.ceil(np.sqrt(n_features)))&#10;        padded_features = sqrt_features ** 2&#10;        &#10;        # Padding nếu cần&#10;        if padded_features &gt; n_features:&#10;            padding = np.zeros((n_samples, padded_features - n_features))&#10;            X_padded = np.hstack([X, padding])&#10;        else:&#10;            X_padded = X&#10;        &#10;        # Reshape cho Conv2D: (samples, height, width, 1)&#10;        X_reshaped = X_padded.reshape(n_samples, sqrt_features, sqrt_features, 1)&#10;    &#10;    else:&#10;        X_reshaped = X&#10;    &#10;    return X_reshaped&#10;&#10;&#10;def create_stratified_kfold(X, y, n_folds=5, shuffle=True, random_state=42):&#10;    &quot;&quot;&quot;&#10;    Tạo Stratified K-Fold để chia dữ liệu&#10;    Stratified giữ nguyên tỷ lệ class trong mỗi fold&#10;    &#10;    Parameters:&#10;    -----------&#10;    X : numpy array&#10;        Features&#10;    y : numpy array&#10;        Labels&#10;    n_folds : int&#10;        Số fold (mặc định 5)&#10;    shuffle : bool&#10;        Có shuffle dữ liệu không&#10;    random_state : int&#10;        Random seed&#10;    &#10;    Returns:&#10;    --------&#10;    folds : list of tuples&#10;        Mỗi tuple chứa (train_indices, val_indices)&#10;    &quot;&quot;&quot;&#10;    &#10;    print(f&quot;\n Đang tạo {n_folds}-Fold Stratified Cross Validation...&quot;)&#10;    &#10;    skf = StratifiedKFold(n_splits=n_folds, shuffle=shuffle, random_state=random_state)&#10;    &#10;    folds = []&#10;    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y)):&#10;        folds.append((train_idx, val_idx))&#10;        &#10;        # Thống kê mỗi fold&#10;        y_train_fold = y[train_idx]&#10;        y_val_fold = y[val_idx]&#10;        &#10;        print(f&quot;\n Fold {fold_idx + 1}:&quot;)&#10;        print(f&quot;   Train: {len(train_idx):,} mẫu | &quot;&#10;              f&quot;Benign: {np.sum(y_train_fold == 0):,} | &quot;&#10;              f&quot;Attack: {np.sum(y_train_fold == 1):,} | &quot;&#10;              f&quot;Ratio: {np.mean(y_train_fold)*100:.2f}%&quot;)&#10;        print(f&quot;   Val:   {len(val_idx):,} mẫu | &quot;&#10;              f&quot;Benign: {np.sum(y_val_fold == 0):,} | &quot;&#10;              f&quot;Attack: {np.sum(y_val_fold == 1):,} | &quot;&#10;              f&quot;Ratio: {np.mean(y_val_fold)*100:.2f}%&quot;)&#10;    &#10;    return folds&#10;&#10;&#10;def create_train_val_test_split(X, y, val_size=0.15, test_size=0.15, random_state=42):&#10;    &quot;&quot;&quot;&#10;    Chia dữ liệu thành Train/Validation/Test&#10;    Sử dụng Stratified split để giữ tỷ lệ class&#10;    &#10;    Parameters:&#10;    -----------&#10;    X : numpy array&#10;        Features&#10;    y : numpy array&#10;        Labels&#10;    val_size : float&#10;        Tỷ lệ validation set&#10;    test_size : float&#10;        Tỷ lệ test set&#10;    random_state : int&#10;        Random seed&#10;    &#10;    Returns:&#10;    --------&#10;    splits : dict&#10;        Dictionary chứa các split&#10;    &quot;&quot;&quot;&#10;    &#10;    print(f&quot;\n Đang chia dữ liệu Train/Val/Test...&quot;)&#10;    print(f&quot;   Tỷ lệ: Train={1-val_size-test_size:.0%} | Val={val_size:.0%} | Test={test_size:.0%}&quot;)&#10;    &#10;    # Chia train+val và test&#10;    X_trainval, X_test, y_trainval, y_test = train_test_split(&#10;        X, y, test_size=test_size, stratify=y, random_state=random_state&#10;    )&#10;    &#10;    # Chia train và val&#10;    val_ratio = val_size / (1 - test_size)&#10;    X_train, X_val, y_train, y_val = train_test_split(&#10;        X_trainval, y_trainval, test_size=val_ratio, stratify=y_trainval, &#10;        random_state=random_state&#10;    )&#10;    &#10;    splits = {&#10;        'X_train': X_train,&#10;        'y_train': y_train,&#10;        'X_val': X_val,&#10;        'y_val': y_val,&#10;        'X_test': X_test,&#10;        'y_test': y_test&#10;    }&#10;    &#10;    # Thống kê&#10;    print(f&quot;\n Kết quả chia dữ liệu:&quot;)&#10;    print(f&quot;   Train: {len(X_train):,} mẫu | &quot;&#10;          f&quot;Benign: {np.sum(y_train == 0):,} | &quot;&#10;          f&quot;Attack: {np.sum(y_train == 1):,}&quot;)&#10;    print(f&quot;   Val:   {len(X_val):,} mẫu | &quot;&#10;          f&quot;Benign: {np.sum(y_val == 0):,} | &quot;&#10;          f&quot;Attack: {np.sum(y_val == 1):,}&quot;)&#10;    print(f&quot;   Test:  {len(X_test):,} mẫu | &quot;&#10;          f&quot;Benign: {np.sum(y_test == 0):,} | &quot;&#10;          f&quot;Attack: {np.sum(y_test == 1):,}&quot;)&#10;    &#10;    return splits&#10;&#10;&#10;def save_folds(folds, X, y, output_dir, reshape_method='1d'):&#10;    &quot;&quot;&quot;&#10;    Lưu các fold đã chia&#10;    &#10;    Parameters:&#10;    -----------&#10;    folds : list of tuples&#10;        Danh sách các fold (train_idx, val_idx)&#10;    X : numpy array&#10;        Features&#10;    y : numpy array&#10;        Labels&#10;    output_dir : str&#10;        Thư mục lưu&#10;    reshape_method : str&#10;        Phương pháp reshape ('1d' hoặc '2d')&#10;    &quot;&quot;&quot;&#10;    &#10;    print(f&quot;\n Đang lưu {len(folds)} folds...&quot;)&#10;    &#10;    # Reshape dữ liệu cho CNN&#10;    X_reshaped = reshape_for_cnn(X, method=reshape_method)&#10;    print(f&quot; Shape sau reshape: {X_reshaped.shape}&quot;)&#10;    &#10;    for fold_idx, (train_idx, val_idx) in enumerate(tqdm(folds, desc=&quot;Lưu folds&quot;)):&#10;        fold_dir = os.path.join(output_dir, f'fold_{fold_idx + 1}')&#10;        os.makedirs(fold_dir, exist_ok=True)&#10;        &#10;        # Lưu train data&#10;        np.save(os.path.join(fold_dir, 'X_train.npy'), X_reshaped[train_idx])&#10;        np.save(os.path.join(fold_dir, 'y_train.npy'), y[train_idx])&#10;        &#10;        # Lưu validation data&#10;        np.save(os.path.join(fold_dir, 'X_val.npy'), X_reshaped[val_idx])&#10;        np.save(os.path.join(fold_dir, 'y_val.npy'), y[val_idx])&#10;    &#10;    # Lưu thông tin về folds&#10;    fold_info = {&#10;        'n_folds': len(folds),&#10;        'n_samples': len(y),&#10;        'n_features': X.shape[1],&#10;        'input_shape': X_reshaped.shape[1:],&#10;        'reshape_method': reshape_method,&#10;        'folds': [(list(train_idx), list(val_idx)) for train_idx, val_idx in folds]&#10;    }&#10;    &#10;    with open(os.path.join(output_dir, 'fold_info.pkl'), 'wb') as f:&#10;        pickle.dump(fold_info, f)&#10;    &#10;    print(f&quot;\n✅ Đã lưu folds vào: {output_dir}&quot;)&#10;&#10;&#10;def save_train_val_test(splits, output_dir, reshape_method='1d'):&#10;    &quot;&quot;&quot;&#10;    Lưu Train/Val/Test splits&#10;    &#10;    Parameters:&#10;    -----------&#10;    splits : dict&#10;        Dictionary chứa các split&#10;    output_dir : str&#10;        Thư mục lưu&#10;    reshape_method : str&#10;        Phương pháp reshape&#10;    &quot;&quot;&quot;&#10;    &#10;    print(f&quot;\n Đang lưu Train/Val/Test splits...&quot;)&#10;    &#10;    split_dir = os.path.join(output_dir, 'train_val_test')&#10;    os.makedirs(split_dir, exist_ok=True)&#10;    &#10;    for key, data in splits.items():&#10;        if key.startswith('X_'):&#10;            # Reshape cho CNN&#10;            data_reshaped = reshape_for_cnn(data, method=reshape_method)&#10;            np.save(os.path.join(split_dir, f'{key}.npy'), data_reshaped)&#10;            print(f&quot;   Saved {key}: {data_reshaped.shape}&quot;)&#10;        else:&#10;            np.save(os.path.join(split_dir, f'{key}.npy'), data)&#10;            print(f&quot;   Saved {key}: {data.shape}&quot;)&#10;    &#10;    # Lưu thông tin về splits&#10;    split_info = {&#10;        'n_train': len(splits['X_train']),&#10;        'n_val': len(splits['X_val']),&#10;        'n_test': len(splits['X_test']),&#10;        'n_features': splits['X_train'].shape[1],&#10;        'input_shape': reshape_for_cnn(splits['X_train'], method=reshape_method).shape[1:],&#10;        'reshape_method': reshape_method&#10;    }&#10;    &#10;    with open(os.path.join(split_dir, 'split_info.pkl'), 'wb') as f:&#10;        pickle.dump(split_info, f)&#10;    &#10;    print(f&quot;\n✅ Đã lưu splits vào: {split_dir}&quot;)&#10;&#10;&#10;# =============================================================================&#10;# HÀM CHÍNH&#10;# =============================================================================&#10;&#10;def prepare_folds(n_folds=5, reshape_method='1d', create_holdout=True, &#10;                  val_size=0.15, test_size=0.15):&#10;    &quot;&quot;&quot;&#10;    Hàm chính để chuẩn bị folds cho training&#10;    &#10;    Parameters:&#10;    -----------&#10;    n_folds : int&#10;        Số fold cho K-Fold CV&#10;    reshape_method : str&#10;        '1d' cho Conv1D, '2d' cho Conv2D&#10;    create_holdout : bool&#10;        Có tạo thêm Train/Val/Test split không&#10;    val_size : float&#10;        Tỷ lệ validation (cho holdout)&#10;    test_size : float&#10;        Tỷ lệ test (cho holdout)&#10;    &quot;&quot;&quot;&#10;    &#10;    print(&quot;=&quot;*70)&#10;    print(&quot; BẮT ĐẦU CHIA FOLDS CHO TRAINING&quot;)&#10;    print(&quot;=&quot;*70)&#10;    &#10;    # Lấy đường dẫn&#10;    INPUT_DIR, OUTPUT_DIR = get_paths()&#10;    &#10;    # Đọc dữ liệu&#10;    X, y, metadata = load_processed_data(INPUT_DIR)&#10;    &#10;    # Tạo K-Fold&#10;    folds = create_stratified_kfold(X, y, n_folds=n_folds)&#10;    &#10;    # Lưu folds&#10;    save_folds(folds, X, y, OUTPUT_DIR, reshape_method=reshape_method)&#10;    &#10;    # Tạo thêm Train/Val/Test split (không dùng K-Fold)&#10;    if create_holdout:&#10;        print(&quot;\n&quot; + &quot;=&quot;*70)&#10;        print(&quot; TẠO THÊM TRAIN/VAL/TEST SPLIT&quot;)&#10;        print(&quot;=&quot;*70)&#10;        &#10;        splits = create_train_val_test_split(X, y, val_size=val_size, test_size=test_size)&#10;        save_train_val_test(splits, OUTPUT_DIR, reshape_method=reshape_method)&#10;    &#10;    print(&quot;\n&quot; + &quot;=&quot;*70)&#10;    print(&quot; HOÀN THÀNH CHIA FOLDS!&quot;)&#10;    print(&quot;=&quot;*70)&#10;    &#10;    # Tóm tắt&#10;    print(f&quot;\n TÓM TẮT:&quot;)&#10;    print(f&quot;   - Số folds: {n_folds}&quot;)&#10;    print(f&quot;   - Reshape method: {reshape_method}&quot;)&#10;    print(f&quot;   - Input shape cho CNN: {reshape_for_cnn(X, method=reshape_method).shape[1:]}&quot;)&#10;    print(f&quot;   - Thư mục output: {OUTPUT_DIR}&quot;)&#10;    &#10;    return folds&#10;&#10;&#10;# =============================================================================&#10;# HÀM TIỆN ÍCH - LOAD FOLD&#10;# =============================================================================&#10;&#10;def load_fold(fold_idx, folds_dir=None):&#10;    &quot;&quot;&quot;&#10;    Load dữ liệu của một fold cụ thể&#10;    &#10;    Parameters:&#10;    -----------&#10;    fold_idx : int&#10;        Index của fold (1-based)&#10;    folds_dir : str&#10;        Thư mục chứa folds (None để tự động detect)&#10;    &#10;    Returns:&#10;    --------&#10;    X_train, y_train, X_val, y_val&#10;    &quot;&quot;&quot;&#10;    &#10;    if folds_dir is None:&#10;        _, folds_dir = get_paths()&#10;    &#10;    fold_dir = os.path.join(folds_dir, f'fold_{fold_idx}')&#10;    &#10;    X_train = np.load(os.path.join(fold_dir, 'X_train.npy'))&#10;    y_train = np.load(os.path.join(fold_dir, 'y_train.npy'))&#10;    X_val = np.load(os.path.join(fold_dir, 'X_val.npy'))&#10;    y_val = np.load(os.path.join(fold_dir, 'y_val.npy'))&#10;    &#10;    print(f&quot; Đã load Fold {fold_idx}:&quot;)&#10;    print(f&quot;   X_train: {X_train.shape}, y_train: {y_train.shape}&quot;)&#10;    print(f&quot;   X_val: {X_val.shape}, y_val: {y_val.shape}&quot;)&#10;    &#10;    return X_train, y_train, X_val, y_val&#10;&#10;&#10;def load_train_val_test(folds_dir=None):&#10;    &quot;&quot;&quot;&#10;    Load Train/Val/Test split&#10;    &#10;    Parameters:&#10;    -----------&#10;    folds_dir : str&#10;        Thư mục chứa folds (None để tự động detect)&#10;    &#10;    Returns:&#10;    --------&#10;    X_train, y_train, X_val, y_val, X_test, y_test&#10;    &quot;&quot;&quot;&#10;    &#10;    if folds_dir is None:&#10;        _, folds_dir = get_paths()&#10;    &#10;    split_dir = os.path.join(folds_dir, 'train_val_test')&#10;    &#10;    X_train = np.load(os.path.join(split_dir, 'X_train.npy'))&#10;    y_train = np.load(os.path.join(split_dir, 'y_train.npy'))&#10;    X_val = np.load(os.path.join(split_dir, 'X_val.npy'))&#10;    y_val = np.load(os.path.join(split_dir, 'y_val.npy'))&#10;    X_test = np.load(os.path.join(split_dir, 'X_test.npy'))&#10;    y_test = np.load(os.path.join(split_dir, 'y_test.npy'))&#10;    &#10;    print(f&quot; Đã load Train/Val/Test split:&quot;)&#10;    print(f&quot;   X_train: {X_train.shape}, y_train: {y_train.shape}&quot;)&#10;    print(f&quot;   X_val: {X_val.shape}, y_val: {y_val.shape}&quot;)&#10;    print(f&quot;   X_test: {X_test.shape}, y_test: {y_test.shape}&quot;)&#10;    &#10;    return X_train, y_train, X_val, y_val, X_test, y_test&#10;&#10;&#10;# =============================================================================&#10;# CHẠY CHƯƠNG TRÌNH&#10;# =============================================================================&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    # Chuẩn bị folds&#10;    folds = prepare_folds(&#10;        n_folds=5,              # Số fold&#10;        reshape_method='1d',    # '1d' cho Conv1D, '2d' cho Conv2D&#10;        create_holdout=True,    # Tạo thêm Train/Val/Test split&#10;        val_size=0.15,          # Tỷ lệ validation&#10;        test_size=0.15          # Tỷ lệ test&#10;    )&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/CNN/preprocess_cicids2018.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/CNN/preprocess_cicids2018.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;=============================================================================&#10;TIỀN XỬ LÝ DATASET CICIDS2018 CHO MÔ HÌNH CNN&#10;Phát hiện lưu lượng mạng IoT bất thường (Binary Classification)&#10;=============================================================================&#10;Tác giả: Auto-generated&#10;Mô tả: &#10;    - Đọc và xử lý từng file CSV trong dataset CICIDS2018&#10;    - Chuyển đổi về binary class (Benign vs Attack)&#10;    - Loại bỏ duplicate, xử lý NaN, Inf&#10;    - Chuẩn hóa dữ liệu và lưu dưới dạng tối ưu (numpy/pickle)&#10;    - Sử dụng chunk để tối ưu bộ nhớ&#10;=============================================================================&#10;&quot;&quot;&quot;&#10;&#10;import os&#10;import numpy as np&#10;import pandas as pd&#10;import pickle&#10;from sklearn.preprocessing import StandardScaler, RobustScaler&#10;from tqdm import tqdm&#10;import warnings&#10;import gc&#10;&#10;warnings.filterwarnings('ignore')&#10;&#10;# =============================================================================&#10;# CẤU HÌNH ĐƯỜNG DẪN - Tự động detect Kaggle hoặc Local&#10;# =============================================================================&#10;def get_paths():&#10;    &quot;&quot;&quot;&#10;    Tự động xác định đường dẫn dựa trên môi trường (Kaggle/Local)&#10;    &quot;&quot;&quot;&#10;    # Kiểm tra nếu đang chạy trên Kaggle&#10;    if os.path.exists('/kaggle/input'):&#10;        # Đường dẫn Kaggle - người dùng cần upload dataset&#10;        DATA_DIR = '/kaggle/input/cicids2018-csv'  # Thay đổi theo tên dataset trên Kaggle&#10;        OUTPUT_DIR = '/kaggle/working/processed_cnn'&#10;        print(&quot; Đang chạy trên KAGGLE&quot;)&#10;    else:&#10;        # Đường dẫn Local&#10;        BASE_DIR = os.path.dirname(os.path.abspath(__file__))&#10;        DATA_DIR = os.path.join(os.path.dirname(BASE_DIR), 'CICIDS2018-CSV')&#10;        OUTPUT_DIR = os.path.join(BASE_DIR, 'processed_data')&#10;        print(&quot; Đang chạy trên LOCAL&quot;)&#10;    &#10;    # Tạo thư mục output nếu chưa tồn tại&#10;    os.makedirs(OUTPUT_DIR, exist_ok=True)&#10;    &#10;    print(f&quot; Thư mục dữ liệu: {DATA_DIR}&quot;)&#10;    print(f&quot; Thư mục output: {OUTPUT_DIR}&quot;)&#10;    &#10;    return DATA_DIR, OUTPUT_DIR&#10;&#10;&#10;# =============================================================================&#10;# DANH SÁCH CÁC FEATURE CẦN LOẠI BỎ&#10;# =============================================================================&#10;# Các feature không cần thiết cho CNN:&#10;# - Timestamp: Thông tin thời gian không liên quan đến pattern traffic&#10;# - Các cột có giá trị hằng số (variance = 0)&#10;# - Các cột trùng lặp thông tin&#10;&#10;COLUMNS_TO_DROP = [&#10;    'Timestamp',           # Thời gian - không cần thiết&#10;    'Fwd Byts/b Avg',      # Thường = 0&#10;    'Fwd Pkts/b Avg',      # Thường = 0&#10;    'Fwd Blk Rate Avg',    # Thường = 0&#10;    'Bwd Byts/b Avg',      # Thường = 0&#10;    'Bwd Pkts/b Avg',      # Thường = 0&#10;    'Bwd Blk Rate Avg',    # Thường = 0&#10;]&#10;&#10;# Cột nhãn&#10;LABEL_COLUMN = 'Label'&#10;&#10;&#10;# =============================================================================&#10;# HÀM XỬ LÝ DỮ LIỆU&#10;# =============================================================================&#10;&#10;def get_csv_files(data_dir):&#10;    &quot;&quot;&quot;&#10;    Lấy danh sách tất cả file CSV trong thư mục&#10;    &quot;&quot;&quot;&#10;    csv_files = []&#10;    for file in os.listdir(data_dir):&#10;        if file.endswith('.csv'):&#10;            csv_files.append(os.path.join(data_dir, file))&#10;    &#10;    print(f&quot;\n Tìm thấy {len(csv_files)} file CSV:&quot;)&#10;    for f in csv_files:&#10;        print(f&quot;   - {os.path.basename(f)}&quot;)&#10;    &#10;    return csv_files&#10;&#10;&#10;def clean_column_names(df):&#10;    &quot;&quot;&quot;&#10;    Làm sạch tên cột: loại bỏ khoảng trắng thừa&#10;    &quot;&quot;&quot;&#10;    df.columns = df.columns.str.strip()&#10;    return df&#10;&#10;&#10;def convert_to_binary_label(label):&#10;    &quot;&quot;&quot;&#10;    Chuyển đổi nhãn multi-class sang binary class&#10;    - Benign -&gt; 0&#10;    - Tất cả các loại Attack -&gt; 1&#10;    &quot;&quot;&quot;&#10;    if isinstance(label, str):&#10;        label = label.strip()&#10;        if label.lower() == 'benign':&#10;            return 0&#10;        else:&#10;            return 1&#10;    return 1  # Mặc định là attack nếu không xác định&#10;&#10;&#10;def process_chunk(chunk, columns_to_drop):&#10;    &quot;&quot;&quot;&#10;    Xử lý từng chunk dữ liệu:&#10;    - Làm sạch tên cột&#10;    - Loại bỏ cột không cần thiết&#10;    - Chuyển đổi nhãn sang binary&#10;    - Xử lý NaN, Inf&#10;    - Loại bỏ duplicate&#10;    &quot;&quot;&quot;&#10;    # Làm sạch tên cột&#10;    chunk = clean_column_names(chunk)&#10;    &#10;    # Kiểm tra và lấy cột Label&#10;    if LABEL_COLUMN not in chunk.columns:&#10;        print(f&quot;⚠️ Không tìm thấy cột '{LABEL_COLUMN}'&quot;)&#10;        return None&#10;    &#10;    # Chuyển đổi nhãn sang binary&#10;    chunk['Label_Binary'] = chunk[LABEL_COLUMN].apply(convert_to_binary_label)&#10;    &#10;    # Loại bỏ cột Label gốc và các cột không cần thiết&#10;    cols_to_remove = [col for col in columns_to_drop + [LABEL_COLUMN] &#10;                      if col in chunk.columns]&#10;    chunk = chunk.drop(columns=cols_to_remove, errors='ignore')&#10;    &#10;    # Lấy các cột số (features)&#10;    feature_cols = chunk.select_dtypes(include=[np.number]).columns.tolist()&#10;    feature_cols = [col for col in feature_cols if col != 'Label_Binary']&#10;    &#10;    # Thay thế Inf bằng NaN, sau đó fill NaN bằng median&#10;    chunk[feature_cols] = chunk[feature_cols].replace([np.inf, -np.inf], np.nan)&#10;    &#10;    # Xử lý NaN - điền bằng 0 (hoặc median nếu cần)&#10;    chunk[feature_cols] = chunk[feature_cols].fillna(0)&#10;    &#10;    # Loại bỏ duplicate&#10;    chunk = chunk.drop_duplicates()&#10;    &#10;    return chunk&#10;&#10;&#10;def process_single_file(file_path, columns_to_drop, chunk_size=100000):&#10;    &quot;&quot;&quot;&#10;    Xử lý một file CSV với chunk&#10;    &quot;&quot;&quot;&#10;    file_name = os.path.basename(file_path)&#10;    print(f&quot;\n{'='*60}&quot;)&#10;    print(f&quot; Đang xử lý: {file_name}&quot;)&#10;    print(f&quot;{'='*60}&quot;)&#10;    &#10;    chunks_list = []&#10;    total_rows = 0&#10;    &#10;    try:&#10;        # Đọc file theo chunk&#10;        chunk_iterator = pd.read_csv(file_path, chunksize=chunk_size, &#10;                                      low_memory=False, encoding='utf-8')&#10;        &#10;        for i, chunk in enumerate(tqdm(chunk_iterator, desc=&quot;Đọc chunks&quot;)):&#10;            processed_chunk = process_chunk(chunk, columns_to_drop)&#10;            if processed_chunk is not None:&#10;                chunks_list.append(processed_chunk)&#10;                total_rows += len(processed_chunk)&#10;            &#10;            # Giải phóng bộ nhớ&#10;            del chunk&#10;            gc.collect()&#10;        &#10;        if chunks_list:&#10;            # Ghép các chunk lại&#10;            df = pd.concat(chunks_list, ignore_index=True)&#10;            &#10;            # Loại bỏ duplicate lần cuối sau khi ghép&#10;            before_dedup = len(df)&#10;            df = df.drop_duplicates()&#10;            after_dedup = len(df)&#10;            &#10;            print(f&quot;✅ Tổng số dòng sau xử lý: {after_dedup:,}&quot;)&#10;            print(f&quot;️ Số dòng duplicate đã loại bỏ: {before_dedup - after_dedup:,}&quot;)&#10;            &#10;            # Thống kê nhãn&#10;            label_counts = df['Label_Binary'].value_counts()&#10;            print(f&quot; Phân bố nhãn:&quot;)&#10;            print(f&quot;   - Benign (0): {label_counts.get(0, 0):,}&quot;)&#10;            print(f&quot;   - Attack (1): {label_counts.get(1, 0):,}&quot;)&#10;            &#10;            return df&#10;        else:&#10;            print(f&quot;❌ Không có dữ liệu hợp lệ trong file&quot;)&#10;            return None&#10;            &#10;    except Exception as e:&#10;        print(f&quot;❌ Lỗi khi xử lý file: {str(e)}&quot;)&#10;        return None&#10;&#10;&#10;def remove_constant_columns(df, feature_cols):&#10;    &quot;&quot;&quot;&#10;    Loại bỏ các cột có giá trị hằng số (variance = 0)&#10;    &quot;&quot;&quot;&#10;    constant_cols = []&#10;    for col in feature_cols:&#10;        if df[col].nunique() &lt;= 1:&#10;            constant_cols.append(col)&#10;    &#10;    if constant_cols:&#10;        print(f&quot;\n️ Loại bỏ {len(constant_cols)} cột hằng số:&quot;)&#10;        for col in constant_cols:&#10;            print(f&quot;   - {col}&quot;)&#10;        df = df.drop(columns=constant_cols)&#10;    &#10;    return df, constant_cols&#10;&#10;&#10;def remove_high_correlation_columns(df, feature_cols, threshold=0.95):&#10;    &quot;&quot;&quot;&#10;    Loại bỏ các cột có tương quan cao (&gt; threshold)&#10;    Giữ lại 1 cột trong mỗi cặp tương quan cao&#10;    &quot;&quot;&quot;&#10;    print(f&quot;\n Kiểm tra tương quan giữa các features (threshold={threshold})...&quot;)&#10;    &#10;    # Tính ma trận tương quan&#10;    corr_matrix = df[feature_cols].corr().abs()&#10;    &#10;    # Lấy tam giác trên của ma trận&#10;    upper_tri = corr_matrix.where(&#10;        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)&#10;    )&#10;    &#10;    # Tìm các cột có tương quan cao&#10;    cols_to_drop = [column for column in upper_tri.columns &#10;                    if any(upper_tri[column] &gt; threshold)]&#10;    &#10;    if cols_to_drop:&#10;        print(f&quot;️ Loại bỏ {len(cols_to_drop)} cột tương quan cao:&quot;)&#10;        for col in cols_to_drop[:10]:  # Chỉ hiển thị 10 cột đầu&#10;            print(f&quot;   - {col}&quot;)&#10;        if len(cols_to_drop) &gt; 10:&#10;            print(f&quot;   ... và {len(cols_to_drop) - 10} cột khác&quot;)&#10;        &#10;        df = df.drop(columns=cols_to_drop)&#10;    &#10;    return df, cols_to_drop&#10;&#10;&#10;def normalize_features(X, scaler=None):&#10;    &quot;&quot;&quot;&#10;    Chuẩn hóa features sử dụng RobustScaler&#10;    RobustScaler ít bị ảnh hưởng bởi outliers&#10;    &quot;&quot;&quot;&#10;    if scaler is None:&#10;        scaler = RobustScaler()&#10;        X_normalized = scaler.fit_transform(X)&#10;    else:&#10;        X_normalized = scaler.transform(X)&#10;    &#10;    return X_normalized, scaler&#10;&#10;&#10;def reshape_for_cnn(X, method='1d'):&#10;    &quot;&quot;&quot;&#10;    Reshape dữ liệu cho CNN&#10;    &#10;    method='1d': Reshape thành (samples, features, 1) cho Conv1D&#10;    method='2d': Reshape thành hình vuông cho Conv2D (nếu có thể)&#10;    &quot;&quot;&quot;&#10;    n_samples, n_features = X.shape&#10;    &#10;    if method == '1d':&#10;        # Reshape cho Conv1D: (samples, features, 1)&#10;        X_reshaped = X.reshape(n_samples, n_features, 1)&#10;        print(f&quot; Reshape cho Conv1D: {X.shape} -&gt; {X_reshaped.shape}&quot;)&#10;        &#10;    elif method == '2d':&#10;        # Tìm kích thước hình vuông gần nhất&#10;        sqrt_features = int(np.ceil(np.sqrt(n_features)))&#10;        padded_features = sqrt_features ** 2&#10;        &#10;        # Padding nếu cần&#10;        if padded_features &gt; n_features:&#10;            padding = np.zeros((n_samples, padded_features - n_features))&#10;            X_padded = np.hstack([X, padding])&#10;        else:&#10;            X_padded = X&#10;        &#10;        # Reshape cho Conv2D: (samples, height, width, 1)&#10;        X_reshaped = X_padded.reshape(n_samples, sqrt_features, sqrt_features, 1)&#10;        print(f&quot; Reshape cho Conv2D: {X.shape} -&gt; {X_reshaped.shape}&quot;)&#10;    &#10;    else:&#10;        X_reshaped = X&#10;    &#10;    return X_reshaped&#10;&#10;&#10;# =============================================================================&#10;# HÀM CHÍNH - TIỀN XỬ LÝ TOÀN BỘ DATASET&#10;# =============================================================================&#10;&#10;def preprocess_dataset(chunk_size=100000, remove_high_corr=True, corr_threshold=0.95):&#10;    &quot;&quot;&quot;&#10;    Hàm chính để tiền xử lý toàn bộ dataset CICIDS2018&#10;    &#10;    Parameters:&#10;    -----------&#10;    chunk_size : int&#10;        Kích thước mỗi chunk khi đọc file&#10;    remove_high_corr : bool&#10;        Có loại bỏ các cột tương quan cao không&#10;    corr_threshold : float&#10;        Ngưỡng tương quan để loại bỏ (mặc định 0.95)&#10;    &#10;    Returns:&#10;    --------&#10;    Lưu các file sau vào thư mục output:&#10;        - X_processed.npy: Features đã chuẩn hóa&#10;        - y_binary.npy: Nhãn binary (0: Benign, 1: Attack)&#10;        - scaler.pkl: Scaler để transform dữ liệu mới&#10;        - feature_names.txt: Tên các features&#10;        - metadata.pkl: Thông tin metadata&#10;    &quot;&quot;&quot;&#10;    &#10;    print(&quot;=&quot;*70)&#10;    print(&quot; BẮT ĐẦU TIỀN XỬ LÝ DATASET CICIDS2018&quot;)&#10;    print(&quot;=&quot;*70)&#10;    &#10;    # Lấy đường dẫn&#10;    DATA_DIR, OUTPUT_DIR = get_paths()&#10;    &#10;    # Lấy danh sách file CSV&#10;    csv_files = get_csv_files(DATA_DIR)&#10;    &#10;    if not csv_files:&#10;        print(&quot;❌ Không tìm thấy file CSV nào!&quot;)&#10;        return&#10;    &#10;    # Xử lý từng file&#10;    all_data = []&#10;    &#10;    for file_path in csv_files:&#10;        df = process_single_file(file_path, COLUMNS_TO_DROP, chunk_size)&#10;        if df is not None:&#10;            all_data.append(df)&#10;        &#10;        # Giải phóng bộ nhớ&#10;        gc.collect()&#10;    &#10;    if not all_data:&#10;        print(&quot;❌ Không có dữ liệu để xử lý!&quot;)&#10;        return&#10;    &#10;    # Ghép tất cả dữ liệu&#10;    print(&quot;\n&quot; + &quot;=&quot;*70)&#10;    print(&quot; GHÉP VÀ XỬ LÝ TOÀN BỘ DỮ LIỆU&quot;)&#10;    print(&quot;=&quot;*70)&#10;    &#10;    full_df = pd.concat(all_data, ignore_index=True)&#10;    del all_data&#10;    gc.collect()&#10;    &#10;    print(f&quot; Tổng số dòng trước khi loại duplicate: {len(full_df):,}&quot;)&#10;    &#10;    # Loại bỏ duplicate toàn cục&#10;    full_df = full_df.drop_duplicates()&#10;    print(f&quot; Tổng số dòng sau khi loại duplicate: {len(full_df):,}&quot;)&#10;    &#10;    # Tách features và labels&#10;    y = full_df['Label_Binary'].values&#10;    X_df = full_df.drop(columns=['Label_Binary'])&#10;    feature_cols = X_df.columns.tolist()&#10;    &#10;    del full_df&#10;    gc.collect()&#10;    &#10;    # Loại bỏ cột hằng số&#10;    X_df, constant_cols = remove_constant_columns(X_df, feature_cols)&#10;    feature_cols = X_df.columns.tolist()&#10;    &#10;    # Loại bỏ cột tương quan cao (tùy chọn)&#10;    if remove_high_corr:&#10;        X_df, high_corr_cols = remove_high_correlation_columns(&#10;            X_df, feature_cols, corr_threshold&#10;        )&#10;        feature_cols = X_df.columns.tolist()&#10;    else:&#10;        high_corr_cols = []&#10;    &#10;    print(f&quot;\n Số features cuối cùng: {len(feature_cols)}&quot;)&#10;    &#10;    # Chuyển sang numpy array&#10;    X = X_df.values.astype(np.float32)&#10;    del X_df&#10;    gc.collect()&#10;    &#10;    # Xử lý NaN và Inf còn sót (nếu có)&#10;    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)&#10;    &#10;    # Chuẩn hóa features&#10;    print(&quot;\n Đang chuẩn hóa features...&quot;)&#10;    X_normalized, scaler = normalize_features(X)&#10;    &#10;    # Thống kê cuối cùng&#10;    print(&quot;\n&quot; + &quot;=&quot;*70)&#10;    print(&quot; THỐNG KÊ CUỐI CÙNG&quot;)&#10;    print(&quot;=&quot;*70)&#10;    print(f&quot;Kích thước X: {X_normalized.shape}&quot;)&#10;    print(f&quot;Kích thước y: {y.shape}&quot;)&#10;    print(f&quot;Số features: {len(feature_cols)}&quot;)&#10;    print(f&quot;Số mẫu Benign (0): {np.sum(y == 0):,}&quot;)&#10;    print(f&quot;Số mẫu Attack (1): {np.sum(y == 1):,}&quot;)&#10;    print(f&quot;Tỷ lệ Attack: {np.mean(y) * 100:.2f}%&quot;)&#10;    &#10;    # Lưu dữ liệu&#10;    print(&quot;\n Đang lưu dữ liệu...&quot;)&#10;    &#10;    # Lưu features và labels&#10;    np.save(os.path.join(OUTPUT_DIR, 'X_processed.npy'), X_normalized)&#10;    np.save(os.path.join(OUTPUT_DIR, 'y_binary.npy'), y)&#10;    &#10;    # Lưu scaler&#10;    with open(os.path.join(OUTPUT_DIR, 'scaler.pkl'), 'wb') as f:&#10;        pickle.dump(scaler, f)&#10;    &#10;    # Lưu tên features&#10;    with open(os.path.join(OUTPUT_DIR, 'feature_names.txt'), 'w') as f:&#10;        for name in feature_cols:&#10;            f.write(f&quot;{name}\n&quot;)&#10;    &#10;    # Lưu metadata&#10;    metadata = {&#10;        'n_samples': X_normalized.shape[0],&#10;        'n_features': X_normalized.shape[1],&#10;        'feature_names': feature_cols,&#10;        'columns_dropped': COLUMNS_TO_DROP,&#10;        'constant_columns_removed': constant_cols,&#10;        'high_corr_columns_removed': high_corr_cols,&#10;        'n_benign': int(np.sum(y == 0)),&#10;        'n_attack': int(np.sum(y == 1)),&#10;        'attack_ratio': float(np.mean(y)),&#10;    }&#10;    &#10;    with open(os.path.join(OUTPUT_DIR, 'metadata.pkl'), 'wb') as f:&#10;        pickle.dump(metadata, f)&#10;    &#10;    print(f&quot;\n✅ ĐÃ LƯU DỮ LIỆU VÀO: {OUTPUT_DIR}&quot;)&#10;    print(&quot;   - X_processed.npy&quot;)&#10;    print(&quot;   - y_binary.npy&quot;)&#10;    print(&quot;   - scaler.pkl&quot;)&#10;    print(&quot;   - feature_names.txt&quot;)&#10;    print(&quot;   - metadata.pkl&quot;)&#10;    &#10;    print(&quot;\n&quot; + &quot;=&quot;*70)&#10;    print(&quot; HOÀN THÀNH TIỀN XỬ LÝ!&quot;)&#10;    print(&quot;=&quot;*70)&#10;    &#10;    return X_normalized, y, scaler, feature_cols, metadata&#10;&#10;&#10;# =============================================================================&#10;# CHẠY CHƯƠNG TRÌNH&#10;# =============================================================================&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    # Chạy tiền xử lý&#10;    result = preprocess_dataset(&#10;        chunk_size=100000,      # Kích thước chunk&#10;        remove_high_corr=True,  # Loại bỏ cột tương quan cao&#10;        corr_threshold=0.95     # Ngưỡng tương quan&#10;    )&#10;    &#10;    if result:&#10;        X, y, scaler, feature_names, metadata = result&#10;        print(f&quot;\n Shape của X: {X.shape}&quot;)&#10;        print(f&quot; Shape của y: {y.shape}&quot;)&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/CNN/simplify_comments.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/CNN/simplify_comments.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;Script tự động rút gọn comment và print trong step3_train_cnn.py&quot;&quot;&quot;&#10;&#10;import re&#10;&#10;# Đọc file&#10;with open('step3_train_cnn.py', 'r', encoding='utf-8') as f:&#10;    content = f.read()&#10;&#10;# Các pattern cần thay thế&#10;replacements = [&#10;    # Loại bỏ các comment dài dòng không cần thiết&#10;    (r'# =+.*=+\n', ''),&#10;    (r'    # Bước \d+:', '   '),&#10;    &#10;    # Rút gọn print statements&#10;    (r'print\(&quot;   ✅ Đã (load|lưu|tạo|train).*&quot;\)', ''),&#10;    (r'print\(f&quot;   ✅.*&quot;\)', ''),&#10;    (r'print\(&quot;\n   .*&quot;\)', ''),&#10;]&#10;&#10;for pattern, replacement in replacements:&#10;    content = re.sub(pattern, replacement, content)&#10;&#10;# Ghi lại file&#10;with open('step3_train_cnn_simplified.py', 'w', encoding='utf-8') as f:&#10;    f.write(content)&#10;&#10;print(&quot;✅ Đã tạo step3_train_cnn_simplified.py&quot;)&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/CNN/step1_clean_data.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/CNN/step1_clean_data.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;======================================================================================&#10;BƯỚC 1: CLEAN VÀ TIỀN XỬ LÝ DATASET CICIDS2018 CHO MÔ HÌNH CNN&#10;======================================================================================&#10;&#10;Script này thực hiện các bước tiền xử lý dữ liệu:&#10;1. Đọc từng file CSV theo chunks để tối ưu bộ nhớ&#10;2. Loại bỏ các cột không cần thiết (IP, Port, Timestamp, Flow ID)&#10;3. Loại bỏ các cột có variance = 0 (zero-variance columns)&#10;4. Xử lý Infinity và NaN bằng Mode của cột&#10;5. Loại bỏ các hàng trùng lặp&#10;6. Chuyển đổi nhãn sang dạng binary (Benign=0, Attack=1)&#10;7. Lưu dữ liệu đã clean vào folder để sử dụng sau&#10;&#10;Có thể chạy trên cả Kaggle và Local&#10;&quot;&quot;&quot;&#10;&#10;import os&#10;import numpy as np&#10;import pandas as pd&#10;import pickle&#10;import json&#10;import gc&#10;from pathlib import Path&#10;from datetime import datetime&#10;import warnings&#10;warnings.filterwarnings('ignore')&#10;&#10;# Kiểm tra môi trường chạy (Kaggle hoặc Local)&#10;IS_KAGGLE = os.path.exists('/kaggle/input')&#10;&#10;# Progress bar&#10;try:&#10;    from tqdm import tqdm&#10;    TQDM_AVAILABLE = True&#10;except ImportError:&#10;    TQDM_AVAILABLE = False&#10;    print(&quot;⚠️  tqdm không có sẵn. Cài đặt bằng: pip install tqdm&quot;)&#10;    tqdm = lambda x, **kwargs: x&#10;&#10;# ============================================================================&#10;# CẤU HÌNH ĐƯỜNG DẪN&#10;# ============================================================================&#10;if IS_KAGGLE:&#10;    # Đường dẫn trên Kaggle - thay đổi theo dataset của bạn&#10;    DATA_DIR = &quot;/kaggle/input/cicids2018&quot;  # Thay đổi nếu tên dataset khác&#10;    OUTPUT_DIR = &quot;/kaggle/working/cleaned_data&quot;&#10;    print(&quot; Đang chạy trên KAGGLE&quot;)&#10;else:&#10;    # Đường dẫn Local&#10;    DATA_DIR = r&quot;D:\PROJECT\Machine Learning\IOT\CICIDS2018-CSV&quot;&#10;    OUTPUT_DIR = r&quot;D:\PROJECT\Machine Learning\IOT\CNN\cleaned_data&quot;&#10;    print(&quot; Đang chạy trên LOCAL&quot;)&#10;&#10;# ============================================================================&#10;# CẤU HÌNH XỬ LÝ DỮ LIỆU&#10;# ============================================================================&#10;&#10;# Kích thước chunk khi đọc CSV (điều chỉnh theo RAM của máy)&#10;CHUNK_SIZE = 300000  # 300k rows mỗi chunk&#10;&#10;# Random state để tái tạo kết quả&#10;RANDOM_STATE = 42&#10;&#10;# ============================================================================&#10;# DANH SÁCH CÁC CỘT CẦN LOẠI BỎ (Identification columns)&#10;# ============================================================================&#10;&#10;COLUMNS_TO_DROP = [&#10;    'Flow ID',          # ID duy nhất cho mỗi flow - không có ý nghĩa phân loại&#10;    'Src IP',           # IP nguồn - không tổng quát&#10;    'Dst IP',           # IP đích - không tổng quát&#10;    'Src Port',         # Port nguồn - có thể bị overfitting&#10;    'Dst Port',         # Port đích - có thể bị overfitting&#10;    'Timestamp',        # Thời gian - không liên quan đến pattern tấn công&#10;]&#10;&#10;# Cột nhãn&#10;LABEL_COLUMN = 'Label'&#10;&#10;# ============================================================================&#10;# CLASS XỬ LÝ DỮ LIỆU&#10;# ============================================================================&#10;&#10;class CICIDS2018_DataCleaner:&#10;    &quot;&quot;&quot;&#10;    Class clean dữ liệu CICIDS2018 cho mô hình CNN&#10;&#10;    Các bước xử lý:&#10;    1. Đọc dữ liệu theo chunks&#10;    2. Loại bỏ cột identification&#10;    3. Loại bỏ zero-variance columns&#10;    4. Xử lý Infinity và NaN bằng Mode&#10;    5. Loại bỏ duplicate&#10;    6. Chuyển đổi nhãn sang binary&#10;    7. Lưu dữ liệu đã clean&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, data_dir, output_dir, chunk_size=CHUNK_SIZE):&#10;        &quot;&quot;&quot;&#10;        Khởi tạo data cleaner&#10;&#10;        Args:&#10;            data_dir: Đường dẫn thư mục chứa file CSV&#10;            output_dir: Đường dẫn thư mục lưu kết quả&#10;            chunk_size: Số dòng mỗi chunk khi đọc CSV&#10;        &quot;&quot;&quot;&#10;        self.data_dir = Path(data_dir)&#10;        self.output_dir = Path(output_dir)&#10;        self.chunk_size = chunk_size&#10;&#10;        # Tạo thư mục output nếu chưa có&#10;        self.output_dir.mkdir(parents=True, exist_ok=True)&#10;&#10;        # Lưu tên các features và thông tin xử lý&#10;        self.feature_names = None&#10;        self.zero_variance_cols = []&#10;        self.column_modes = {}  # Lưu mode của từng cột để xử lý NaN/Inf&#10;&#10;        # Thống kê&#10;        self.stats = {&#10;            'total_rows_read': 0,&#10;            'rows_after_cleaning': 0,&#10;            'duplicates_removed': 0,&#10;            'nan_replaced': 0,&#10;            'inf_replaced': 0,&#10;            'zero_variance_cols_removed': 0,&#10;            'benign_count': 0,&#10;            'attack_count': 0,&#10;            'feature_count': 0,&#10;            'processing_time': 0.0  # Float để lưu thời gian xử lý (giây)&#10;        }&#10;&#10;    def _get_csv_files(self):&#10;        &quot;&quot;&quot;Lấy danh sách các file CSV trong thư mục data&quot;&quot;&quot;&#10;        csv_files = list(self.data_dir.glob(&quot;*_TrafficForML_CICFlowMeter.csv&quot;))&#10;        if not csv_files:&#10;            # Thử pattern khác cho Kaggle&#10;            csv_files = list(self.data_dir.glob(&quot;*.csv&quot;))&#10;            # Loại bỏ file zip nếu có&#10;            csv_files = [f for f in csv_files if not f.name.endswith('.zip')]&#10;&#10;        if not csv_files:&#10;            raise FileNotFoundError(f&quot;Không tìm thấy file CSV trong {self.data_dir}&quot;)&#10;&#10;        print(f&quot;\n Tìm thấy {len(csv_files)} file CSV:&quot;)&#10;        for f in sorted(csv_files):&#10;            print(f&quot;   - {f.name}&quot;)&#10;        return sorted(csv_files)&#10;&#10;    def _clean_column_names(self, df):&#10;        &quot;&quot;&quot;Chuẩn hóa tên cột (loại bỏ khoảng trắng thừa)&quot;&quot;&quot;&#10;        df.columns = df.columns.str.strip()&#10;        return df&#10;&#10;    def _drop_identification_columns(self, df):&#10;        &quot;&quot;&quot;Loại bỏ các cột identification không cần thiết cho huấn luyện&quot;&quot;&quot;&#10;        columns_to_drop = [col for col in COLUMNS_TO_DROP if col in df.columns]&#10;        if columns_to_drop:&#10;            df = df.drop(columns=columns_to_drop)&#10;        return df&#10;&#10;    def _convert_to_numeric(self, df):&#10;        &quot;&quot;&quot;Chuyển đổi các cột về dạng số&quot;&quot;&quot;&#10;        feature_cols = [col for col in df.columns if col != LABEL_COLUMN]&#10;        for col in feature_cols:&#10;            if df[col].dtype == 'object':&#10;                df[col] = pd.to_numeric(df[col], errors='coerce')&#10;        return df&#10;&#10;    def _convert_to_binary_label(self, df):&#10;        &quot;&quot;&quot;&#10;        Chuyển đổi nhãn sang dạng binary:&#10;        - Benign -&gt; 0 (lưu lượng bình thường)&#10;        - Tất cả các loại tấn công khác -&gt; 1 (lưu lượng bất thường)&#10;        &quot;&quot;&quot;&#10;        if LABEL_COLUMN not in df.columns:&#10;            raise ValueError(f&quot;Không tìm thấy cột '{LABEL_COLUMN}' trong dữ liệu&quot;)&#10;&#10;        # Chuẩn hóa nhãn (loại bỏ khoảng trắng, lowercase)&#10;        df[LABEL_COLUMN] = df[LABEL_COLUMN].astype(str).str.strip().str.lower()&#10;&#10;        # Loại bỏ các hàng có nhãn là 'label' (header bị lẫn vào data)&#10;        df = df[df[LABEL_COLUMN] != 'label']&#10;&#10;        # Chuyển đổi sang binary: Benign=0, Attack=1&#10;        df['binary_label'] = (df[LABEL_COLUMN] != 'benign').astype(int)&#10;&#10;        # Xóa cột Label gốc, giữ lại binary_label&#10;        df = df.drop(columns=[LABEL_COLUMN])&#10;&#10;        return df&#10;&#10;    def _first_pass_collect_info(self, csv_files):&#10;        &quot;&quot;&quot;&#10;        Lần đọc đầu tiên: Thu thập thông tin về columns và tính mode&#10;&#10;        Mục đích:&#10;        - Xác định các cột có variance = 0&#10;        - Tính mode của từng cột để thay thế NaN/Inf&#10;        &quot;&quot;&quot;&#10;        print(&quot;\n&quot; + &quot;=&quot;*80)&#10;        print(&quot; BƯỚC 1: THU THẬP THÔNG TIN TỪ DỮ LIỆU&quot;)&#10;        print(&quot;=&quot;*80)&#10;&#10;        all_columns = None&#10;        column_value_counts = {}  # Để tính mode&#10;        column_min_max = {}  # Để kiểm tra variance&#10;&#10;        for csv_file in csv_files:&#10;            print(f&quot;\n   Đang scan: {csv_file.name}&quot;)&#10;            chunk_iterator = pd.read_csv(csv_file, chunksize=self.chunk_size,&#10;                                        low_memory=False, encoding='utf-8')&#10;&#10;            for chunk in chunk_iterator:&#10;                chunk = self._clean_column_names(chunk)&#10;                chunk = self._drop_identification_columns(chunk)&#10;                chunk = self._convert_to_numeric(chunk)&#10;&#10;                if all_columns is None:&#10;                    all_columns = [col for col in chunk.columns if col != LABEL_COLUMN]&#10;                    for col in all_columns:&#10;                        column_value_counts[col] = {}&#10;                        column_min_max[col] = {'min': np.inf, 'max': -np.inf}&#10;&#10;                # Thu thập thông tin cho mỗi cột&#10;                for col in all_columns:&#10;                    if col in chunk.columns:&#10;                        # Thay thế inf trước khi tính&#10;                        col_data = chunk[col].replace([np.inf, -np.inf], np.nan)&#10;                        valid_data = col_data.dropna()&#10;&#10;                        if len(valid_data) &gt; 0:&#10;                            # Cập nhật min/max&#10;                            col_min = valid_data.min()&#10;                            col_max = valid_data.max()&#10;                            column_min_max[col]['min'] = min(column_min_max[col]['min'], col_min)&#10;                            column_min_max[col]['max'] = max(column_min_max[col]['max'], col_max)&#10;&#10;                            # Thu thập value counts cho mode (lấy top 10 để tiết kiệm bộ nhớ)&#10;                            vc = valid_data.value_counts().head(10).to_dict()&#10;                            for val, count in vc.items():&#10;                                if val not in column_value_counts[col]:&#10;                                    column_value_counts[col][val] = 0&#10;                                column_value_counts[col][val] += count&#10;&#10;                gc.collect()&#10;&#10;        # Xác định zero-variance columns&#10;        print(&quot;\n   Đang phân tích variance của các cột...&quot;)&#10;        for col in all_columns:&#10;            if column_min_max[col]['min'] == column_min_max[col]['max']:&#10;                self.zero_variance_cols.append(col)&#10;&#10;        # Tính mode cho mỗi cột&#10;        print(&quot;   Đang tính mode cho mỗi cột...&quot;)&#10;        for col in all_columns:&#10;            if col not in self.zero_variance_cols:&#10;                if column_value_counts[col]:&#10;                    # Mode là giá trị xuất hiện nhiều nhất&#10;                    mode_val = max(column_value_counts[col], key=column_value_counts[col].get)&#10;                    self.column_modes[col] = mode_val&#10;                else:&#10;                    self.column_modes[col] = 0  # Fallback nếu không có dữ liệu hợp lệ&#10;&#10;        self.stats['zero_variance_cols_removed'] = len(self.zero_variance_cols)&#10;&#10;        print(f&quot;\n   ✅ Số cột zero-variance sẽ loại bỏ: {len(self.zero_variance_cols)}&quot;)&#10;        if self.zero_variance_cols:&#10;            print(f&quot;      Các cột: {self.zero_variance_cols}&quot;)&#10;        print(f&quot;   ✅ Số cột sẽ giữ lại: {len(all_columns) - len(self.zero_variance_cols)}&quot;)&#10;&#10;        return all_columns&#10;&#10;    def _handle_nan_inf_with_mode(self, df):&#10;        &quot;&quot;&quot;&#10;        Xử lý NaN và Infinity bằng Mode của cột&#10;&#10;        Replace Infinity and NaN with the Mode of the column&#10;        &quot;&quot;&quot;&#10;        feature_cols = [col for col in df.columns if col != 'binary_label']&#10;&#10;        for col in feature_cols:&#10;            if col in self.column_modes:&#10;                mode_val = self.column_modes[col]&#10;&#10;                # Đếm số lượng inf và nan&#10;                inf_mask = np.isinf(df[col])&#10;                nan_mask = df[col].isna()&#10;&#10;                self.stats['inf_replaced'] += inf_mask.sum()&#10;                self.stats['nan_replaced'] += nan_mask.sum()&#10;&#10;                # Thay thế inf bằng nan trước&#10;                df[col] = df[col].replace([np.inf, -np.inf], np.nan)&#10;&#10;                # Thay thế tất cả nan bằng mode&#10;                df[col] = df[col].fillna(mode_val)&#10;&#10;        return df&#10;&#10;    def _drop_zero_variance_columns(self, df):&#10;        &quot;&quot;&quot;Loại bỏ các cột có variance = 0&quot;&quot;&quot;&#10;        cols_to_drop = [col for col in self.zero_variance_cols if col in df.columns]&#10;        if cols_to_drop:&#10;            df = df.drop(columns=cols_to_drop)&#10;        return df&#10;&#10;    def _process_single_file(self, csv_file):&#10;        &quot;&quot;&quot;&#10;        Xử lý một file CSV theo chunks&#10;&#10;        Args:&#10;            csv_file: Đường dẫn file CSV&#10;&#10;        Returns:&#10;            DataFrame đã được xử lý&#10;        &quot;&quot;&quot;&#10;        print(f&quot;\n Đang xử lý: {csv_file.name}&quot;)&#10;&#10;        processed_chunks = []&#10;        chunk_iterator = pd.read_csv(csv_file, chunksize=self.chunk_size,&#10;                                     low_memory=False, encoding='utf-8')&#10;&#10;        # Progress bar cho chunks&#10;        if TQDM_AVAILABLE:&#10;            file_size = csv_file.stat().st_size&#10;            estimated_chunks = max(1, file_size // (self.chunk_size * 500))&#10;            chunk_iterator = tqdm(chunk_iterator, desc=&quot;   Chunks&quot;,&#10;                                  total=estimated_chunks, unit=&quot;chunk&quot;)&#10;&#10;        for chunk in chunk_iterator:&#10;            self.stats['total_rows_read'] += len(chunk)&#10;&#10;            # Bước 1: Chuẩn hóa tên cột&#10;            chunk = self._clean_column_names(chunk)&#10;&#10;            # Bước 2: Loại bỏ cột identification&#10;            chunk = self._drop_identification_columns(chunk)&#10;&#10;            # Bước 3: Chuyển đổi sang dạng số&#10;            chunk = self._convert_to_numeric(chunk)&#10;&#10;            # Bước 4: Chuyển đổi nhãn sang binary&#10;            chunk = self._convert_to_binary_label(chunk)&#10;&#10;            # Bước 5: Loại bỏ zero-variance columns&#10;            chunk = self._drop_zero_variance_columns(chunk)&#10;&#10;            # Bước 6: Xử lý NaN và Inf bằng Mode&#10;            chunk = self._handle_nan_inf_with_mode(chunk)&#10;&#10;            processed_chunks.append(chunk)&#10;            gc.collect()&#10;&#10;        # Gộp các chunks lại&#10;        if processed_chunks:&#10;            df = pd.concat(processed_chunks, ignore_index=True)&#10;            del processed_chunks&#10;            gc.collect()&#10;            return df&#10;&#10;        return None&#10;&#10;    def clean_all_files(self):&#10;        &quot;&quot;&quot;&#10;        Clean tất cả các file CSV&#10;&#10;        Returns:&#10;            DataFrame đã clean hoàn chỉnh&#10;        &quot;&quot;&quot;&#10;        start_time = datetime.now()&#10;        print(&quot;\n&quot; + &quot;=&quot;*80)&#10;        print(&quot; BẮT ĐẦU CLEAN DỮ LIỆU CICIDS2018&quot;)&#10;        print(&quot;=&quot;*80)&#10;&#10;        csv_files = self._get_csv_files()&#10;&#10;        # Bước 1: Thu thập thông tin (mode, zero-variance)&#10;        all_columns = self._first_pass_collect_info(csv_files)&#10;&#10;        # Bước 2: Xử lý từng file&#10;        print(&quot;\n&quot; + &quot;=&quot;*80)&#10;        print(&quot; BƯỚC 2: CLEAN DỮ LIỆU&quot;)&#10;        print(&quot;=&quot;*80)&#10;&#10;        all_dataframes = []&#10;        for csv_file in csv_files:&#10;            df = self._process_single_file(csv_file)&#10;            if df is not None:&#10;                all_dataframes.append(df)&#10;                print(f&quot;   ✅ Đã xử lý: {len(df):,} mẫu&quot;)&#10;&#10;        # Gộp tất cả lại&#10;        print(&quot;\n&quot; + &quot;-&quot;*80)&#10;        print(&quot; ĐANG GỘP DỮ LIỆU...&quot;)&#10;&#10;        df_combined = pd.concat(all_dataframes, ignore_index=True)&#10;        del all_dataframes&#10;        gc.collect()&#10;&#10;        print(f&quot;   Tổng số mẫu sau khi gộp: {len(df_combined):,}&quot;)&#10;&#10;        # Loại bỏ duplicate trên toàn bộ dataset&#10;        print(&quot;   Đang loại bỏ duplicate...&quot;)&#10;        rows_before = len(df_combined)&#10;        df_combined = df_combined.drop_duplicates()&#10;        rows_after = len(df_combined)&#10;        self.stats['duplicates_removed'] = rows_before - rows_after&#10;        print(f&quot;   Số mẫu sau khi loại duplicate: {len(df_combined):,}&quot;)&#10;        print(f&quot;   Số duplicate đã loại: {self.stats['duplicates_removed']:,}&quot;)&#10;&#10;        # Đếm số lượng mỗi class&#10;        self.stats['benign_count'] = int((df_combined['binary_label'] == 0).sum())&#10;        self.stats['attack_count'] = int((df_combined['binary_label'] == 1).sum())&#10;&#10;        # Cập nhật thống kê&#10;        self.stats['rows_after_cleaning'] = len(df_combined)&#10;        self.stats['feature_count'] = len(df_combined.columns) - 1  # Trừ cột label&#10;&#10;        # Lưu tên features&#10;        self.feature_names = [col for col in df_combined.columns if col != 'binary_label']&#10;&#10;        end_time = datetime.now()&#10;        self.stats['processing_time'] = (end_time - start_time).total_seconds()&#10;&#10;        return df_combined&#10;&#10;    def save_cleaned_data(self, df):&#10;        &quot;&quot;&quot;&#10;        Lưu dữ liệu đã clean&#10;&#10;        Lưu thành các file:&#10;        - cleaned_data.parquet (dữ liệu đã clean, chưa normalize)&#10;        - feature_names.txt&#10;        - cleaning_metadata.json&#10;        &quot;&quot;&quot;&#10;        print(&quot;\n&quot; + &quot;=&quot;*80)&#10;        print(&quot; ĐANG LƯU DỮ LIỆU ĐÃ CLEAN...&quot;)&#10;        print(&quot;=&quot;*80)&#10;&#10;        # Lưu dữ liệu dạng parquet (nhanh và nhỏ gọn)&#10;        parquet_path = self.output_dir / 'cleaned_data.parquet'&#10;        df.to_parquet(parquet_path, index=False)&#10;        print(f&quot;   ✅ Đã lưu: {parquet_path}&quot;)&#10;        print(f&quot;      Kích thước file: {parquet_path.stat().st_size / (1024*1024):.2f} MB&quot;)&#10;&#10;        # Cũng lưu dạng CSV để dễ kiểm tra (optional, có thể comment nếu file quá lớn)&#10;        # csv_path = self.output_dir / 'cleaned_data.csv'&#10;        # df.to_csv(csv_path, index=False)&#10;        # print(f&quot;   ✅ Đã lưu: {csv_path}&quot;)&#10;&#10;        # Lưu feature names&#10;        with open(self.output_dir / 'feature_names.txt', 'w') as f:&#10;            for name in self.feature_names:&#10;                f.write(name + '\n')&#10;        print(f&quot;   ✅ Đã lưu: feature_names.txt&quot;)&#10;&#10;        # Lưu column modes (để có thể sử dụng cho dữ liệu mới)&#10;        with open(self.output_dir / 'column_modes.pkl', 'wb') as f:&#10;            pickle.dump(self.column_modes, f)&#10;        print(f&quot;   ✅ Đã lưu: column_modes.pkl&quot;)&#10;&#10;        # Lưu zero-variance columns&#10;        with open(self.output_dir / 'zero_variance_cols.pkl', 'wb') as f:&#10;            pickle.dump(self.zero_variance_cols, f)&#10;        print(f&quot;   ✅ Đã lưu: zero_variance_cols.pkl&quot;)&#10;&#10;        # Chuyển đổi stats sang kiểu Python native&#10;        stats_native = {}&#10;        for key, value in self.stats.items():&#10;            if hasattr(value, 'item'):&#10;                stats_native[key] = value.item()&#10;            elif isinstance(value, (np.integer, np.floating)):&#10;                stats_native[key] = int(value) if isinstance(value, np.integer) else float(value)&#10;            else:&#10;                stats_native[key] = value&#10;&#10;        # Lưu metadata&#10;        metadata = {&#10;            'n_features': len(self.feature_names),&#10;            'feature_names': self.feature_names,&#10;            'total_samples': int(len(df)),&#10;            'benign_count': self.stats['benign_count'],&#10;            'attack_count': self.stats['attack_count'],&#10;            'benign_ratio': self.stats['benign_count'] / len(df),&#10;            'attack_ratio': self.stats['attack_count'] / len(df),&#10;            'zero_variance_cols': self.zero_variance_cols,&#10;            'stats': stats_native,&#10;            'created_at': datetime.now().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;)&#10;        }&#10;&#10;        with open(self.output_dir / 'cleaning_metadata.json', 'w', encoding='utf-8') as f:&#10;            json.dump(metadata, f, indent=4, ensure_ascii=False)&#10;        print(f&quot;   ✅ Đã lưu: cleaning_metadata.json&quot;)&#10;&#10;        print(f&quot;\n Tất cả file được lưu tại: {self.output_dir}&quot;)&#10;&#10;    def print_summary(self):&#10;        &quot;&quot;&quot;In tóm tắt quá trình xử lý&quot;&quot;&quot;&#10;        print(&quot;\n&quot; + &quot;=&quot;*80)&#10;        print(&quot; TÓM TẮT CLEAN DỮ LIỆU&quot;)&#10;        print(&quot;=&quot;*80)&#10;        print(f&quot;   Tổng số dòng đọc được:        {self.stats['total_rows_read']:,}&quot;)&#10;        print(f&quot;   Số dòng sau khi clean:        {self.stats['rows_after_cleaning']:,}&quot;)&#10;        print(f&quot;   Số duplicate đã loại:         {self.stats['duplicates_removed']:,}&quot;)&#10;        print(f&quot;   Số NaN đã thay bằng mode:     {self.stats['nan_replaced']:,}&quot;)&#10;        print(f&quot;   Số Inf đã thay bằng mode:     {self.stats['inf_replaced']:,}&quot;)&#10;        print(f&quot;   Số cột zero-variance đã loại: {self.stats['zero_variance_cols_removed']}&quot;)&#10;        print(f&quot;   Số features còn lại:          {self.stats['feature_count']}&quot;)&#10;        print(f&quot;\n    PHÂN BỐ NHÃN:&quot;)&#10;        print(f&quot;   Số mẫu Benign (0):  {self.stats['benign_count']:,} ({self.stats['benign_count']/self.stats['rows_after_cleaning']*100:.1f}%)&quot;)&#10;        print(f&quot;   Số mẫu Attack (1):  {self.stats['attack_count']:,} ({self.stats['attack_count']/self.stats['rows_after_cleaning']*100:.1f}%)&quot;)&#10;        print(f&quot;\n   Thời gian xử lý: {self.stats['processing_time']:.2f} giây&quot;)&#10;        print(&quot;=&quot;*80)&#10;&#10;&#10;def main():&#10;    &quot;&quot;&quot;Hàm chính để chạy cleaning&quot;&quot;&quot;&#10;&#10;    print(&quot;\n&quot; + &quot;=&quot;*80)&#10;    print(&quot; BƯỚC 1: CLEAN DỮ LIỆU CICIDS2018 CHO CNN&quot;)&#10;    print(&quot;   Phát hiện lưu lượng mạng IoT bất thường&quot;)&#10;    print(&quot;=&quot;*80)&#10;&#10;    # Khởi tạo cleaner&#10;    cleaner = CICIDS2018_DataCleaner(&#10;        data_dir=DATA_DIR,&#10;        output_dir=OUTPUT_DIR,&#10;        chunk_size=CHUNK_SIZE&#10;    )&#10;&#10;    # Clean tất cả các file&#10;    df = cleaner.clean_all_files()&#10;&#10;    # Lưu dữ liệu đã clean&#10;    cleaner.save_cleaned_data(df)&#10;&#10;    # In tóm tắt&#10;    cleaner.print_summary()&#10;&#10;    print(&quot;\n✅ HOÀN THÀNH BƯỚC 1!&quot;)&#10;    print(&quot;   Dữ liệu đã được clean và lưu vào folder.&quot;)&#10;    print(&quot;   Chạy step2_prepare_training_data.py để chia train/val/test và cân bằng dữ liệu.&quot;)&#10;&#10;    return cleaner&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    cleaner = main()&#10;&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;BƯỚC 1: CLEAN DỮ LIỆU CICIDS2018 CHO CNN&#10;- Loại bỏ cột không cần thiết, zero-variance columns&#10;- Xử lý NaN/Inf, duplicate&#10;- Chuyển nhãn sang binary (Benign=0, Attack=1)&#10;&quot;&quot;&quot;&#10;&#10;import os&#10;import numpy as np&#10;import pandas as pd&#10;import pickle&#10;import json&#10;import gc&#10;from pathlib import Path&#10;from datetime import datetime&#10;import warnings&#10;warnings.filterwarnings('ignore')&#10;&#10;# Kiểm tra môi trường chạy (Kaggle hoặc Local)&#10;IS_KAGGLE = os.path.exists('/kaggle/input')&#10;&#10;# Progress bar&#10;try:&#10;    from tqdm import tqdm&#10;    TQDM_AVAILABLE = True&#10;except ImportError:&#10;    TQDM_AVAILABLE = False&#10;    print(&quot;⚠️  tqdm không có sẵn. Cài đặt bằng: pip install tqdm&quot;)&#10;    tqdm = lambda x, **kwargs: x&#10;&#10;# Cấu hình đường dẫn&#10;if IS_KAGGLE:&#10;    DATA_DIR = &quot;/kaggle/input/cicids2018&quot;&#10;    OUTPUT_DIR = &quot;/kaggle/working/cleaned_data&quot;&#10;    print(&quot; Kaggle&quot;)&#10;else:&#10;    DATA_DIR = r&quot;D:\PROJECT\Machine Learning\IOT\CICIDS2018-CSV&quot;&#10;    OUTPUT_DIR = r&quot;D:\PROJECT\Machine Learning\IOT\CNN\cleaned_data&quot;&#10;    print(&quot; Local&quot;)&#10;&#10;# Cấu hình xử lý&#10;CHUNK_SIZE = 300000&#10;RANDOM_STATE = 42&#10;&#10;# Cột cần loại bỏ&#10;COLUMNS_TO_DROP = ['Flow ID', 'Src IP', 'Dst IP', 'Src Port', 'Dst Port', 'Timestamp']&#10;LABEL_COLUMN = 'Label'&#10;&#10;&#10;class CICIDS2018_DataCleaner:&#10;    &quot;&quot;&quot;Clean dữ liệu CICIDS2018: loại bỏ cột thừa, xử lý NaN/Inf, duplicate, chuyển binary label&quot;&quot;&quot;&#10;&#10;    def __init__(self, data_dir, output_dir, chunk_size=CHUNK_SIZE):&#10;        self.data_dir = Path(data_dir)&#10;        self.output_dir = Path(output_dir)&#10;        self.chunk_size = chunk_size&#10;        self.output_dir.mkdir(parents=True, exist_ok=True)&#10;        &#10;        self.feature_names = None&#10;        self.zero_variance_cols = []&#10;        self.column_modes = {}&#10;        &#10;        self.stats = {&#10;            'total_rows_read': 0,&#10;            'rows_after_cleaning': 0,&#10;            'duplicates_removed': 0,&#10;            'nan_replaced': 0,&#10;            'inf_replaced': 0,&#10;            'zero_variance_cols_removed': 0,&#10;            'benign_count': 0,&#10;            'attack_count': 0,&#10;            'feature_count': 0,&#10;            'processing_time': 0.0&#10;        }&#10;&#10;    def _get_csv_files(self):&#10;        csv_files = list(self.data_dir.glob(&quot;*_TrafficForML_CICFlowMeter.csv&quot;))&#10;        if not csv_files:&#10;            csv_files = list(self.data_dir.glob(&quot;*.csv&quot;))&#10;            csv_files = [f for f in csv_files if not f.name.endswith('.zip')]&#10;&#10;        if not csv_files:&#10;            raise FileNotFoundError(f&quot;Không tìm thấy file CSV trong {self.data_dir}&quot;)&#10;&#10;        print(f&quot;\n Tìm thấy {len(csv_files)} file CSV&quot;)&#10;        return sorted(csv_files)&#10;&#10;    def _clean_column_names(self, df):&#10;        df.columns = df.columns.str.strip()&#10;        return df&#10;&#10;    def _drop_identification_columns(self, df):&#10;        columns_to_drop = [col for col in COLUMNS_TO_DROP if col in df.columns]&#10;        if columns_to_drop:&#10;            df = df.drop(columns=columns_to_drop)&#10;        return df&#10;&#10;    def _convert_to_numeric(self, df):&#10;        feature_cols = [col for col in df.columns if col != LABEL_COLUMN]&#10;        for col in feature_cols:&#10;            if df[col].dtype == 'object':&#10;                df[col] = pd.to_numeric(df[col], errors='coerce')&#10;        return df&#10;&#10;    def _convert_to_binary_label(self, df):&#10;        if LABEL_COLUMN not in df.columns:&#10;            raise ValueError(f&quot;Không tìm thấy cột '{LABEL_COLUMN}'&quot;)&#10;&#10;        df[LABEL_COLUMN] = df[LABEL_COLUMN].astype(str).str.strip().str.lower()&#10;        df = df[df[LABEL_COLUMN] != 'label']&#10;        df['binary_label'] = (df[LABEL_COLUMN] != 'benign').astype(int)&#10;        df = df.drop(columns=[LABEL_COLUMN])&#10;&#10;        return df&#10;&#10;    def _first_pass_collect_info(self, csv_files):&#10;        print(&quot;\n&quot; + &quot;=&quot;*80)&#10;        print(&quot; THU THẬP THÔNG TIN VÀ TÍNH MODE&quot;)&#10;        print(&quot;=&quot;*80)&#10;&#10;        all_columns = None&#10;        column_value_counts = {}&#10;        column_min_max = {}&#10;&#10;        for csv_file in csv_files:&#10;            chunk_iterator = pd.read_csv(csv_file, chunksize=self.chunk_size,&#10;                                        low_memory=False, encoding='utf-8')&#10;&#10;            for chunk in chunk_iterator:&#10;                chunk = self._clean_column_names(chunk)&#10;                chunk = self._drop_identification_columns(chunk)&#10;                chunk = self._convert_to_numeric(chunk)&#10;&#10;                if all_columns is None:&#10;                    all_columns = [col for col in chunk.columns if col != LABEL_COLUMN]&#10;                    for col in all_columns:&#10;                        column_value_counts[col] = {}&#10;                        column_min_max[col] = {'min': np.inf, 'max': -np.inf}&#10;&#10;                for col in all_columns:&#10;                    if col in chunk.columns:&#10;                        col_data = chunk[col].replace([np.inf, -np.inf], np.nan)&#10;                        valid_data = col_data.dropna()&#10;&#10;                        if len(valid_data) &gt; 0:&#10;                            col_min = valid_data.min()&#10;                            col_max = valid_data.max()&#10;                            column_min_max[col]['min'] = min(column_min_max[col]['min'], col_min)&#10;                            column_min_max[col]['max'] = max(column_min_max[col]['max'], col_max)&#10;&#10;                            vc = valid_data.value_counts().head(10).to_dict()&#10;                            for val, count in vc.items():&#10;                                if val not in column_value_counts[col]:&#10;                                    column_value_counts[col][val] = 0&#10;                                column_value_counts[col][val] += count&#10;&#10;                gc.collect()&#10;&#10;        for col in all_columns:&#10;            if column_min_max[col]['min'] == column_min_max[col]['max']:&#10;                self.zero_variance_cols.append(col)&#10;&#10;        for col in all_columns:&#10;            if col not in self.zero_variance_cols:&#10;                if column_value_counts[col]:&#10;                    mode_val = max(column_value_counts[col], key=column_value_counts[col].get)&#10;                    self.column_modes[col] = mode_val&#10;                else:&#10;                    self.column_modes[col] = 0&#10;&#10;        self.stats['zero_variance_cols_removed'] = len(self.zero_variance_cols)&#10;        print(f&quot;   Loại bỏ {len(self.zero_variance_cols)} cột zero-variance&quot;)&#10;        print(f&quot;   Giữ lại {len(all_columns) - len(self.zero_variance_cols)} features&quot;)&#10;&#10;        return all_columns&#10;&#10;    def _handle_nan_inf_with_mode(self, df):&#10;        feature_cols = [col for col in df.columns if col != 'binary_label']&#10;&#10;        for col in feature_cols:&#10;            if col in self.column_modes:&#10;                mode_val = self.column_modes[col]&#10;&#10;                inf_mask = np.isinf(df[col])&#10;                nan_mask = df[col].isna()&#10;&#10;                self.stats['inf_replaced'] += inf_mask.sum()&#10;                self.stats['nan_replaced'] += nan_mask.sum()&#10;&#10;                df[col] = df[col].replace([np.inf, -np.inf], np.nan)&#10;                df[col] = df[col].fillna(mode_val)&#10;&#10;        return df&#10;&#10;    def _drop_zero_variance_columns(self, df):&#10;        cols_to_drop = [col for col in self.zero_variance_cols if col in df.columns]&#10;        if cols_to_drop:&#10;            df = df.drop(columns=cols_to_drop)&#10;        return df&#10;&#10;    def _process_single_file(self, csv_file):&#10;        print(f&quot;\n {csv_file.name}&quot;)&#10;&#10;        processed_chunks = []&#10;        chunk_iterator = pd.read_csv(csv_file, chunksize=self.chunk_size,&#10;                                     low_memory=False, encoding='utf-8')&#10;&#10;        if TQDM_AVAILABLE:&#10;            file_size = csv_file.stat().st_size&#10;            estimated_chunks = max(1, file_size // (self.chunk_size * 500))&#10;            chunk_iterator = tqdm(chunk_iterator, desc=&quot;   Chunks&quot;,&#10;                                  total=estimated_chunks, unit=&quot;chunk&quot;)&#10;&#10;        for chunk in chunk_iterator:&#10;            self.stats['total_rows_read'] += len(chunk)&#10;&#10;            chunk = self._clean_column_names(chunk)&#10;            chunk = self._drop_identification_columns(chunk)&#10;            chunk = self._convert_to_numeric(chunk)&#10;            chunk = self._convert_to_binary_label(chunk)&#10;            chunk = self._drop_zero_variance_columns(chunk)&#10;            chunk = self._handle_nan_inf_with_mode(chunk)&#10;&#10;            processed_chunks.append(chunk)&#10;            gc.collect()&#10;&#10;        if processed_chunks:&#10;            df = pd.concat(processed_chunks, ignore_index=True)&#10;            del processed_chunks&#10;            gc.collect()&#10;            print(f&quot;   ✅ {len(df):,} mẫu&quot;)&#10;            return df&#10;&#10;        return None&#10;&#10;    def clean_all_files(self):&#10;        start_time = datetime.now()&#10;        print(&quot;\n&quot; + &quot;=&quot;*80)&#10;        print(&quot; CLEAN DỮ LIỆU CICIDS2018&quot;)&#10;        print(&quot;=&quot;*80)&#10;&#10;        csv_files = self._get_csv_files()&#10;        all_columns = self._first_pass_collect_info(csv_files)&#10;&#10;        print(&quot;\n&quot; + &quot;=&quot;*80)&#10;        print(&quot; CLEAN DỮ LIỆU&quot;)&#10;        print(&quot;=&quot;*80)&#10;&#10;        all_dataframes = []&#10;        for csv_file in csv_files:&#10;            df = self._process_single_file(csv_file)&#10;            if df is not None:&#10;                all_dataframes.append(df)&#10;&#10;        print(&quot;\n   Gộp dữ liệu và loại duplicate...&quot;)&#10;        df_combined = pd.concat(all_dataframes, ignore_index=True)&#10;        del all_dataframes&#10;        gc.collect()&#10;&#10;        rows_before = len(df_combined)&#10;        df_combined = df_combined.drop_duplicates()&#10;        rows_after = len(df_combined)&#10;        self.stats['duplicates_removed'] = rows_before - rows_after&#10;&#10;        self.stats['benign_count'] = int((df_combined['binary_label'] == 0).sum())&#10;        self.stats['attack_count'] = int((df_combined['binary_label'] == 1).sum())&#10;        self.stats['rows_after_cleaning'] = len(df_combined)&#10;        self.stats['feature_count'] = len(df_combined.columns) - 1&#10;&#10;        self.feature_names = [col for col in df_combined.columns if col != 'binary_label']&#10;&#10;        end_time = datetime.now()&#10;        self.stats['processing_time'] = (end_time - start_time).total_seconds()&#10;&#10;        return df_combined&#10;&#10;    def save_cleaned_data(self, df):&#10;        print(&quot;\n&quot; + &quot;=&quot;*80)&#10;        print(&quot; LƯU DỮ LIỆU&quot;)&#10;        print(&quot;=&quot;*80)&#10;&#10;        parquet_path = self.output_dir / 'cleaned_data.parquet'&#10;        df.to_parquet(parquet_path, index=False)&#10;        print(f&quot;   ✅ cleaned_data.parquet ({parquet_path.stat().st_size / (1024*1024):.1f} MB)&quot;)&#10;&#10;        with open(self.output_dir / 'feature_names.txt', 'w') as f:&#10;            for name in self.feature_names:&#10;                f.write(name + '\n')&#10;        print(f&quot;   ✅ feature_names.txt&quot;)&#10;&#10;        with open(self.output_dir / 'column_modes.pkl', 'wb') as f:&#10;            pickle.dump(self.column_modes, f)&#10;        print(f&quot;   ✅ column_modes.pkl&quot;)&#10;&#10;        with open(self.output_dir / 'zero_variance_cols.pkl', 'wb') as f:&#10;            pickle.dump(self.zero_variance_cols, f)&#10;        print(f&quot;   ✅ zero_variance_cols.pkl&quot;)&#10;&#10;        stats_native = {}&#10;        for key, value in self.stats.items():&#10;            if hasattr(value, 'item'):&#10;                stats_native[key] = value.item()&#10;            elif isinstance(value, (np.integer, np.floating)):&#10;                stats_native[key] = int(value) if isinstance(value, np.integer) else float(value)&#10;            else:&#10;                stats_native[key] = value&#10;&#10;        metadata = {&#10;            'n_features': len(self.feature_names),&#10;            'feature_names': self.feature_names,&#10;            'total_samples': int(len(df)),&#10;            'benign_count': self.stats['benign_count'],&#10;            'attack_count': self.stats['attack_count'],&#10;            'benign_ratio': self.stats['benign_count'] / len(df),&#10;            'attack_ratio': self.stats['attack_count'] / len(df),&#10;            'zero_variance_cols': self.zero_variance_cols,&#10;            'stats': stats_native,&#10;            'created_at': datetime.now().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;)&#10;        }&#10;&#10;        with open(self.output_dir / 'cleaning_metadata.json', 'w', encoding='utf-8') as f:&#10;            json.dump(metadata, f, indent=4, ensure_ascii=False)&#10;        print(f&quot;   ✅ cleaning_metadata.json&quot;)&#10;&#10;        print(f&quot;\n Đã lưu vào: {self.output_dir}&quot;)&#10;&#10;    def print_summary(self):&#10;        print(&quot;\n&quot; + &quot;=&quot;*80)&#10;        print(&quot; TÓM TẮT&quot;)&#10;        print(&quot;=&quot;*80)&#10;        print(f&quot;   Tổng số dòng đọc:     {self.stats['total_rows_read']:,}&quot;)&#10;        print(f&quot;   Sau khi clean:        {self.stats['rows_after_cleaning']:,}&quot;)&#10;        print(f&quot;   Duplicate loại bỏ:    {self.stats['duplicates_removed']:,}&quot;)&#10;        print(f&quot;   NaN thay thế:         {self.stats['nan_replaced']:,}&quot;)&#10;        print(f&quot;   Inf thay thế:         {self.stats['inf_replaced']:,}&quot;)&#10;        print(f&quot;   Zero-variance loại:   {self.stats['zero_variance_cols_removed']}&quot;)&#10;        print(f&quot;   Features:             {self.stats['feature_count']}&quot;)&#10;        print(f&quot;\n   Benign:  {self.stats['benign_count']:,} ({self.stats['benign_count']/self.stats['rows_after_cleaning']*100:.1f}%)&quot;)&#10;        print(f&quot;   Attack:  {self.stats['attack_count']:,} ({self.stats['attack_count']/self.stats['rows_after_cleaning']*100:.1f}%)&quot;)&#10;        print(f&quot;\n   Thời gian: {self.stats['processing_time']:.2f}s&quot;)&#10;        print(&quot;=&quot;*80)&#10;&#10;&#10;def main():&#10;    print(&quot;\n&quot; + &quot;=&quot;*80)&#10;    print(&quot; BƯỚC 1: CLEAN DỮ LIỆU CICIDS2018 CHO CNN&quot;)&#10;    print(&quot;=&quot;*80)&#10;&#10;    cleaner = CICIDS2018_DataCleaner(&#10;        data_dir=DATA_DIR,&#10;        output_dir=OUTPUT_DIR,&#10;        chunk_size=CHUNK_SIZE&#10;    )&#10;&#10;    df = cleaner.clean_all_files()&#10;    cleaner.save_cleaned_data(df)&#10;    cleaner.print_summary()&#10;&#10;    print(&quot;\n✅ HOÀN THÀNH!&quot;)&#10;    print(&quot;   Chạy step2_prepare_training_data.py để chia train/val/test&quot;)&#10;&#10;    return cleaner&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    cleaner = main()&#10;&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/CNN/step2_prepare_training_data.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/CNN/step2_prepare_training_data.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;======================================================================================&#10;BƯỚC 2: CHUẨN BỊ DỮ LIỆU TRAINING CHO CNN - CÂN BẰNG VÀ CHIA TRAIN/VAL/TEST&#10;======================================================================================&#10;&#10;Script này thực hiện:&#10;1. Đọc dữ liệu đã clean từ step1&#10;2. Cân bằng số lượng nhãn (70% Benign, 30% Attack hoặc tỷ lệ tùy chỉnh)&#10;3. Áp dụng Log Transform: log_e(1+x)&#10;4. Chuẩn hóa bằng StandardScaler&#10;5. Reshape cho CNN 1D&#10;6. Chia train/val/test với stratify để giữ tỷ lệ&#10;7. Lưu dữ liệu đã xử lý để train&#10;&#10;Có thể chạy trên cả Kaggle và Local&#10;&quot;&quot;&quot;&#10;&#10;import os&#10;import numpy as np&#10;import pandas as pd&#10;import pickle&#10;import json&#10;import gc&#10;from pathlib import Path&#10;from datetime import datetime&#10;import warnings&#10;warnings.filterwarnings('ignore')&#10;&#10;# ============================================================================&#10;# THƯ VIỆN CHUẨN HÓA VÀ XỬ LÝ DỮ LIỆU&#10;# ============================================================================&#10;from sklearn.preprocessing import StandardScaler&#10;from sklearn.model_selection import train_test_split&#10;&#10;# Kiểm tra môi trường chạy (Kaggle hoặc Local)&#10;IS_KAGGLE = os.path.exists('/kaggle/input')&#10;&#10;# ============================================================================&#10;# CẤU HÌNH ĐƯỜNG DẪN&#10;# ============================================================================&#10;if IS_KAGGLE:&#10;    CLEANED_DATA_DIR = &quot;/kaggle/working/cleaned_data&quot;&#10;    OUTPUT_DIR = &quot;/kaggle/working/training_data&quot;&#10;    print(&quot; Đang chạy trên KAGGLE&quot;)&#10;else:&#10;    CLEANED_DATA_DIR = r&quot;D:\PROJECT\Machine Learning\IOT\CNN\cleaned_data&quot;&#10;    OUTPUT_DIR = r&quot;D:\PROJECT\Machine Learning\IOT\CNN\training_data&quot;&#10;    print(&quot; Đang chạy trên LOCAL&quot;)&#10;&#10;# ============================================================================&#10;# CẤU HÌNH CÂN BẰNG DỮ LIỆU&#10;# ============================================================================&#10;&#10;# Random state để tái tạo kết quả&#10;RANDOM_STATE = 42&#10;&#10;# Tổng số mẫu mong muốn (train + val + test)&#10;TOTAL_SAMPLES = 3000000  # 3 triệu mẫu&#10;&#10;# Tỷ lệ phần trăm cho mỗi class&#10;BENIGN_RATIO = 0.70  # 70% Benign&#10;ATTACK_RATIO = 0.30  # 30% Attack&#10;&#10;# Tính số lượng mẫu cho mỗi class&#10;TARGET_BENIGN = int(TOTAL_SAMPLES * BENIGN_RATIO)  # 2,100,000&#10;TARGET_ATTACK = int(TOTAL_SAMPLES * ATTACK_RATIO)  # 900,000&#10;&#10;# Tỷ lệ chia train/val/test&#10;TEST_SIZE = 0.20   # 20% cho test&#10;VAL_SIZE = 0.10    # 10% cho validation (từ tổng)&#10;# Train sẽ là 70%&#10;&#10;# ============================================================================&#10;# CLASS CHUẨN BỊ DỮ LIỆU TRAINING&#10;# ============================================================================&#10;&#10;class TrainingDataPreparer:&#10;    &quot;&quot;&quot;&#10;    Class chuẩn bị dữ liệu training cho CNN&#10;&#10;    Các bước:&#10;    1. Đọc dữ liệu đã clean&#10;    2. Cân bằng dữ liệu theo tỷ lệ mong muốn&#10;    3. Áp dụng log transform: log_e(1+x)&#10;    4. Chuẩn hóa bằng StandardScaler&#10;    5. Reshape cho CNN&#10;    6. Chia train/val/test&#10;    7. Lưu dữ liệu&#10;    &quot;&quot;&quot;&#10;&#10;    def __init__(self, cleaned_data_dir, output_dir,&#10;                 total_samples=TOTAL_SAMPLES,&#10;                 benign_ratio=BENIGN_RATIO,&#10;                 attack_ratio=ATTACK_RATIO,&#10;                 test_size=TEST_SIZE,&#10;                 val_size=VAL_SIZE):&#10;        &quot;&quot;&quot;&#10;        Khởi tạo preparer&#10;&#10;        Args:&#10;            cleaned_data_dir: Đường dẫn thư mục chứa dữ liệu đã clean&#10;            output_dir: Đường dẫn thư mục lưu kết quả&#10;            total_samples: Tổng số mẫu mong muốn&#10;            benign_ratio: Tỷ lệ Benign (0-1)&#10;            attack_ratio: Tỷ lệ Attack (0-1)&#10;            test_size: Tỷ lệ test set&#10;            val_size: Tỷ lệ validation set&#10;        &quot;&quot;&quot;&#10;        self.cleaned_data_dir = Path(cleaned_data_dir)&#10;        self.output_dir = Path(output_dir)&#10;        self.total_samples = total_samples&#10;        self.benign_ratio = benign_ratio&#10;        self.attack_ratio = attack_ratio&#10;        self.test_size = test_size&#10;        self.val_size = val_size&#10;&#10;        # Tính target cho mỗi class&#10;        self.target_benign = int(total_samples * benign_ratio)&#10;        self.target_attack = int(total_samples * attack_ratio)&#10;&#10;        # Khởi tạo scaler&#10;        self.scaler = StandardScaler()&#10;&#10;        # Tạo thư mục output&#10;        self.output_dir.mkdir(parents=True, exist_ok=True)&#10;&#10;        # Lưu tên features&#10;        self.feature_names = None&#10;&#10;        # Thống kê&#10;        self.stats = {&#10;            'original_benign': 0,&#10;            'original_attack': 0,&#10;            'sampled_benign': 0,&#10;            'sampled_attack': 0,&#10;            'train_samples': 0,&#10;            'val_samples': 0,&#10;            'test_samples': 0,&#10;            'n_features': 0&#10;        }&#10;&#10;    def load_cleaned_data(self):&#10;        &quot;&quot;&quot;Đọc dữ liệu đã clean từ step1&quot;&quot;&quot;&#10;        print(&quot;\n&quot; + &quot;=&quot;*80)&#10;        print(&quot; ĐANG ĐỌC DỮ LIỆU ĐÃ CLEAN...&quot;)&#10;        print(&quot;=&quot;*80)&#10;&#10;        parquet_path = self.cleaned_data_dir / 'cleaned_data.parquet'&#10;&#10;        if not parquet_path.exists():&#10;            raise FileNotFoundError(&#10;                f&quot;Không tìm thấy file {parquet_path}\n&quot;&#10;                f&quot;Hãy chạy step1_clean_data.py trước!&quot;&#10;            )&#10;&#10;        df = pd.read_parquet(parquet_path)&#10;&#10;        # Đọc feature names&#10;        feature_names_path = self.cleaned_data_dir / 'feature_names.txt'&#10;        if feature_names_path.exists():&#10;            with open(feature_names_path, 'r') as f:&#10;                self.feature_names = [line.strip() for line in f.readlines()]&#10;        else:&#10;            self.feature_names = [col for col in df.columns if col != 'binary_label']&#10;&#10;        # Thống kê&#10;        self.stats['original_benign'] = int((df['binary_label'] == 0).sum())&#10;        self.stats['original_attack'] = int((df['binary_label'] == 1).sum())&#10;        self.stats['n_features'] = len(self.feature_names)&#10;&#10;        print(f&quot;   ✅ Đã đọc: {len(df):,} mẫu&quot;)&#10;        print(f&quot;    Phân bố gốc:&quot;)&#10;        print(f&quot;      - Benign: {self.stats['original_benign']:,} ({self.stats['original_benign']/len(df)*100:.1f}%)&quot;)&#10;        print(f&quot;      - Attack: {self.stats['original_attack']:,} ({self.stats['original_attack']/len(df)*100:.1f}%)&quot;)&#10;        print(f&quot;    Số features: {self.stats['n_features']}&quot;)&#10;&#10;        return df&#10;&#10;    def balanced_sample(self, df):&#10;        &quot;&quot;&quot;&#10;        Sample dữ liệu với tỷ lệ cân bằng mong muốn&#10;&#10;        Chiến lược:&#10;        - Nếu có đủ mẫu: lấy đúng số lượng target&#10;        - Nếu không đủ Attack: giảm Benign tương ứng để giữ tỷ lệ&#10;        - Nếu không đủ cả hai: lấy tối đa có thể với tỷ lệ đúng&#10;        &quot;&quot;&quot;&#10;        print(&quot;\n&quot; + &quot;=&quot;*80)&#10;        print(&quot;⚖️ ĐANG CÂN BẰNG DỮ LIỆU...&quot;)&#10;        print(&quot;=&quot;*80)&#10;&#10;        # Tách theo class&#10;        df_benign = df[df['binary_label'] == 0]&#10;        df_attack = df[df['binary_label'] == 1]&#10;&#10;        n_benign = len(df_benign)&#10;        n_attack = len(df_attack)&#10;&#10;        print(f&quot;\n    Target mong muốn:&quot;)&#10;        print(f&quot;      - Tổng: {self.total_samples:,}&quot;)&#10;        print(f&quot;      - Benign: {self.target_benign:,} ({self.benign_ratio*100:.0f}%)&quot;)&#10;        print(f&quot;      - Attack: {self.target_attack:,} ({self.attack_ratio*100:.0f}%)&quot;)&#10;&#10;        # Xác định số lượng thực tế có thể lấy&#10;        # Ưu tiên giữ đúng tỷ lệ&#10;        actual_attack = min(self.target_attack, n_attack)&#10;        # Tính Benign dựa trên Attack thực tế để giữ tỷ lệ&#10;        actual_benign = int(actual_attack * (self.benign_ratio / self.attack_ratio))&#10;        actual_benign = min(actual_benign, n_benign)&#10;&#10;        # Nếu Benign bị giới hạn, điều chỉnh Attack&#10;        if actual_benign &lt; int(actual_attack * (self.benign_ratio / self.attack_ratio)):&#10;            actual_attack = int(actual_benign * (self.attack_ratio / self.benign_ratio))&#10;&#10;        print(f&quot;\n    Số lượng thực tế sẽ lấy:&quot;)&#10;        print(f&quot;      - Benign: {actual_benign:,}&quot;)&#10;        print(f&quot;      - Attack: {actual_attack:,}&quot;)&#10;        print(f&quot;      - Tổng: {actual_benign + actual_attack:,}&quot;)&#10;        print(f&quot;      - Tỷ lệ thực tế: {actual_benign/(actual_benign+actual_attack)*100:.1f}% - {actual_attack/(actual_benign+actual_attack)*100:.1f}%&quot;)&#10;&#10;        if actual_benign &lt; self.target_benign or actual_attack &lt; self.target_attack:&#10;            print(f&quot;\n   ⚠️ Không đủ mẫu để đạt target!&quot;)&#10;            print(f&quot;      Có sẵn: Benign={n_benign:,}, Attack={n_attack:,}&quot;)&#10;&#10;        # Random sample từ mỗi class&#10;        print(f&quot;\n    Đang sample...&quot;)&#10;&#10;        # Sử dụng random sampling&#10;        df_benign_sampled = df_benign.sample(n=actual_benign, random_state=RANDOM_STATE)&#10;        df_attack_sampled = df_attack.sample(n=actual_attack, random_state=RANDOM_STATE)&#10;&#10;        # Gộp lại và shuffle&#10;        df_balanced = pd.concat([df_benign_sampled, df_attack_sampled], ignore_index=True)&#10;        df_balanced = df_balanced.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)&#10;&#10;        # Cập nhật stats&#10;        self.stats['sampled_benign'] = actual_benign&#10;        self.stats['sampled_attack'] = actual_attack&#10;&#10;        print(f&quot;\n   ✅ Kết quả sau khi cân bằng:&quot;)&#10;        print(f&quot;      - Benign: {actual_benign:,} ({actual_benign/(actual_benign+actual_attack)*100:.1f}%)&quot;)&#10;        print(f&quot;      - Attack: {actual_attack:,} ({actual_attack/(actual_benign+actual_attack)*100:.1f}%)&quot;)&#10;        print(f&quot;      - Tổng: {len(df_balanced):,}&quot;)&#10;&#10;        # Giải phóng bộ nhớ&#10;        del df_benign, df_attack, df_benign_sampled, df_attack_sampled&#10;        gc.collect()&#10;&#10;        return df_balanced&#10;&#10;    def apply_log_transform(self, X):&#10;        &quot;&quot;&quot;&#10;        Áp dụng Log Transform: log_e(1+x)&#10;&#10;        Lưu ý: log(1+x) giúp:&#10;        - Giảm skewness của dữ liệu&#10;        - Xử lý các giá trị lớn&#10;        - Bảo toàn giá trị 0 (log(1+0) = 0)&#10;        &quot;&quot;&quot;&#10;        print(&quot;\n ĐANG ÁP DỤNG LOG TRANSFORM: log_e(1+x)...&quot;)&#10;&#10;        # Đảm bảo không có giá trị âm (log không xác định cho số âm)&#10;        # Với dữ liệu network flow, các giá trị thường &gt;= 0&#10;        # Nếu có giá trị âm, ta shift để min = 0&#10;        min_val = X.min()&#10;        if min_val &lt; 0:&#10;            print(f&quot;   ⚠️ Phát hiện giá trị âm (min={min_val:.4f}), đang shift...&quot;)&#10;            X = X - min_val  # Shift để min = 0&#10;&#10;        # Áp dụng log(1+x)&#10;        X_log = np.log1p(X)  # log1p(x) = log(1+x), ổn định hơn với x nhỏ&#10;&#10;        print(f&quot;   ✅ Log transform hoàn tất&quot;)&#10;        print(f&quot;      Range trước: [{X.min():.4f}, {X.max():.4f}]&quot;)&#10;        print(f&quot;      Range sau:   [{X_log.min():.4f}, {X_log.max():.4f}]&quot;)&#10;&#10;        return X_log&#10;&#10;    def normalize_features(self, X):&#10;        &quot;&quot;&quot;&#10;        Chuẩn hóa features bằng StandardScaler&#10;        &quot;&quot;&quot;&#10;        print(&quot;\n ĐANG CHUẨN HÓA BẰNG STANDARDSCALER...&quot;)&#10;&#10;        X_normalized = self.scaler.fit_transform(X)&#10;&#10;        print(f&quot;   ✅ StandardScaler hoàn tất&quot;)&#10;        print(f&quot;      Mean: {X_normalized.mean():.6f}&quot;)&#10;        print(f&quot;      Std:  {X_normalized.std():.6f}&quot;)&#10;&#10;        return X_normalized&#10;&#10;    def reshape_for_cnn(self, X):&#10;        &quot;&quot;&quot;&#10;        Reshape dữ liệu cho CNN 1D&#10;        CNN 1D yêu cầu input shape: (samples, features, channels)&#10;        &quot;&quot;&quot;&#10;        print(&quot;\n ĐANG RESHAPE CHO CNN 1D...&quot;)&#10;&#10;        X_reshaped = X.reshape(X.shape[0], X.shape[1], 1)&#10;&#10;        print(f&quot;   ✅ Shape: {X.shape} -&gt; {X_reshaped.shape}&quot;)&#10;        print(f&quot;      (samples, features, channels)&quot;)&#10;&#10;        return X_reshaped&#10;&#10;    def split_data(self, X, y):&#10;        &quot;&quot;&quot;&#10;        Chia dữ liệu thành train/val/test&#10;&#10;        Thêm validation: Train 70%, Val 10%, Test 20%&#10;&#10;        Sử dụng stratify để giữ tỷ lệ class trong tất cả các tập&#10;        &quot;&quot;&quot;&#10;        print(&quot;\n ĐANG CHIA DỮ LIỆU TRAIN/VAL/TEST...&quot;)&#10;&#10;        # Bước 1: Chia train+val / test (80/20)&#10;        X_temp, X_test, y_temp, y_test = train_test_split(&#10;            X, y,&#10;            test_size=self.test_size,&#10;            random_state=RANDOM_STATE,&#10;            stratify=y  # Giữ tỷ lệ class&#10;        )&#10;&#10;        # Bước 2: Chia train / val&#10;        val_ratio_from_temp = self.val_size / (1 - self.test_size)&#10;        X_train, X_val, y_train, y_val = train_test_split(&#10;            X_temp, y_temp,&#10;            test_size=val_ratio_from_temp,&#10;            random_state=RANDOM_STATE,&#10;            stratify=y_temp&#10;        )&#10;&#10;        # Cập nhật stats&#10;        self.stats['train_samples'] = len(X_train)&#10;        self.stats['val_samples'] = len(X_val)&#10;        self.stats['test_samples'] = len(X_test)&#10;&#10;        print(f&quot;\n    KẾT QUẢ CHIA DỮ LIỆU:&quot;)&#10;        print(f&quot;   {'='*50}&quot;)&#10;        print(f&quot;   {'Set':&lt;10} {'Samples':&gt;12} {'Benign':&gt;12} {'Attack':&gt;12}&quot;)&#10;        print(f&quot;   {'-'*50}&quot;)&#10;        print(f&quot;   {'Train':&lt;10} {len(X_train):&gt;12,} {(y_train==0).sum():&gt;12,} {(y_train==1).sum():&gt;12,}&quot;)&#10;        print(f&quot;   {'Val':&lt;10} {len(X_val):&gt;12,} {(y_val==0).sum():&gt;12,} {(y_val==1).sum():&gt;12,}&quot;)&#10;        print(f&quot;   {'Test':&lt;10} {len(X_test):&gt;12,} {(y_test==0).sum():&gt;12,} {(y_test==1).sum():&gt;12,}&quot;)&#10;        print(f&quot;   {'-'*50}&quot;)&#10;        print(f&quot;   {'Total':&lt;10} {len(X_train)+len(X_val)+len(X_test):&gt;12,}&quot;)&#10;&#10;        # Kiểm tra tỷ lệ&#10;        print(f&quot;\n    TỶ LỆ ATTACK TRONG MỖI TẬP:&quot;)&#10;        print(f&quot;      Train: {(y_train==1).sum()/len(y_train)*100:.1f}%&quot;)&#10;        print(f&quot;      Val:   {(y_val==1).sum()/len(y_val)*100:.1f}%&quot;)&#10;        print(f&quot;      Test:  {(y_test==1).sum()/len(y_test)*100:.1f}%&quot;)&#10;&#10;        return X_train, X_val, X_test, y_train, y_val, y_test&#10;&#10;    def save_training_data(self, X_train, X_val, X_test, y_train, y_val, y_test):&#10;        &quot;&quot;&quot;&#10;        Lưu dữ liệu training&#10;&#10;        Lưu các file:&#10;        - X_train.npy, X_val.npy, X_test.npy&#10;        - y_train.npy, y_val.npy, y_test.npy&#10;        - scaler.pkl&#10;        - training_metadata.json&#10;        - feature_names.txt&#10;        &quot;&quot;&quot;&#10;        print(&quot;\n&quot; + &quot;=&quot;*80)&#10;        print(&quot; ĐANG LƯU DỮ LIỆU TRAINING...&quot;)&#10;        print(&quot;=&quot;*80)&#10;&#10;        # Lưu numpy arrays&#10;        np.save(self.output_dir / 'X_train.npy', X_train)&#10;        np.save(self.output_dir / 'X_val.npy', X_val)&#10;        np.save(self.output_dir / 'X_test.npy', X_test)&#10;        np.save(self.output_dir / 'y_train.npy', y_train)&#10;        np.save(self.output_dir / 'y_val.npy', y_val)&#10;        np.save(self.output_dir / 'y_test.npy', y_test)&#10;&#10;        print(f&quot;   ✅ X_train.npy: {X_train.shape}&quot;)&#10;        print(f&quot;   ✅ X_val.npy:   {X_val.shape}&quot;)&#10;        print(f&quot;   ✅ X_test.npy:  {X_test.shape}&quot;)&#10;        print(f&quot;   ✅ y_train.npy: {y_train.shape}&quot;)&#10;        print(f&quot;   ✅ y_val.npy:   {y_val.shape}&quot;)&#10;        print(f&quot;   ✅ y_test.npy:  {y_test.shape}&quot;)&#10;&#10;        # Lưu scaler&#10;        with open(self.output_dir / 'scaler.pkl', 'wb') as f:&#10;            pickle.dump(self.scaler, f)&#10;        print(f&quot;   ✅ scaler.pkl&quot;)&#10;&#10;        # Lưu feature names&#10;        with open(self.output_dir / 'feature_names.txt', 'w') as f:&#10;            for name in self.feature_names:&#10;                f.write(name + '\n')&#10;        print(f&quot;   ✅ feature_names.txt&quot;)&#10;&#10;        # Chuẩn bị metadata&#10;        metadata = {&#10;            'n_features': len(self.feature_names),&#10;            'input_shape': [int(X_train.shape[1]), int(X_train.shape[2])],&#10;            'train_samples': int(X_train.shape[0]),&#10;            'val_samples': int(X_val.shape[0]),&#10;            'test_samples': int(X_test.shape[0]),&#10;            'total_samples': int(X_train.shape[0] + X_val.shape[0] + X_test.shape[0]),&#10;            'class_distribution': {&#10;                'train': {&#10;                    'benign': int((y_train == 0).sum()),&#10;                    'attack': int((y_train == 1).sum())&#10;                },&#10;                'val': {&#10;                    'benign': int((y_val == 0).sum()),&#10;                    'attack': int((y_val == 1).sum())&#10;                },&#10;                'test': {&#10;                    'benign': int((y_test == 0).sum()),&#10;                    'attack': int((y_test == 1).sum())&#10;                }&#10;            },&#10;            'benign_ratio': float(self.benign_ratio),&#10;            'attack_ratio': float(self.attack_ratio),&#10;            'preprocessing': {&#10;                'log_transform': 'log_e(1+x)',&#10;                'normalization': 'StandardScaler'&#10;            },&#10;            'created_at': datetime.now().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;)&#10;        }&#10;&#10;        with open(self.output_dir / 'training_metadata.json', 'w', encoding='utf-8') as f:&#10;            json.dump(metadata, f, indent=4, ensure_ascii=False)&#10;        print(f&quot;   ✅ training_metadata.json&quot;)&#10;&#10;        print(f&quot;\n Tất cả file được lưu tại: {self.output_dir}&quot;)&#10;&#10;    def calculate_class_weights(self, y_train):&#10;        &quot;&quot;&quot;&#10;        Tính class weights cho training&#10;&#10;        Sử dụng khi dữ liệu vẫn còn imbalanced&#10;        &quot;&quot;&quot;&#10;        from sklearn.utils.class_weight import compute_class_weight&#10;&#10;        classes = np.unique(y_train)&#10;        weights = compute_class_weight('balanced', classes=classes, y=y_train)&#10;        class_weights = dict(zip(classes, weights))&#10;&#10;        print(f&quot;\n⚖️ CLASS WEIGHTS (cho training):&quot;)&#10;        print(f&quot;   Class 0 (Benign): {class_weights[0]:.4f}&quot;)&#10;        print(f&quot;   Class 1 (Attack): {class_weights[1]:.4f}&quot;)&#10;&#10;        # Lưu class weights&#10;        with open(self.output_dir / 'class_weights.pkl', 'wb') as f:&#10;            pickle.dump(class_weights, f)&#10;        print(f&quot;   ✅ Đã lưu class_weights.pkl&quot;)&#10;&#10;        return class_weights&#10;&#10;&#10;def main():&#10;    &quot;&quot;&quot;Hàm chính&quot;&quot;&quot;&#10;&#10;    print(&quot;\n&quot; + &quot;=&quot;*80)&#10;    print(&quot; BƯỚC 2: CHUẨN BỊ DỮ LIỆU TRAINING CHO CNN&quot;)&#10;    print(&quot;   Cân bằng và chia train/val/test&quot;)&#10;    print(&quot;=&quot;*80)&#10;&#10;    print(f&quot;\n CẤU HÌNH:&quot;)&#10;    print(f&quot;   - Tổng mẫu mong muốn: {TOTAL_SAMPLES:,}&quot;)&#10;    print(f&quot;   - Tỷ lệ Benign: {BENIGN_RATIO*100:.0f}%&quot;)&#10;    print(f&quot;   - Tỷ lệ Attack: {ATTACK_RATIO*100:.0f}%&quot;)&#10;    print(f&quot;   - Train/Val/Test: {(1-TEST_SIZE-VAL_SIZE)*100:.0f}%/{VAL_SIZE*100:.0f}%/{TEST_SIZE*100:.0f}%&quot;)&#10;&#10;    # Khởi tạo preparer&#10;    preparer = TrainingDataPreparer(&#10;        cleaned_data_dir=CLEANED_DATA_DIR,&#10;        output_dir=OUTPUT_DIR,&#10;        total_samples=TOTAL_SAMPLES,&#10;        benign_ratio=BENIGN_RATIO,&#10;        attack_ratio=ATTACK_RATIO,&#10;        test_size=TEST_SIZE,&#10;        val_size=VAL_SIZE&#10;    )&#10;&#10;    # Bước 1: Đọc dữ liệu đã clean&#10;    df = preparer.load_cleaned_data()&#10;&#10;    # Bước 2: Cân bằng dữ liệu&#10;    df = preparer.balanced_sample(df)&#10;&#10;    # Tách features và labels&#10;    X = df.drop(columns=['binary_label']).values&#10;    y = df['binary_label'].values&#10;&#10;    # Giải phóng bộ nhớ DataFrame&#10;    del df&#10;    gc.collect()&#10;&#10;    # Bước 3: Áp dụng Log Transform&#10;    X = preparer.apply_log_transform(X)&#10;&#10;    # Bước 4: Chuẩn hóa&#10;    X = preparer.normalize_features(X)&#10;&#10;    # Bước 5: Reshape cho CNN&#10;    X = preparer.reshape_for_cnn(X)&#10;&#10;    # Bước 6: Chia train/val/test&#10;    X_train, X_val, X_test, y_train, y_val, y_test = preparer.split_data(X, y)&#10;&#10;    # Giải phóng bộ nhớ&#10;    del X, y&#10;    gc.collect()&#10;&#10;    # Bước 7: Tính class weights&#10;    class_weights = preparer.calculate_class_weights(y_train)&#10;&#10;    # Bước 8: Lưu dữ liệu&#10;    preparer.save_training_data(X_train, X_val, X_test, y_train, y_val, y_test)&#10;&#10;    print(&quot;\n&quot; + &quot;=&quot;*80)&#10;    print(&quot;✅ HOÀN THÀNH BƯỚC 2!&quot;)&#10;    print(&quot;   Dữ liệu đã sẵn sàng cho việc huấn luyện CNN.&quot;)&#10;    print(&quot;   Chạy step3_train_cnn.py để train mô hình.&quot;)&#10;    print(&quot;=&quot;*80)&#10;&#10;    return preparer&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    preparer = main()&#10;&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;BƯỚC 2: CHUẨN BỊ DỮ LIỆU TRAINING - CÂN BẰNG VÀ CHIA TRAIN/VAL/TEST&#10;- Cân bằng nhãn theo tỷ lệ (70% Benign, 30% Attack)&#10;- Log Transform: log_e(1+x) và StandardScaler&#10;- Reshape cho CNN 1D&#10;- Chia train 70%, val 10%, test 20%&#10;&quot;&quot;&quot;&#10;&#10;import os&#10;import numpy as np&#10;import pandas as pd&#10;import pickle&#10;import json&#10;import gc&#10;from pathlib import Path&#10;from datetime import datetime&#10;import warnings&#10;warnings.filterwarnings('ignore')&#10;&#10;from sklearn.preprocessing import StandardScaler&#10;from sklearn.model_selection import train_test_split&#10;&#10;IS_KAGGLE = os.path.exists('/kaggle/input')&#10;&#10;# Cấu hình đường dẫn&#10;if IS_KAGGLE:&#10;    CLEANED_DATA_DIR = &quot;/kaggle/working/cleaned_data&quot;&#10;    OUTPUT_DIR = &quot;/kaggle/working/training_data&quot;&#10;    print(&quot; Kaggle&quot;)&#10;else:&#10;    CLEANED_DATA_DIR = r&quot;D:\PROJECT\Machine Learning\IOT\CNN\cleaned_data&quot;&#10;    OUTPUT_DIR = r&quot;D:\PROJECT\Machine Learning\IOT\CNN\training_data&quot;&#10;    print(&quot; Local&quot;)&#10;&#10;# Cấu hình cân bằng dữ liệu&#10;RANDOM_STATE = 42&#10;TOTAL_SAMPLES = 3000000&#10;BENIGN_RATIO = 0.70&#10;ATTACK_RATIO = 0.30&#10;TARGET_BENIGN = int(TOTAL_SAMPLES * BENIGN_RATIO)&#10;TARGET_ATTACK = int(TOTAL_SAMPLES * ATTACK_RATIO)&#10;&#10;# Tỷ lệ chia&#10;TEST_SIZE = 0.20&#10;VAL_SIZE = 0.10&#10;&#10;class TrainingDataPreparer:&#10;    &quot;&quot;&quot;Chuẩn bị dữ liệu training cho CNN: cân bằng, normalize, chia train/val/test&quot;&quot;&quot;&#10;&#10;    def __init__(self, cleaned_data_dir, output_dir,&#10;                 total_samples=TOTAL_SAMPLES,&#10;                 benign_ratio=BENIGN_RATIO,&#10;                 attack_ratio=ATTACK_RATIO,&#10;                 test_size=TEST_SIZE,&#10;                 val_size=VAL_SIZE):&#10;        self.cleaned_data_dir = Path(cleaned_data_dir)&#10;        self.output_dir = Path(output_dir)&#10;        self.total_samples = total_samples&#10;        self.benign_ratio = benign_ratio&#10;        self.attack_ratio = attack_ratio&#10;        self.test_size = test_size&#10;        self.val_size = val_size&#10;&#10;        self.target_benign = int(total_samples * benign_ratio)&#10;        self.target_attack = int(total_samples * attack_ratio)&#10;&#10;        self.scaler = StandardScaler()&#10;        self.output_dir.mkdir(parents=True, exist_ok=True)&#10;        self.feature_names = None&#10;&#10;        self.stats = {&#10;            'original_benign': 0,&#10;            'original_attack': 0,&#10;            'sampled_benign': 0,&#10;            'sampled_attack': 0,&#10;            'train_samples': 0,&#10;            'val_samples': 0,&#10;            'test_samples': 0,&#10;            'n_features': 0&#10;        }&#10;&#10;    def load_cleaned_data(self):&#10;        print(&quot;\n&quot; + &quot;=&quot;*80)&#10;        print(&quot; ĐỌC DỮ LIỆU&quot;)&#10;        print(&quot;=&quot;*80)&#10;&#10;        parquet_path = self.cleaned_data_dir / 'cleaned_data.parquet'&#10;        if not parquet_path.exists():&#10;            raise FileNotFoundError(f&quot;Không tìm thấy {parquet_path}\nChạy step1_clean_data.py trước!&quot;)&#10;&#10;        df = pd.read_parquet(parquet_path)&#10;&#10;        feature_names_path = self.cleaned_data_dir / 'feature_names.txt'&#10;        if feature_names_path.exists():&#10;            with open(feature_names_path, 'r') as f:&#10;                self.feature_names = [line.strip() for line in f.readlines()]&#10;        else:&#10;            self.feature_names = [col for col in df.columns if col != 'binary_label']&#10;&#10;        self.stats['original_benign'] = int((df['binary_label'] == 0).sum())&#10;        self.stats['original_attack'] = int((df['binary_label'] == 1).sum())&#10;        self.stats['n_features'] = len(self.feature_names)&#10;&#10;        print(f&quot;   {len(df):,} mẫu - Benign: {self.stats['original_benign']:,}, Attack: {self.stats['original_attack']:,}&quot;)&#10;        print(f&quot;   Features: {self.stats['n_features']}&quot;)&#10;&#10;        return df&#10;&#10;    def balanced_sample(self, df):&#10;        print(&quot;\n&quot; + &quot;=&quot;*80)&#10;        print(&quot;⚖️ CÂN BẰNG DỮ LIỆU&quot;)&#10;        print(&quot;=&quot;*80)&#10;&#10;        df_benign = df[df['binary_label'] == 0]&#10;        df_attack = df[df['binary_label'] == 1]&#10;&#10;        n_benign = len(df_benign)&#10;        n_attack = len(df_attack)&#10;&#10;        print(f&quot;   Target: {self.total_samples:,} (Benign {self.benign_ratio*100:.0f}%, Attack {self.attack_ratio*100:.0f}%)&quot;)&#10;&#10;        actual_attack = min(self.target_attack, n_attack)&#10;        actual_benign = int(actual_attack * (self.benign_ratio / self.attack_ratio))&#10;        actual_benign = min(actual_benign, n_benign)&#10;&#10;        if actual_benign &lt; int(actual_attack * (self.benign_ratio / self.attack_ratio)):&#10;            actual_attack = int(actual_benign * (self.attack_ratio / self.benign_ratio))&#10;&#10;        print(f&quot;   Lấy: Benign {actual_benign:,}, Attack {actual_attack:,} = {actual_benign + actual_attack:,}&quot;)&#10;        print(f&quot;   Tỷ lệ: {actual_benign/(actual_benign+actual_attack)*100:.1f}% - {actual_attack/(actual_benign+actual_attack)*100:.1f}%&quot;)&#10;&#10;        df_benign_sampled = df_benign.sample(n=actual_benign, random_state=RANDOM_STATE)&#10;        df_attack_sampled = df_attack.sample(n=actual_attack, random_state=RANDOM_STATE)&#10;&#10;        df_balanced = pd.concat([df_benign_sampled, df_attack_sampled], ignore_index=True)&#10;        df_balanced = df_balanced.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)&#10;&#10;        self.stats['sampled_benign'] = actual_benign&#10;        self.stats['sampled_attack'] = actual_attack&#10;&#10;        del df_benign, df_attack, df_benign_sampled, df_attack_sampled&#10;        gc.collect()&#10;&#10;        return df_balanced&#10;&#10;    def apply_log_transform(self, X):&#10;        print(&quot;\n Log Transform: log_e(1+x)&quot;)&#10;&#10;        min_val = X.min()&#10;        if min_val &lt; 0:&#10;            X = X - min_val&#10;&#10;        X_log = np.log1p(X)&#10;        print(f&quot;   Range: [{X.min():.4f}, {X.max():.4f}] -&gt; [{X_log.min():.4f}, {X_log.max():.4f}]&quot;)&#10;&#10;        return X_log&#10;&#10;    def normalize_features(self, X):&#10;        print(&quot;\n StandardScaler&quot;)&#10;        X_normalized = self.scaler.fit_transform(X)&#10;        print(f&quot;   Mean: {X_normalized.mean():.6f}, Std: {X_normalized.std():.6f}&quot;)&#10;        return X_normalized&#10;&#10;    def reshape_for_cnn(self, X):&#10;        print(f&quot;\n Reshape: {X.shape} -&gt; &quot;, end=&quot;&quot;)&#10;        X_reshaped = X.reshape(X.shape[0], X.shape[1], 1)&#10;        print(f&quot;{X_reshaped.shape}&quot;)&#10;        return X_reshaped&#10;&#10;    def split_data(self, X, y):&#10;        print(&quot;\n CHIA TRAIN/VAL/TEST&quot;)&#10;&#10;        X_temp, X_test, y_temp, y_test = train_test_split(&#10;            X, y, test_size=self.test_size, random_state=RANDOM_STATE, stratify=y&#10;        )&#10;&#10;        val_ratio_from_temp = self.val_size / (1 - self.test_size)&#10;        X_train, X_val, y_train, y_val = train_test_split(&#10;            X_temp, y_temp, test_size=val_ratio_from_temp, random_state=RANDOM_STATE, stratify=y_temp&#10;        )&#10;&#10;        self.stats['train_samples'] = len(X_train)&#10;        self.stats['val_samples'] = len(X_val)&#10;        self.stats['test_samples'] = len(X_test)&#10;&#10;        print(f&quot;\n   Train: {len(X_train):,} (Benign: {(y_train==0).sum():,}, Attack: {(y_train==1).sum():,})&quot;)&#10;        print(f&quot;   Val:   {len(X_val):,} (Benign: {(y_val==0).sum():,}, Attack: {(y_val==1).sum():,})&quot;)&#10;        print(f&quot;   Test:  {len(X_test):,} (Benign: {(y_test==0).sum():,}, Attack: {(y_test==1).sum():,})&quot;)&#10;        print(f&quot;   Tỷ lệ Attack: Train {(y_train==1).sum()/len(y_train)*100:.1f}%, Val {(y_val==1).sum()/len(y_val)*100:.1f}%, Test {(y_test==1).sum()/len(y_test)*100:.1f}%&quot;)&#10;&#10;        return X_train, X_val, X_test, y_train, y_val, y_test&#10;&#10;    def save_training_data(self, X_train, X_val, X_test, y_train, y_val, y_test):&#10;        print(&quot;\n&quot; + &quot;=&quot;*80)&#10;        print(&quot; LƯU DỮ LIỆU&quot;)&#10;        print(&quot;=&quot;*80)&#10;&#10;        np.save(self.output_dir / 'X_train.npy', X_train)&#10;        np.save(self.output_dir / 'X_val.npy', X_val)&#10;        np.save(self.output_dir / 'X_test.npy', X_test)&#10;        np.save(self.output_dir / 'y_train.npy', y_train)&#10;        np.save(self.output_dir / 'y_val.npy', y_val)&#10;        np.save(self.output_dir / 'y_test.npy', y_test)&#10;&#10;        print(f&quot;   ✅ Đã lưu arrays: X_train{X_train.shape}, X_val{X_val.shape}, X_test{X_test.shape}&quot;)&#10;&#10;        with open(self.output_dir / 'scaler.pkl', 'wb') as f:&#10;            pickle.dump(self.scaler, f)&#10;&#10;        with open(self.output_dir / 'feature_names.txt', 'w') as f:&#10;            for name in self.feature_names:&#10;                f.write(name + '\n')&#10;&#10;        print(f&quot;   ✅ scaler.pkl, feature_names.txt&quot;)&#10;&#10;        metadata = {&#10;            'n_features': len(self.feature_names),&#10;            'input_shape': [int(X_train.shape[1]), int(X_train.shape[2])],&#10;            'train_samples': int(X_train.shape[0]),&#10;            'val_samples': int(X_val.shape[0]),&#10;            'test_samples': int(X_test.shape[0]),&#10;            'total_samples': int(X_train.shape[0] + X_val.shape[0] + X_test.shape[0]),&#10;            'class_distribution': {&#10;                'train': {'benign': int((y_train == 0).sum()), 'attack': int((y_train == 1).sum())},&#10;                'val': {'benign': int((y_val == 0).sum()), 'attack': int((y_val == 1).sum())},&#10;                'test': {'benign': int((y_test == 0).sum()), 'attack': int((y_test == 1).sum())}&#10;            },&#10;            'benign_ratio': float(self.benign_ratio),&#10;            'attack_ratio': float(self.attack_ratio),&#10;            'preprocessing': {'log_transform': 'log_e(1+x)', 'normalization': 'StandardScaler'},&#10;            'created_at': datetime.now().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;)&#10;        }&#10;&#10;        with open(self.output_dir / 'training_metadata.json', 'w', encoding='utf-8') as f:&#10;            json.dump(metadata, f, indent=4, ensure_ascii=False)&#10;&#10;        print(f&quot;   ✅ training_metadata.json&quot;)&#10;        print(f&quot;\n {self.output_dir}&quot;)&#10;&#10;    def calculate_class_weights(self, y_train):&#10;        from sklearn.utils.class_weight import compute_class_weight&#10;&#10;        classes = np.unique(y_train)&#10;        weights = compute_class_weight('balanced', classes=classes, y=y_train)&#10;        class_weights = dict(zip(classes, weights))&#10;&#10;        print(f&quot;\n⚖️ Class Weights: Benign {class_weights[0]:.4f}, Attack {class_weights[1]:.4f}&quot;)&#10;&#10;        with open(self.output_dir / 'class_weights.pkl', 'wb') as f:&#10;            pickle.dump(class_weights, f)&#10;&#10;        return class_weights&#10;&#10;&#10;def main():&#10;    print(&quot;\n&quot; + &quot;=&quot;*80)&#10;    print(&quot; BƯỚC 2: CHUẨN BỊ DỮ LIỆU TRAINING&quot;)&#10;    print(&quot;=&quot;*80)&#10;    print(f&quot;   Target: {TOTAL_SAMPLES:,} ({BENIGN_RATIO*100:.0f}% Benign, {ATTACK_RATIO*100:.0f}% Attack)&quot;)&#10;&#10;    preparer = TrainingDataPreparer(&#10;        cleaned_data_dir=CLEANED_DATA_DIR,&#10;        output_dir=OUTPUT_DIR,&#10;        total_samples=TOTAL_SAMPLES,&#10;        benign_ratio=BENIGN_RATIO,&#10;        attack_ratio=ATTACK_RATIO,&#10;        test_size=TEST_SIZE,&#10;        val_size=VAL_SIZE&#10;    )&#10;&#10;    df = preparer.load_cleaned_data()&#10;    df = preparer.balanced_sample(df)&#10;&#10;    X = df.drop(columns=['binary_label']).values&#10;    y = df['binary_label'].values&#10;    del df&#10;    gc.collect()&#10;&#10;    X = preparer.apply_log_transform(X)&#10;    X = preparer.normalize_features(X)&#10;    X = preparer.reshape_for_cnn(X)&#10;&#10;    X_train, X_val, X_test, y_train, y_val, y_test = preparer.split_data(X, y)&#10;    del X, y&#10;    gc.collect()&#10;&#10;    class_weights = preparer.calculate_class_weights(y_train)&#10;    preparer.save_training_data(X_train, X_val, X_test, y_train, y_val, y_test)&#10;&#10;    print(&quot;\n✅ HOÀN THÀNH! Chạy step3_train_cnn.py để train model&quot;)&#10;&#10;    return preparer&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    preparer = main()&#10;&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/CNN/step3_train_cnn.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/CNN/step3_train_cnn.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;======================================================================================&#10;BƯỚC 3: TRAIN MÔ HÌNH CNN CHO PHÁT HIỆN LƯU LƯỢNG MẠNG IOT BẤT THƯỜNG&#10;======================================================================================&#10;&#10;Kiến trúc CNN theo yêu cầu:&#10;- Input Layer: Shape (num_features, 1)&#10;- Conv1D (32 filters, kernel 2) -&gt; MaxPooling1D (2)&#10;- Conv1D (32 filters, kernel 2) -&gt; MaxPooling1D (2)&#10;- Conv1D (64 filters, kernel 2) -&gt; MaxPooling1D (2)&#10;- Conv1D (64 filters, kernel 2) -&gt; MaxPooling1D (2)&#10;- Conv1D (64 filters, kernel 2) -&gt; MaxPooling1D (2)&#10;- BatchNormalization + Dropout (0.5)&#10;- Flatten&#10;- Dense(1, activation='sigmoid')&#10;&#10;Loss: binary_crossentropy&#10;Optimizer: Adam&#10;Metrics: Accuracy, Precision, Recall&#10;&#10;Có thể chạy trên cả Kaggle và Local&#10;&quot;&quot;&quot;&#10;&#10;import os&#10;import numpy as np&#10;import pickle&#10;import json&#10;from pathlib import Path&#10;from datetime import datetime&#10;import warnings&#10;warnings.filterwarnings('ignore')&#10;&#10;# ============================================================================&#10;# TENSORFLOW/KERAS&#10;# ============================================================================&#10;import tensorflow as tf&#10;from tensorflow import keras&#10;from tensorflow.keras.models import Sequential&#10;from tensorflow.keras.layers import (&#10;    Conv1D, MaxPooling1D, Flatten, Dense,&#10;    Dropout, BatchNormalization, Input&#10;)&#10;from tensorflow.keras.callbacks import (&#10;    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard&#10;)&#10;from tensorflow.keras.metrics import Precision, Recall&#10;&#10;# Kiểm tra GPU&#10;print(&quot;=&quot;*80)&#10;print(&quot;️ THÔNG TIN HỆ THỐNG&quot;)&#10;print(&quot;=&quot;*80)&#10;print(f&quot;TensorFlow version: {tf.__version__}&quot;)&#10;gpus = tf.config.list_physical_devices('GPU')&#10;if gpus:&#10;    print(f&quot;✅ GPU available: {len(gpus)} GPU(s)&quot;)&#10;    for gpu in gpus:&#10;        print(f&quot;   - {gpu}&quot;)&#10;    # Cấu hình GPU memory growth để tránh chiếm hết bộ nhớ&#10;    for gpu in gpus:&#10;        tf.config.experimental.set_memory_growth(gpu, True)&#10;else:&#10;    print(&quot;⚠️ Không có GPU, sẽ sử dụng CPU&quot;)&#10;&#10;# Kiểm tra môi trường chạy&#10;IS_KAGGLE = os.path.exists('/kaggle/input')&#10;&#10;# ============================================================================&#10;# CẤU HÌNH ĐƯỜNG DẪN&#10;# ============================================================================&#10;if IS_KAGGLE:&#10;    TRAINING_DATA_DIR = &quot;/kaggle/working/training_data&quot;&#10;    MODEL_DIR = &quot;/kaggle/working/models&quot;&#10;    LOG_DIR = &quot;/kaggle/working/logs&quot;&#10;    print(&quot; Đang chạy trên KAGGLE&quot;)&#10;else:&#10;    TRAINING_DATA_DIR = r&quot;D:\PROJECT\Machine Learning\IOT\CNN\training_data&quot;&#10;    MODEL_DIR = r&quot;D:\PROJECT\Machine Learning\IOT\CNN\models&quot;&#10;    LOG_DIR = r&quot;D:\PROJECT\Machine Learning\IOT\CNN\logs&quot;&#10;    print(&quot; Đang chạy trên LOCAL&quot;)&#10;&#10;# ============================================================================&#10;# CẤU HÌNH HUẤN LUYỆN&#10;# ============================================================================&#10;&#10;# Hyperparameters&#10;BATCH_SIZE = 256        # Batch size cho training&#10;EPOCHS = 50             # Số epochs tối đa&#10;LEARNING_RATE = 0.001   # Learning rate ban đầu&#10;&#10;# Regularization&#10;DROPOUT_RATE = 0.5      # Dropout rate trước Flatten&#10;&#10;# Early stopping&#10;PATIENCE = 10           # Số epochs chờ trước khi dừng&#10;&#10;# Random seed&#10;RANDOM_SEED = 42&#10;np.random.seed(RANDOM_SEED)&#10;tf.random.set_seed(RANDOM_SEED)&#10;&#10;&#10;# ============================================================================&#10;# HÀM XÂY DỰNG MÔ HÌNH CNN&#10;# ============================================================================&#10;&#10;def build_cnn_model(input_shape):&#10;    &quot;&quot;&quot;&#10;    Xây dựng mô hình CNN cho phân loại binary&#10;&#10;    Kiến trúc theo yêu cầu:&#10;    - 5 lớp Conv1D với MaxPooling&#10;    - BatchNormalization và Dropout trước Flatten&#10;    - Output layer với sigmoid activation&#10;&#10;    Args:&#10;        input_shape: Shape của input (n_features, 1)&#10;&#10;    Returns:&#10;        model: Keras Sequential model&#10;    &quot;&quot;&quot;&#10;    print(&quot;\n&quot; + &quot;=&quot;*80)&#10;    print(&quot;️ ĐANG XÂY DỰNG MÔ HÌNH CNN&quot;)&#10;    print(&quot;=&quot;*80)&#10;    print(f&quot;   Input shape: {input_shape}&quot;)&#10;&#10;    model = Sequential(name='CNN_Binary_Classification')&#10;&#10;    # Input layer&#10;    model.add(Input(shape=input_shape))&#10;&#10;    # ========== KHỐI CONV 1 ==========&#10;    # Conv1D (32 filters, kernel 2x1) -&gt; MaxPooling1D (2)&#10;    model.add(Conv1D(&#10;        filters=32,&#10;        kernel_size=2,&#10;        activation='relu',&#10;        padding='same',  # Giữ nguyên kích thước&#10;        name='conv1d_1'&#10;    ))&#10;    model.add(MaxPooling1D(pool_size=2, name='maxpool_1'))&#10;&#10;    # ========== KHỐI CONV 2 ==========&#10;    # Conv1D (32 filters, kernel 2x1) -&gt; MaxPooling1D (2)&#10;    model.add(Conv1D(&#10;        filters=32,&#10;        kernel_size=2,&#10;        activation='relu',&#10;        padding='same',&#10;        name='conv1d_2'&#10;    ))&#10;    model.add(MaxPooling1D(pool_size=2, name='maxpool_2'))&#10;&#10;    # ========== KHỐI CONV 3 ==========&#10;    # Conv1D (64 filters, kernel 2x1) -&gt; MaxPooling1D (2)&#10;    model.add(Conv1D(&#10;        filters=64,&#10;        kernel_size=2,&#10;        activation='relu',&#10;        padding='same',&#10;        name='conv1d_3'&#10;    ))&#10;    model.add(MaxPooling1D(pool_size=2, name='maxpool_3'))&#10;&#10;    # ========== KHỐI CONV 4 ==========&#10;    # Conv1D (64 filters, kernel 2x1) -&gt; MaxPooling1D (2)&#10;    model.add(Conv1D(&#10;        filters=64,&#10;        kernel_size=2,&#10;        activation='relu',&#10;        padding='same',&#10;        name='conv1d_4'&#10;    ))&#10;    model.add(MaxPooling1D(pool_size=2, name='maxpool_4'))&#10;&#10;    # ========== KHỐI CONV 5 ==========&#10;    # Conv1D (64 filters, kernel 2x1) -&gt; MaxPooling1D (2)&#10;    model.add(Conv1D(&#10;        filters=64,&#10;        kernel_size=2,&#10;        activation='relu',&#10;        padding='same',&#10;        name='conv1d_5'&#10;    ))&#10;    model.add(MaxPooling1D(pool_size=2, name='maxpool_5'))&#10;&#10;    # ========== REGULARIZATION ==========&#10;    # BatchNormalization và Dropout trước Flatten&#10;    model.add(BatchNormalization(name='batch_norm'))&#10;    model.add(Dropout(DROPOUT_RATE, name='dropout'))&#10;&#10;    # ========== FLATTEN ==========&#10;    model.add(Flatten(name='flatten'))&#10;&#10;    # ========== OUTPUT LAYER ==========&#10;    # Dense(1, activation='sigmoid') cho binary classification&#10;    model.add(Dense(1, activation='sigmoid', name='output'))&#10;&#10;    # ========== COMPILE ==========&#10;    model.compile(&#10;        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),&#10;        loss='binary_crossentropy',&#10;        metrics=[&#10;            'accuracy',&#10;            Precision(name='precision'),&#10;            Recall(name='recall')&#10;        ]&#10;    )&#10;&#10;    # In tóm tắt mô hình&#10;    print(&quot;\n    KIẾN TRÚC MÔ HÌNH:&quot;)&#10;    model.summary()&#10;&#10;    return model&#10;&#10;&#10;def load_training_data(data_dir):&#10;    &quot;&quot;&quot;&#10;    Load dữ liệu training đã được chuẩn bị từ step 2&#10;&#10;    Args:&#10;        data_dir: Đường dẫn thư mục chứa dữ liệu&#10;&#10;    Returns:&#10;        X_train, X_val, X_test, y_train, y_val, y_test, class_weights&#10;    &quot;&quot;&quot;&#10;    print(&quot;\n&quot; + &quot;=&quot;*80)&#10;    print(&quot; ĐANG LOAD DỮ LIỆU TRAINING...&quot;)&#10;    print(&quot;=&quot;*80)&#10;&#10;    data_dir = Path(data_dir)&#10;&#10;    # Load numpy arrays&#10;    X_train = np.load(data_dir / 'X_train.npy')&#10;    X_val = np.load(data_dir / 'X_val.npy')&#10;    X_test = np.load(data_dir / 'X_test.npy')&#10;    y_train = np.load(data_dir / 'y_train.npy')&#10;    y_val = np.load(data_dir / 'y_val.npy')&#10;    y_test = np.load(data_dir / 'y_test.npy')&#10;&#10;    print(f&quot;   ✅ X_train: {X_train.shape}&quot;)&#10;    print(f&quot;   ✅ X_val:   {X_val.shape}&quot;)&#10;    print(f&quot;   ✅ X_test:  {X_test.shape}&quot;)&#10;    print(f&quot;   ✅ y_train: {y_train.shape}&quot;)&#10;    print(f&quot;   ✅ y_val:   {y_val.shape}&quot;)&#10;    print(f&quot;   ✅ y_test:  {y_test.shape}&quot;)&#10;&#10;    # Load class weights nếu có&#10;    class_weights = None&#10;    class_weights_path = data_dir / 'class_weights.pkl'&#10;    if class_weights_path.exists():&#10;        with open(class_weights_path, 'rb') as f:&#10;            class_weights = pickle.load(f)&#10;        print(f&quot;\n   ⚖️ Class weights loaded:&quot;)&#10;        print(f&quot;      Class 0 (Benign): {class_weights[0]:.4f}&quot;)&#10;        print(f&quot;      Class 1 (Attack): {class_weights[1]:.4f}&quot;)&#10;&#10;    # Thống kê phân bố&#10;    print(f&quot;\n    PHÂN BỐ DỮ LIỆU:&quot;)&#10;    for name, y in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:&#10;        benign = (y == 0).sum()&#10;        attack = (y == 1).sum()&#10;        total = len(y)&#10;        print(f&quot;      {name}: Benign={benign:,} ({benign/total*100:.1f}%), Attack={attack:,} ({attack/total*100:.1f}%)&quot;)&#10;&#10;    return X_train, X_val, X_test, y_train, y_val, y_test, class_weights&#10;&#10;&#10;def create_callbacks(model_dir, log_dir):&#10;    &quot;&quot;&quot;&#10;    Tạo các callbacks cho training&#10;&#10;    Callbacks:&#10;    - EarlyStopping: Dừng sớm khi val_loss không giảm&#10;    - ModelCheckpoint: Lưu model tốt nhất&#10;    - ReduceLROnPlateau: Giảm learning rate khi plateau&#10;    - TensorBoard: Logging cho visualization&#10;    &quot;&quot;&quot;&#10;    print(&quot;\n ĐANG CẤU HÌNH CALLBACKS...&quot;)&#10;&#10;    model_dir = Path(model_dir)&#10;    log_dir = Path(log_dir)&#10;    model_dir.mkdir(parents=True, exist_ok=True)&#10;    log_dir.mkdir(parents=True, exist_ok=True)&#10;&#10;    callbacks = []&#10;&#10;    # 1. Early Stopping&#10;    # Dừng training khi val_loss không cải thiện sau PATIENCE epochs&#10;    early_stopping = EarlyStopping(&#10;        monitor='val_loss',&#10;        patience=PATIENCE,&#10;        verbose=1,&#10;        mode='min',&#10;        restore_best_weights=True  # Khôi phục weights tốt nhất&#10;    )&#10;    callbacks.append(early_stopping)&#10;    print(f&quot;   ✅ EarlyStopping: patience={PATIENCE}&quot;)&#10;&#10;    # 2. Model Checkpoint&#10;    # Lưu model có val_loss thấp nhất&#10;    checkpoint_path = model_dir / 'best_model.keras'&#10;    model_checkpoint = ModelCheckpoint(&#10;        filepath=str(checkpoint_path),&#10;        monitor='val_loss',&#10;        verbose=1,&#10;        save_best_only=True,&#10;        mode='min'&#10;    )&#10;    callbacks.append(model_checkpoint)&#10;    print(f&quot;   ✅ ModelCheckpoint: {checkpoint_path}&quot;)&#10;&#10;    # 3. Reduce Learning Rate on Plateau&#10;    # Giảm LR khi val_loss không giảm&#10;    reduce_lr = ReduceLROnPlateau(&#10;        monitor='val_loss',&#10;        factor=0.5,        # Giảm LR còn 1/2&#10;        patience=5,        # Chờ 5 epochs&#10;        min_lr=1e-7,       # LR tối thiểu&#10;        verbose=1&#10;    )&#10;    callbacks.append(reduce_lr)&#10;    print(f&quot;   ✅ ReduceLROnPlateau: factor=0.5, patience=5&quot;)&#10;&#10;    # 4. TensorBoard (optional)&#10;    tensorboard_log = log_dir / datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)&#10;    tensorboard = TensorBoard(&#10;        log_dir=str(tensorboard_log),&#10;        histogram_freq=1&#10;    )&#10;    callbacks.append(tensorboard)&#10;    print(f&quot;   ✅ TensorBoard: {tensorboard_log}&quot;)&#10;&#10;    return callbacks&#10;&#10;&#10;def train_model(model, X_train, y_train, X_val, y_val, class_weights, callbacks):&#10;    &quot;&quot;&quot;&#10;    Huấn luyện mô hình&#10;&#10;    Args:&#10;        model: Keras model&#10;        X_train, y_train: Dữ liệu training&#10;        X_val, y_val: Dữ liệu validation&#10;        class_weights: Dictionary class weights&#10;        callbacks: List các callbacks&#10;&#10;    Returns:&#10;        history: Training history&#10;    &quot;&quot;&quot;&#10;    print(&quot;\n&quot; + &quot;=&quot;*80)&#10;    print(&quot; BẮT ĐẦU HUẤN LUYỆN MÔ HÌNH&quot;)&#10;    print(&quot;=&quot;*80)&#10;    print(f&quot;   Epochs: {EPOCHS}&quot;)&#10;    print(f&quot;   Batch size: {BATCH_SIZE}&quot;)&#10;    print(f&quot;   Learning rate: {LEARNING_RATE}&quot;)&#10;    print(f&quot;   Class weights: {'Có' if class_weights else 'Không'}&quot;)&#10;&#10;    start_time = datetime.now()&#10;&#10;    history = model.fit(&#10;        X_train, y_train,&#10;        batch_size=BATCH_SIZE,&#10;        epochs=EPOCHS,&#10;        validation_data=(X_val, y_val),&#10;        class_weight=class_weights,  # Sử dụng class weights để xử lý imbalance&#10;        callbacks=callbacks,&#10;        verbose=1&#10;    )&#10;&#10;    end_time = datetime.now()&#10;    training_time = (end_time - start_time).total_seconds()&#10;&#10;    print(f&quot;\n   ⏱️ Thời gian training: {training_time/60:.2f} phút&quot;)&#10;    print(f&quot;    Best val_loss: {min(history.history['val_loss']):.4f}&quot;)&#10;    print(f&quot;    Best val_accuracy: {max(history.history['val_accuracy']):.4f}&quot;)&#10;&#10;    return history, training_time&#10;&#10;&#10;def evaluate_model(model, X_test, y_test):&#10;    &quot;&quot;&quot;&#10;    Đánh giá mô hình trên test set&#10;&#10;    Args:&#10;        model: Trained model&#10;        X_test, y_test: Dữ liệu test&#10;&#10;    Returns:&#10;        results: Dictionary kết quả đánh giá&#10;    &quot;&quot;&quot;&#10;    print(&quot;\n&quot; + &quot;=&quot;*80)&#10;    print(&quot; ĐÁNH GIÁ MÔ HÌNH TRÊN TEST SET&quot;)&#10;    print(&quot;=&quot;*80)&#10;&#10;    # Evaluate&#10;    loss, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=1)&#10;&#10;    # Tính F1-score&#10;    f1_score = 2 * (precision * recall) / (precision + recall + 1e-7)&#10;&#10;    results = {&#10;        'test_loss': float(loss),&#10;        'test_accuracy': float(accuracy),&#10;        'test_precision': float(precision),&#10;        'test_recall': float(recall),&#10;        'test_f1_score': float(f1_score)&#10;    }&#10;&#10;    print(f&quot;\n    KẾT QUẢ:&quot;)&#10;    print(f&quot;   {'='*40}&quot;)&#10;    print(f&quot;   Loss:      {loss:.4f}&quot;)&#10;    print(f&quot;   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)&quot;)&#10;    print(f&quot;   Precision: {precision:.4f}&quot;)&#10;    print(f&quot;   Recall:    {recall:.4f}&quot;)&#10;    print(f&quot;   F1-Score:  {f1_score:.4f}&quot;)&#10;    print(f&quot;   {'='*40}&quot;)&#10;&#10;    # Predictions cho confusion matrix&#10;    y_pred_prob = model.predict(X_test, verbose=0)&#10;    y_pred = (y_pred_prob &gt; 0.5).astype(int).flatten()&#10;&#10;    # Confusion matrix&#10;    from sklearn.metrics import confusion_matrix, classification_report&#10;&#10;    cm = confusion_matrix(y_test, y_pred)&#10;    print(f&quot;\n    CONFUSION MATRIX:&quot;)&#10;    print(f&quot;                 Predicted&quot;)&#10;    print(f&quot;                 Benign  Attack&quot;)&#10;    print(f&quot;   Actual Benign  {cm[0,0]:&gt;6}  {cm[0,1]:&gt;6}&quot;)&#10;    print(f&quot;   Actual Attack  {cm[1,0]:&gt;6}  {cm[1,1]:&gt;6}&quot;)&#10;&#10;    print(f&quot;\n    CLASSIFICATION REPORT:&quot;)&#10;    report = classification_report(y_test, y_pred, target_names=['Benign', 'Attack'])&#10;    print(report)&#10;&#10;    # Lưu classification report dạng dictionary&#10;    report_dict = classification_report(y_test, y_pred, target_names=['Benign', 'Attack'], output_dict=True)&#10;&#10;    # Thêm confusion matrix và các metrics khác vào results&#10;    results['confusion_matrix'] = cm.tolist()&#10;    results['classification_report'] = report_dict&#10;&#10;    # Thêm các metrics chi tiết cho từng class&#10;    results['benign_precision'] = float(report_dict['Benign']['precision'])&#10;    results['benign_recall'] = float(report_dict['Benign']['recall'])&#10;    results['benign_f1'] = float(report_dict['Benign']['f1-score'])&#10;    results['attack_precision'] = float(report_dict['Attack']['precision'])&#10;    results['attack_recall'] = float(report_dict['Attack']['recall'])&#10;    results['attack_f1'] = float(report_dict['Attack']['f1-score'])&#10;&#10;    # Tính thêm một số metrics bổ sung&#10;    tn, fp, fn, tp = cm.ravel()&#10;    results['true_negative'] = int(tn)&#10;    results['false_positive'] = int(fp)&#10;    results['false_negative'] = int(fn)&#10;    results['true_positive'] = int(tp)&#10;    results['specificity'] = float(tn / (tn + fp + 1e-7))  # True Negative Rate&#10;    results['false_positive_rate'] = float(fp / (fp + tn + 1e-7))&#10;    results['false_negative_rate'] = float(fn / (fn + tp + 1e-7))&#10;&#10;    return results, y_pred, y_pred_prob&#10;&#10;&#10;def save_model_and_results(model, history, results, training_time, model_dir, y_pred=None, y_pred_prob=None):&#10;    &quot;&quot;&quot;&#10;    Lưu model và kết quả training&#10;&#10;    Args:&#10;        model: Trained model&#10;        history: Training history&#10;        results: Evaluation results&#10;        training_time: Thời gian training (seconds)&#10;        model_dir: Đường dẫn lưu&#10;        y_pred: Predictions (optional)&#10;        y_pred_prob: Prediction probabilities (optional)&#10;    &quot;&quot;&quot;&#10;    print(&quot;\n&quot; + &quot;=&quot;*80)&#10;    print(&quot; ĐANG LƯU MODEL VÀ KẾT QUẢ...&quot;)&#10;    print(&quot;=&quot;*80)&#10;&#10;    model_dir = Path(model_dir)&#10;    model_dir.mkdir(parents=True, exist_ok=True)&#10;&#10;    # Lưu model cuối cùng&#10;    final_model_path = model_dir / 'final_model.keras'&#10;    model.save(final_model_path)&#10;    print(f&quot;   ✅ Final model: {final_model_path}&quot;)&#10;&#10;    # Lưu model weights&#10;    weights_path = model_dir / 'model_weights.weights.h5'&#10;    model.save_weights(weights_path)&#10;    print(f&quot;   ✅ Model weights: {weights_path}&quot;)&#10;&#10;    # Lưu training history&#10;    history_dict = {key: [float(v) for v in values] for key, values in history.history.items()}&#10;    with open(model_dir / 'training_history.json', 'w') as f:&#10;        json.dump(history_dict, f, indent=4)&#10;    print(f&quot;   ✅ Training history: training_history.json&quot;)&#10;&#10;    # Lưu kết quả đánh giá với thông tin bổ sung&#10;    results['training_time_seconds'] = float(training_time)&#10;    results['training_time_minutes'] = float(training_time / 60)&#10;    results['epochs_trained'] = int(len(history.history['loss']))&#10;&#10;    # Thêm best validation metrics&#10;    results['best_val_loss'] = float(min(history.history['val_loss']))&#10;    results['best_val_accuracy'] = float(max(history.history['val_accuracy']))&#10;    results['best_val_precision'] = float(max(history.history['val_precision']))&#10;    results['best_val_recall'] = float(max(history.history['val_recall']))&#10;&#10;    # Tính best val F1-score&#10;    val_precisions = history.history['val_precision']&#10;    val_recalls = history.history['val_recall']&#10;    val_f1_scores = [2 * (p * r) / (p + r + 1e-7) for p, r in zip(val_precisions, val_recalls)]&#10;    results['best_val_f1_score'] = float(max(val_f1_scores))&#10;&#10;    # Epoch nào đạt best val_loss&#10;    results['best_val_loss_epoch'] = int(np.argmin(history.history['val_loss']) + 1)&#10;    results['best_val_accuracy_epoch'] = int(np.argmax(history.history['val_accuracy']) + 1)&#10;&#10;    with open(model_dir / 'evaluation_results.json', 'w') as f:&#10;        json.dump(results, f, indent=4)&#10;    print(f&quot;   ✅ Evaluation results: evaluation_results.json&quot;)&#10;&#10;    # Lưu predictions nếu có&#10;    if y_pred is not None:&#10;        np.save(model_dir / 'y_pred.npy', y_pred)&#10;        print(f&quot;   ✅ Predictions: y_pred.npy&quot;)&#10;&#10;    if y_pred_prob is not None:&#10;        np.save(model_dir / 'y_pred_prob.npy', y_pred_prob)&#10;        print(f&quot;   ✅ Prediction probabilities: y_pred_prob.npy&quot;)&#10;&#10;    # Lưu cấu hình training&#10;    config = {&#10;        'batch_size': BATCH_SIZE,&#10;        'epochs': EPOCHS,&#10;        'learning_rate': LEARNING_RATE,&#10;        'dropout_rate': DROPOUT_RATE,&#10;        'patience': PATIENCE,&#10;        'random_seed': RANDOM_SEED,&#10;        'tensorflow_version': tf.__version__,&#10;        'created_at': datetime.now().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;)&#10;    }&#10;    with open(model_dir / 'training_config.json', 'w') as f:&#10;        json.dump(config, f, indent=4)&#10;    print(f&quot;   ✅ Training config: training_config.json&quot;)&#10;&#10;    print(f&quot;\n Tất cả file được lưu tại: {model_dir}&quot;)&#10;&#10;&#10;def plot_training_history(history, model_dir):&#10;    &quot;&quot;&quot;&#10;    Vẽ biểu đồ training history&#10;&#10;    Args:&#10;        history: Training history&#10;        model_dir: Đường dẫn lưu hình&#10;    &quot;&quot;&quot;&#10;    try:&#10;        import matplotlib.pyplot as plt&#10;&#10;        model_dir = Path(model_dir)&#10;        fig, axes = plt.subplots(2, 2, figsize=(14, 10))&#10;&#10;        # 1. Loss&#10;        axes[0, 0].plot(history.history['loss'], label='Train Loss')&#10;        axes[0, 0].plot(history.history['val_loss'], label='Val Loss')&#10;        axes[0, 0].set_title('Model Loss')&#10;        axes[0, 0].set_xlabel('Epoch')&#10;        axes[0, 0].set_ylabel('Loss')&#10;        axes[0, 0].legend()&#10;        axes[0, 0].grid(True)&#10;&#10;        # 2. Accuracy&#10;        axes[0, 1].plot(history.history['accuracy'], label='Train Accuracy')&#10;        axes[0, 1].plot(history.history['val_accuracy'], label='Val Accuracy')&#10;        axes[0, 1].set_title('Model Accuracy')&#10;        axes[0, 1].set_xlabel('Epoch')&#10;        axes[0, 1].set_ylabel('Accuracy')&#10;        axes[0, 1].legend()&#10;        axes[0, 1].grid(True)&#10;&#10;        # 3. Precision&#10;        axes[1, 0].plot(history.history['precision'], label='Train Precision')&#10;        axes[1, 0].plot(history.history['val_precision'], label='Val Precision')&#10;        axes[1, 0].set_title('Model Precision')&#10;        axes[1, 0].set_xlabel('Epoch')&#10;        axes[1, 0].set_ylabel('Precision')&#10;        axes[1, 0].legend()&#10;        axes[1, 0].grid(True)&#10;&#10;        # 4. Recall&#10;        axes[1, 1].plot(history.history['recall'], label='Train Recall')&#10;        axes[1, 1].plot(history.history['val_recall'], label='Val Recall')&#10;        axes[1, 1].set_title('Model Recall')&#10;        axes[1, 1].set_xlabel('Epoch')&#10;        axes[1, 1].set_ylabel('Recall')&#10;        axes[1, 1].legend()&#10;        axes[1, 1].grid(True)&#10;&#10;        plt.tight_layout()&#10;        plt.savefig(model_dir / 'training_history.png', dpi=150)&#10;        plt.close()&#10;        print(f&quot;   ✅ Training history plot: training_history.png&quot;)&#10;&#10;    except ImportError:&#10;        print(&quot;   ⚠️ matplotlib không có sẵn, bỏ qua việc vẽ biểu đồ&quot;)&#10;&#10;&#10;def plot_confusion_matrix(cm, model_dir):&#10;    &quot;&quot;&quot;&#10;    Vẽ confusion matrix dưới dạng heatmap&#10;&#10;    Args:&#10;        cm: Confusion matrix (numpy array hoặc list)&#10;        model_dir: Đường dẫn lưu hình&#10;    &quot;&quot;&quot;&#10;    try:&#10;        import matplotlib.pyplot as plt&#10;        import seaborn as sns&#10;&#10;        model_dir = Path(model_dir)&#10;&#10;        # Convert to numpy array if needed&#10;        if isinstance(cm, list):&#10;            cm = np.array(cm)&#10;&#10;        # Vẽ heatmap&#10;        plt.figure(figsize=(8, 6))&#10;        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',&#10;                   xticklabels=['Benign', 'Attack'],&#10;                   yticklabels=['Benign', 'Attack'],&#10;                   cbar_kws={'label': 'Count'})&#10;        plt.title('Confusion Matrix', fontsize=16, fontweight='bold')&#10;        plt.ylabel('Actual', fontsize=12)&#10;        plt.xlabel('Predicted', fontsize=12)&#10;        plt.tight_layout()&#10;        plt.savefig(model_dir / 'confusion_matrix.png', dpi=150, bbox_inches='tight')&#10;        plt.close()&#10;        print(f&quot;   ✅ Confusion matrix plot: confusion_matrix.png&quot;)&#10;&#10;        # Vẽ normalized confusion matrix&#10;        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]&#10;        plt.figure(figsize=(8, 6))&#10;        sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',&#10;                   xticklabels=['Benign', 'Attack'],&#10;                   yticklabels=['Benign', 'Attack'],&#10;                   cbar_kws={'label': 'Percentage'})&#10;        plt.title('Normalized Confusion Matrix', fontsize=16, fontweight='bold')&#10;        plt.ylabel('Actual', fontsize=12)&#10;        plt.xlabel('Predicted', fontsize=12)&#10;        plt.tight_layout()&#10;        plt.savefig(model_dir / 'confusion_matrix_normalized.png', dpi=150, bbox_inches='tight')&#10;        plt.close()&#10;        print(f&quot;   ✅ Normalized confusion matrix plot: confusion_matrix_normalized.png&quot;)&#10;&#10;    except ImportError as e:&#10;        print(f&quot;   ⚠️ matplotlib/seaborn không có sẵn, bỏ qua việc vẽ confusion matrix: {e}&quot;)&#10;&#10;&#10;def main():&#10;    &quot;&quot;&quot;Hàm chính để train model&quot;&quot;&quot;&#10;&#10;    print(&quot;\n&quot; + &quot;=&quot;*80)&#10;    print(&quot; HUẤN LUYỆN MÔ HÌNH CNN - PHÁT HIỆN LƯU LƯỢNG MẠNG BẤT THƯỜNG&quot;)&#10;    print(&quot;   Binary Classification: Benign vs Attack&quot;)&#10;    print(&quot;=&quot;*80)&#10;&#10;    # Bước 1: Load dữ liệu&#10;    X_train, X_val, X_test, y_train, y_val, y_test, class_weights = load_training_data(TRAINING_DATA_DIR)&#10;&#10;    # Bước 2: Xây dựng mô hình&#10;    input_shape = (X_train.shape[1], X_train.shape[2])  # (n_features, 1)&#10;    model = build_cnn_model(input_shape)&#10;&#10;    # Bước 3: Tạo callbacks&#10;    callbacks = create_callbacks(MODEL_DIR, LOG_DIR)&#10;&#10;    # Bước 4: Huấn luyện&#10;    history, training_time = train_model(&#10;        model, X_train, y_train, X_val, y_val, class_weights, callbacks&#10;    )&#10;&#10;    # Bước 5: Đánh giá&#10;    results, y_pred, y_pred_prob = evaluate_model(model, X_test, y_test)&#10;&#10;    # Bước 6: Lưu model và kết quả&#10;    save_model_and_results(model, history, results, training_time, MODEL_DIR, y_pred, y_pred_prob)&#10;&#10;    # Bước 7: Vẽ biểu đồ&#10;    plot_training_history(history, MODEL_DIR)&#10;&#10;    # Bước 8: Vẽ confusion matrix&#10;    if 'confusion_matrix' in results:&#10;        plot_confusion_matrix(results['confusion_matrix'], MODEL_DIR)&#10;&#10;    print(&quot;\n&quot; + &quot;=&quot;*80)&#10;    print(&quot;✅ HOÀN THÀNH HUẤN LUYỆN!&quot;)&#10;    print(f&quot;   Test Accuracy:  {results['test_accuracy']*100:.2f}%&quot;)&#10;    print(f&quot;   Test Precision: {results['test_precision']*100:.2f}%&quot;)&#10;    print(f&quot;   Test Recall:    {results['test_recall']*100:.2f}%&quot;)&#10;    print(f&quot;   Test F1-Score:  {results['test_f1_score']*100:.2f}%&quot;)&#10;    print(&quot;=&quot;*80)&#10;&#10;    return model, history, results&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    model, history, results = main()&#10;&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;BƯỚC 3: TRAIN CNN CHO PHÁT HIỆN LƯU LƯỢNG MẠNG IOT BẤT THƯỜNG&#10;Binary Classification: Benign (0) vs Attack (1)&#10;&quot;&quot;&quot;&#10;&#10;import os&#10;import numpy as np&#10;import pickle&#10;import json&#10;from pathlib import Path&#10;from datetime import datetime&#10;import warnings&#10;warnings.filterwarnings('ignore')&#10;&#10;import tensorflow as tf&#10;from tensorflow import keras&#10;from tensorflow.keras.models import Sequential&#10;from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, Input&#10;from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard&#10;from tensorflow.keras.metrics import Precision, Recall&#10;&#10;# Kiểm tra GPU&#10;print(f&quot;TensorFlow {tf.__version__}&quot;)&#10;gpus = tf.config.list_physical_devices('GPU')&#10;if gpus:&#10;    print(f&quot;✅ {len(gpus)} GPU(s)&quot;)&#10;    for gpu in gpus:&#10;        tf.config.experimental.set_memory_growth(gpu, True)&#10;else:&#10;    print(&quot;⚠️ CPU mode&quot;)&#10;&#10;IS_KAGGLE = os.path.exists('/kaggle/input')&#10;&#10;# Cấu hình đường dẫn&#10;if IS_KAGGLE:&#10;    TRAINING_DATA_DIR = &quot;/kaggle/working/training_data&quot;&#10;    MODEL_DIR = &quot;/kaggle/working/models&quot;&#10;    LOG_DIR = &quot;/kaggle/working/logs&quot;&#10;    print(&quot; Kaggle&quot;)&#10;else:&#10;    TRAINING_DATA_DIR = r&quot;D:\PROJECT\Machine Learning\IOT\CNN\training_data&quot;&#10;    MODEL_DIR = r&quot;D:\PROJECT\Machine Learning\IOT\CNN\models&quot;&#10;    LOG_DIR = r&quot;D:\PROJECT\Machine Learning\IOT\CNN\logs&quot;&#10;    print(&quot; Local&quot;)&#10;&#10;# Hyperparameters&#10;BATCH_SIZE = 256&#10;EPOCHS = 50&#10;LEARNING_RATE = 0.001&#10;DROPOUT_RATE = 0.5&#10;PATIENCE = 10&#10;RANDOM_SEED = 42&#10;&#10;np.random.seed(RANDOM_SEED)&#10;tf.random.set_seed(RANDOM_SEED)&#10;&#10;&#10;# ============================================================================&#10;# HÀM XÂY DỰNG MÔ HÌNH CNN&#10;# ============================================================================&#10;&#10;def build_cnn_model(input_shape):&#10;    &quot;&quot;&quot;&#10;    Xây dựng mô hình CNN cho phân loại binary&#10;&#10;    Kiến trúc theo yêu cầu:&#10;    - 5 lớp Conv1D với MaxPooling&#10;    - BatchNormalization và Dropout trước Flatten&#10;    - Output layer với sigmoid activation&#10;&#10;    Args:&#10;        input_shape: Shape của input (n_features, 1)&#10;&#10;    Returns:&#10;        model: Keras Sequential model&#10;    &quot;&quot;&quot;&#10;    print(&quot;\n&quot; + &quot;=&quot;*80)&#10;    print(&quot;️ ĐANG XÂY DỰNG MÔ HÌNH CNN&quot;)&#10;    print(&quot;=&quot;*80)&#10;    print(f&quot;   Input shape: {input_shape}&quot;)&#10;&#10;    model = Sequential(name='CNN_Binary_Classification')&#10;&#10;    # Input layer&#10;    model.add(Input(shape=input_shape))&#10;&#10;    # ========== KHỐI CONV 1 ==========&#10;    # Conv1D (32 filters, kernel 2x1) -&gt; MaxPooling1D (2)&#10;    model.add(Conv1D(&#10;        filters=32,&#10;        kernel_size=2,&#10;        activation='relu',&#10;        padding='same',  # Giữ nguyên kích thước&#10;        name='conv1d_1'&#10;    ))&#10;    model.add(MaxPooling1D(pool_size=2, name='maxpool_1'))&#10;&#10;    # Conv blocks&#10;    model.add(Conv1D(32, 2, activation='relu', padding='same', name='conv1d_2'))&#10;    model.add(MaxPooling1D(2, name='maxpool_2'))&#10;&#10;    model.add(Conv1D(64, 2, activation='relu', padding='same', name='conv1d_3'))&#10;    model.add(MaxPooling1D(2, name='maxpool_3'))&#10;&#10;    model.add(Conv1D(64, 2, activation='relu', padding='same', name='conv1d_4'))&#10;    model.add(MaxPooling1D(2, name='maxpool_4'))&#10;&#10;    model.add(Conv1D(64, 2, activation='relu', padding='same', name='conv1d_5'))&#10;    model.add(MaxPooling1D(2, name='maxpool_5'))&#10;&#10;    # Regularization&#10;    model.add(BatchNormalization(name='batch_norm'))&#10;    model.add(Dropout(DROPOUT_RATE, name='dropout'))&#10;&#10;    # Output&#10;    model.add(Flatten(name='flatten'))&#10;    model.add(Dense(1, activation='sigmoid', name='output'))&#10;&#10;    # Compile&#10;    model.compile(&#10;        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),&#10;        loss='binary_crossentropy',&#10;        metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]&#10;    )&#10;&#10;    model.summary()&#10;    return model&#10;&#10;&#10;def load_training_data(data_dir):&#10;    &quot;&quot;&quot;&#10;    Load dữ liệu training đã được chuẩn bị từ step 2&#10;&#10;    Args:&#10;        data_dir: Đường dẫn thư mục chứa dữ liệu&#10;&#10;    Returns:&#10;        X_train, X_val, X_test, y_train, y_val, y_test, class_weights&#10;    &quot;&quot;&quot;&#10;    print(&quot;\n&quot; + &quot;=&quot;*80)&#10;    print(&quot; ĐANG LOAD DỮ LIỆU TRAINING...&quot;)&#10;    print(&quot;=&quot;*80)&#10;&#10;    data_dir = Path(data_dir)&#10;&#10;    # Load numpy arrays&#10;    X_train = np.load(data_dir / 'X_train.npy')&#10;    X_val = np.load(data_dir / 'X_val.npy')&#10;    X_test = np.load(data_dir / 'X_test.npy')&#10;    y_train = np.load(data_dir / 'y_train.npy')&#10;    y_val = np.load(data_dir / 'y_val.npy')&#10;    y_test = np.load(data_dir / 'y_test.npy')&#10;&#10;    print(f&quot;   ✅ X_train: {X_train.shape}&quot;)&#10;    print(f&quot;   ✅ X_val:   {X_val.shape}&quot;)&#10;    print(f&quot;   ✅ X_test:  {X_test.shape}&quot;)&#10;    print(f&quot;   ✅ y_train: {y_train.shape}&quot;)&#10;    print(f&quot;   ✅ y_val:   {y_val.shape}&quot;)&#10;    print(f&quot;   ✅ y_test:  {y_test.shape}&quot;)&#10;&#10;    # Load class weights nếu có&#10;    class_weights = None&#10;    class_weights_path = data_dir / 'class_weights.pkl'&#10;    if class_weights_path.exists():&#10;        with open(class_weights_path, 'rb') as f:&#10;            class_weights = pickle.load(f)&#10;        print(f&quot;\n   ⚖️ Class weights loaded:&quot;)&#10;        print(f&quot;      Class 0 (Benign): {class_weights[0]:.4f}&quot;)&#10;        print(f&quot;      Class 1 (Attack): {class_weights[1]:.4f}&quot;)&#10;&#10;    # Thống kê phân bố&#10;    print(f&quot;\n    PHÂN BỐ DỮ LIỆU:&quot;)&#10;    for name, y in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:&#10;        benign = (y == 0).sum()&#10;        attack = (y == 1).sum()&#10;        total = len(y)&#10;        print(f&quot;      {name}: Benign={benign:,} ({benign/total*100:.1f}%), Attack={attack:,} ({attack/total*100:.1f}%)&quot;)&#10;&#10;    return X_train, X_val, X_test, y_train, y_val, y_test, class_weights&#10;&#10;&#10;def create_callbacks(model_dir, log_dir):&#10;    &quot;&quot;&quot;&#10;    Tạo các callbacks cho training&#10;&#10;    Callbacks:&#10;    - EarlyStopping: Dừng sớm khi val_loss không giảm&#10;    - ModelCheckpoint: Lưu model tốt nhất&#10;    - ReduceLROnPlateau: Giảm learning rate khi plateau&#10;    - TensorBoard: Logging cho visualization&#10;    &quot;&quot;&quot;&#10;    print(&quot;\n ĐANG CẤU HÌNH CALLBACKS...&quot;)&#10;&#10;    model_dir = Path(model_dir)&#10;    log_dir = Path(log_dir)&#10;    model_dir.mkdir(parents=True, exist_ok=True)&#10;    log_dir.mkdir(parents=True, exist_ok=True)&#10;&#10;    callbacks = []&#10;&#10;    # 1. Early Stopping&#10;    # Dừng training khi val_loss không cải thiện sau PATIENCE epochs&#10;    early_stopping = EarlyStopping(&#10;        monitor='val_loss',&#10;        patience=PATIENCE,&#10;        verbose=1,&#10;        mode='min',&#10;        restore_best_weights=True  # Khôi phục weights tốt nhất&#10;    )&#10;    callbacks.append(early_stopping)&#10;    print(f&quot;   ✅ EarlyStopping: patience={PATIENCE}&quot;)&#10;&#10;    # 2. Model Checkpoint&#10;    # Lưu model có val_loss thấp nhất&#10;    checkpoint_path = model_dir / 'best_model.keras'&#10;    model_checkpoint = ModelCheckpoint(&#10;        filepath=str(checkpoint_path),&#10;        monitor='val_loss',&#10;        verbose=1,&#10;        save_best_only=True,&#10;        mode='min'&#10;    )&#10;    callbacks.append(model_checkpoint)&#10;    print(f&quot;   ✅ ModelCheckpoint: {checkpoint_path}&quot;)&#10;&#10;    # 3. Reduce Learning Rate on Plateau&#10;    # Giảm LR khi val_loss không giảm&#10;    reduce_lr = ReduceLROnPlateau(&#10;        monitor='val_loss',&#10;        factor=0.5,        # Giảm LR còn 1/2&#10;        patience=5,        # Chờ 5 epochs&#10;        min_lr=1e-7,       # LR tối thiểu&#10;        verbose=1&#10;    )&#10;    callbacks.append(reduce_lr)&#10;    print(f&quot;   ✅ ReduceLROnPlateau: factor=0.5, patience=5&quot;)&#10;&#10;    # 4. TensorBoard (optional)&#10;    tensorboard_log = log_dir / datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)&#10;    tensorboard = TensorBoard(&#10;        log_dir=str(tensorboard_log),&#10;        histogram_freq=1&#10;    )&#10;    callbacks.append(tensorboard)&#10;    print(f&quot;   ✅ TensorBoard: {tensorboard_log}&quot;)&#10;&#10;    return callbacks&#10;&#10;&#10;def train_model(model, X_train, y_train, X_val, y_val, class_weights, callbacks):&#10;    &quot;&quot;&quot;&#10;    Huấn luyện mô hình&#10;&#10;    Args:&#10;        model: Keras model&#10;        X_train, y_train: Dữ liệu training&#10;        X_val, y_val: Dữ liệu validation&#10;        class_weights: Dictionary class weights&#10;        callbacks: List các callbacks&#10;&#10;    Returns:&#10;        history: Training history&#10;    &quot;&quot;&quot;&#10;    print(&quot;\n&quot; + &quot;=&quot;*80)&#10;    print(&quot; BẮT ĐẦU HUẤN LUYỆN MÔ HÌNH&quot;)&#10;    print(&quot;=&quot;*80)&#10;    print(f&quot;   Epochs: {EPOCHS}&quot;)&#10;    print(f&quot;   Batch size: {BATCH_SIZE}&quot;)&#10;    print(f&quot;   Learning rate: {LEARNING_RATE}&quot;)&#10;    print(f&quot;   Class weights: {'Có' if class_weights else 'Không'}&quot;)&#10;&#10;    start_time = datetime.now()&#10;&#10;    history = model.fit(&#10;        X_train, y_train,&#10;        batch_size=BATCH_SIZE,&#10;        epochs=EPOCHS,&#10;        validation_data=(X_val, y_val),&#10;        class_weight=class_weights,  # Sử dụng class weights để xử lý imbalance&#10;        callbacks=callbacks,&#10;        verbose=1&#10;    )&#10;&#10;    end_time = datetime.now()&#10;    training_time = (end_time - start_time).total_seconds()&#10;&#10;    print(f&quot;\n   ⏱️ Thời gian training: {training_time/60:.2f} phút&quot;)&#10;    print(f&quot;    Best val_loss: {min(history.history['val_loss']):.4f}&quot;)&#10;    print(f&quot;    Best val_accuracy: {max(history.history['val_accuracy']):.4f}&quot;)&#10;&#10;    return history, training_time&#10;&#10;&#10;def evaluate_model(model, X_test, y_test):&#10;    &quot;&quot;&quot;&#10;    Đánh giá mô hình trên test set&#10;&#10;    Args:&#10;        model: Trained model&#10;        X_test, y_test: Dữ liệu test&#10;&#10;    Returns:&#10;        results: Dictionary kết quả đánh giá&#10;    &quot;&quot;&quot;&#10;    print(&quot;\n&quot; + &quot;=&quot;*80)&#10;    print(&quot; ĐÁNH GIÁ MÔ HÌNH TRÊN TEST SET&quot;)&#10;    print(&quot;=&quot;*80)&#10;&#10;    # Evaluate&#10;    loss, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=1)&#10;&#10;    # Tính F1-score&#10;    f1_score = 2 * (precision * recall) / (precision + recall + 1e-7)&#10;&#10;    results = {&#10;        'test_loss': float(loss),&#10;        'test_accuracy': float(accuracy),&#10;        'test_precision': float(precision),&#10;        'test_recall': float(recall),&#10;        'test_f1_score': float(f1_score)&#10;    }&#10;&#10;    print(f&quot;\n    KẾT QUẢ:&quot;)&#10;    print(f&quot;   {'='*40}&quot;)&#10;    print(f&quot;   Loss:      {loss:.4f}&quot;)&#10;    print(f&quot;   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)&quot;)&#10;    print(f&quot;   Precision: {precision:.4f}&quot;)&#10;    print(f&quot;   Recall:    {recall:.4f}&quot;)&#10;    print(f&quot;   F1-Score:  {f1_score:.4f}&quot;)&#10;    print(f&quot;   {'='*40}&quot;)&#10;&#10;    # Predictions cho confusion matrix&#10;    y_pred_prob = model.predict(X_test, verbose=0)&#10;    y_pred = (y_pred_prob &gt; 0.5).astype(int).flatten()&#10;&#10;    # Confusion matrix&#10;    from sklearn.metrics import confusion_matrix, classification_report&#10;&#10;    cm = confusion_matrix(y_test, y_pred)&#10;    print(f&quot;\n    CONFUSION MATRIX:&quot;)&#10;    print(f&quot;                 Predicted&quot;)&#10;    print(f&quot;                 Benign  Attack&quot;)&#10;    print(f&quot;   Actual Benign  {cm[0,0]:&gt;6}  {cm[0,1]:&gt;6}&quot;)&#10;    print(f&quot;   Actual Attack  {cm[1,0]:&gt;6}  {cm[1,1]:&gt;6}&quot;)&#10;&#10;    print(f&quot;\n    CLASSIFICATION REPORT:&quot;)&#10;    report = classification_report(y_test, y_pred, target_names=['Benign', 'Attack'])&#10;    print(report)&#10;&#10;    # Lưu classification report dạng dictionary&#10;    report_dict = classification_report(y_test, y_pred, target_names=['Benign', 'Attack'], output_dict=True)&#10;&#10;    # Thêm confusion matrix và các metrics khác vào results&#10;    results['confusion_matrix'] = cm.tolist()&#10;    results['classification_report'] = report_dict&#10;&#10;    # Thêm các metrics chi tiết cho từng class&#10;    results['benign_precision'] = float(report_dict['Benign']['precision'])&#10;    results['benign_recall'] = float(report_dict['Benign']['recall'])&#10;    results['benign_f1'] = float(report_dict['Benign']['f1-score'])&#10;    results['attack_precision'] = float(report_dict['Attack']['precision'])&#10;    results['attack_recall'] = float(report_dict['Attack']['recall'])&#10;    results['attack_f1'] = float(report_dict['Attack']['f1-score'])&#10;&#10;    # Tính thêm một số metrics bổ sung&#10;    tn, fp, fn, tp = cm.ravel()&#10;    results['true_negative'] = int(tn)&#10;    results['false_positive'] = int(fp)&#10;    results['false_negative'] = int(fn)&#10;    results['true_positive'] = int(tp)&#10;    results['specificity'] = float(tn / (tn + fp + 1e-7))  # True Negative Rate&#10;    results['false_positive_rate'] = float(fp / (fp + tn + 1e-7))&#10;    results['false_negative_rate'] = float(fn / (fn + tp + 1e-7))&#10;&#10;    return results, y_pred, y_pred_prob&#10;&#10;&#10;def save_model_and_results(model, history, results, training_time, model_dir, y_pred=None, y_pred_prob=None):&#10;    &quot;&quot;&quot;&#10;    Lưu model và kết quả training&#10;&#10;    Args:&#10;        model: Trained model&#10;        history: Training history&#10;        results: Evaluation results&#10;        training_time: Thời gian training (seconds)&#10;        model_dir: Đường dẫn lưu&#10;        y_pred: Predictions (optional)&#10;        y_pred_prob: Prediction probabilities (optional)&#10;    &quot;&quot;&quot;&#10;    print(&quot;\n&quot; + &quot;=&quot;*80)&#10;    print(&quot; ĐANG LƯU MODEL VÀ KẾT QUẢ...&quot;)&#10;    print(&quot;=&quot;*80)&#10;&#10;    model_dir = Path(model_dir)&#10;    model_dir.mkdir(parents=True, exist_ok=True)&#10;&#10;    # Lưu model cuối cùng&#10;    final_model_path = model_dir / 'final_model.keras'&#10;    model.save(final_model_path)&#10;    print(f&quot;   ✅ Final model: {final_model_path}&quot;)&#10;&#10;    # Lưu model weights&#10;    weights_path = model_dir / 'model_weights.weights.h5'&#10;    model.save_weights(weights_path)&#10;    print(f&quot;   ✅ Model weights: {weights_path}&quot;)&#10;&#10;    # Lưu training history&#10;    history_dict = {key: [float(v) for v in values] for key, values in history.history.items()}&#10;    with open(model_dir / 'training_history.json', 'w') as f:&#10;        json.dump(history_dict, f, indent=4)&#10;    print(f&quot;   ✅ Training history: training_history.json&quot;)&#10;&#10;    # Lưu kết quả đánh giá với thông tin bổ sung&#10;    results['training_time_seconds'] = float(training_time)&#10;    results['training_time_minutes'] = float(training_time / 60)&#10;    results['epochs_trained'] = int(len(history.history['loss']))&#10;&#10;    # Thêm best validation metrics&#10;    results['best_val_loss'] = float(min(history.history['val_loss']))&#10;    results['best_val_accuracy'] = float(max(history.history['val_accuracy']))&#10;    results['best_val_precision'] = float(max(history.history['val_precision']))&#10;    results['best_val_recall'] = float(max(history.history['val_recall']))&#10;&#10;    # Tính best val F1-score&#10;    val_precisions = history.history['val_precision']&#10;    val_recalls = history.history['val_recall']&#10;    val_f1_scores = [2 * (p * r) / (p + r + 1e-7) for p, r in zip(val_precisions, val_recalls)]&#10;    results['best_val_f1_score'] = float(max(val_f1_scores))&#10;&#10;    # Epoch nào đạt best val_loss&#10;    results['best_val_loss_epoch'] = int(np.argmin(history.history['val_loss']) + 1)&#10;    results['best_val_accuracy_epoch'] = int(np.argmax(history.history['val_accuracy']) + 1)&#10;&#10;    with open(model_dir / 'evaluation_results.json', 'w') as f:&#10;        json.dump(results, f, indent=4)&#10;    print(f&quot;   ✅ Evaluation results: evaluation_results.json&quot;)&#10;&#10;    # Lưu predictions nếu có&#10;    if y_pred is not None:&#10;        np.save(model_dir / 'y_pred.npy', y_pred)&#10;        print(f&quot;   ✅ Predictions: y_pred.npy&quot;)&#10;&#10;    if y_pred_prob is not None:&#10;        np.save(model_dir / 'y_pred_prob.npy', y_pred_prob)&#10;        print(f&quot;   ✅ Prediction probabilities: y_pred_prob.npy&quot;)&#10;&#10;    # Lưu cấu hình training&#10;    config = {&#10;        'batch_size': BATCH_SIZE,&#10;        'epochs': EPOCHS,&#10;        'learning_rate': LEARNING_RATE,&#10;        'dropout_rate': DROPOUT_RATE,&#10;        'patience': PATIENCE,&#10;        'random_seed': RANDOM_SEED,&#10;        'tensorflow_version': tf.__version__,&#10;        'created_at': datetime.now().strftime(&quot;%Y-%m-%d %H:%M:%S&quot;)&#10;    }&#10;    with open(model_dir / 'training_config.json', 'w') as f:&#10;        json.dump(config, f, indent=4)&#10;    print(f&quot;   ✅ Training config: training_config.json&quot;)&#10;&#10;    print(f&quot;\n Tất cả file được lưu tại: {model_dir}&quot;)&#10;&#10;&#10;def plot_training_history(history, model_dir):&#10;    &quot;&quot;&quot;&#10;    Vẽ biểu đồ training history&#10;&#10;    Args:&#10;        history: Training history&#10;        model_dir: Đường dẫn lưu hình&#10;    &quot;&quot;&quot;&#10;    try:&#10;        import matplotlib.pyplot as plt&#10;&#10;        model_dir = Path(model_dir)&#10;        fig, axes = plt.subplots(2, 2, figsize=(14, 10))&#10;&#10;        # 1. Loss&#10;        axes[0, 0].plot(history.history['loss'], label='Train Loss')&#10;        axes[0, 0].plot(history.history['val_loss'], label='Val Loss')&#10;        axes[0, 0].set_title('Model Loss')&#10;        axes[0, 0].set_xlabel('Epoch')&#10;        axes[0, 0].set_ylabel('Loss')&#10;        axes[0, 0].legend()&#10;        axes[0, 0].grid(True)&#10;&#10;        # 2. Accuracy&#10;        axes[0, 1].plot(history.history['accuracy'], label='Train Accuracy')&#10;        axes[0, 1].plot(history.history['val_accuracy'], label='Val Accuracy')&#10;        axes[0, 1].set_title('Model Accuracy')&#10;        axes[0, 1].set_xlabel('Epoch')&#10;        axes[0, 1].set_ylabel('Accuracy')&#10;        axes[0, 1].legend()&#10;        axes[0, 1].grid(True)&#10;&#10;        # 3. Precision&#10;        axes[1, 0].plot(history.history['precision'], label='Train Precision')&#10;        axes[1, 0].plot(history.history['val_precision'], label='Val Precision')&#10;        axes[1, 0].set_title('Model Precision')&#10;        axes[1, 0].set_xlabel('Epoch')&#10;        axes[1, 0].set_ylabel('Precision')&#10;        axes[1, 0].legend()&#10;        axes[1, 0].grid(True)&#10;&#10;        # 4. Recall&#10;        axes[1, 1].plot(history.history['recall'], label='Train Recall')&#10;        axes[1, 1].plot(history.history['val_recall'], label='Val Recall')&#10;        axes[1, 1].set_title('Model Recall')&#10;        axes[1, 1].set_xlabel('Epoch')&#10;        axes[1, 1].set_ylabel('Recall')&#10;        axes[1, 1].legend()&#10;        axes[1, 1].grid(True)&#10;&#10;        plt.tight_layout()&#10;        plt.savefig(model_dir / 'training_history.png', dpi=150)&#10;        plt.close()&#10;        print(f&quot;   ✅ Training history plot: training_history.png&quot;)&#10;&#10;    except ImportError:&#10;        print(&quot;   ⚠️ matplotlib không có sẵn, bỏ qua việc vẽ biểu đồ&quot;)&#10;&#10;&#10;def plot_confusion_matrix(cm, model_dir):&#10;    &quot;&quot;&quot;&#10;    Vẽ confusion matrix dưới dạng heatmap&#10;&#10;    Args:&#10;        cm: Confusion matrix (numpy array hoặc list)&#10;        model_dir: Đường dẫn lưu hình&#10;    &quot;&quot;&quot;&#10;    try:&#10;        import matplotlib.pyplot as plt&#10;        import seaborn as sns&#10;&#10;        model_dir = Path(model_dir)&#10;&#10;        # Convert to numpy array if needed&#10;        if isinstance(cm, list):&#10;            cm = np.array(cm)&#10;&#10;        # Vẽ heatmap&#10;        plt.figure(figsize=(8, 6))&#10;        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',&#10;                   xticklabels=['Benign', 'Attack'],&#10;                   yticklabels=['Benign', 'Attack'],&#10;                   cbar_kws={'label': 'Count'})&#10;        plt.title('Confusion Matrix', fontsize=16, fontweight='bold')&#10;        plt.ylabel('Actual', fontsize=12)&#10;        plt.xlabel('Predicted', fontsize=12)&#10;        plt.tight_layout()&#10;        plt.savefig(model_dir / 'confusion_matrix.png', dpi=150, bbox_inches='tight')&#10;        plt.close()&#10;        print(f&quot;   ✅ Confusion matrix plot: confusion_matrix.png&quot;)&#10;&#10;        # Vẽ normalized confusion matrix&#10;        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]&#10;        plt.figure(figsize=(8, 6))&#10;        sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',&#10;                   xticklabels=['Benign', 'Attack'],&#10;                   yticklabels=['Benign', 'Attack'],&#10;                   cbar_kws={'label': 'Percentage'})&#10;        plt.title('Normalized Confusion Matrix', fontsize=16, fontweight='bold')&#10;        plt.ylabel('Actual', fontsize=12)&#10;        plt.xlabel('Predicted', fontsize=12)&#10;        plt.tight_layout()&#10;        plt.savefig(model_dir / 'confusion_matrix_normalized.png', dpi=150, bbox_inches='tight')&#10;        plt.close()&#10;        print(f&quot;   ✅ Normalized confusion matrix plot: confusion_matrix_normalized.png&quot;)&#10;&#10;    except ImportError as e:&#10;        print(f&quot;   ⚠️ matplotlib/seaborn không có sẵn, bỏ qua việc vẽ confusion matrix: {e}&quot;)&#10;&#10;&#10;def main():&#10;    &quot;&quot;&quot;Hàm chính để train model&quot;&quot;&quot;&#10;&#10;    print(&quot;\n&quot; + &quot;=&quot;*80)&#10;    print(&quot; HUẤN LUYỆN MÔ HÌNH CNN - PHÁT HIỆN LƯU LƯỢNG MẠNG BẤT THƯỜNG&quot;)&#10;    print(&quot;   Binary Classification: Benign vs Attack&quot;)&#10;    print(&quot;=&quot;*80)&#10;&#10;    # Bước 1: Load dữ liệu&#10;    X_train, X_val, X_test, y_train, y_val, y_test, class_weights = load_training_data(TRAINING_DATA_DIR)&#10;&#10;    # Bước 2: Xây dựng mô hình&#10;    input_shape = (X_train.shape[1], X_train.shape[2])  # (n_features, 1)&#10;    model = build_cnn_model(input_shape)&#10;&#10;    # Bước 3: Tạo callbacks&#10;    callbacks = create_callbacks(MODEL_DIR, LOG_DIR)&#10;&#10;    # Bước 4: Huấn luyện&#10;    history, training_time = train_model(&#10;        model, X_train, y_train, X_val, y_val, class_weights, callbacks&#10;    )&#10;&#10;    # Bước 5: Đánh giá&#10;    results, y_pred, y_pred_prob = evaluate_model(model, X_test, y_test)&#10;&#10;    # Bước 6: Lưu model và kết quả&#10;    save_model_and_results(model, history, results, training_time, MODEL_DIR, y_pred, y_pred_prob)&#10;&#10;    # Bước 7: Vẽ biểu đồ&#10;    plot_training_history(history, MODEL_DIR)&#10;&#10;    # Bước 8: Vẽ confusion matrix&#10;    if 'confusion_matrix' in results:&#10;        plot_confusion_matrix(results['confusion_matrix'], MODEL_DIR)&#10;&#10;    print(&quot;\n&quot; + &quot;=&quot;*80)&#10;    print(&quot;✅ HOÀN THÀNH HUẤN LUYỆN!&quot;)&#10;    print(f&quot;   Test Accuracy:  {results['test_accuracy']*100:.2f}%&quot;)&#10;    print(f&quot;   Test Precision: {results['test_precision']*100:.2f}%&quot;)&#10;    print(f&quot;   Test Recall:    {results['test_recall']*100:.2f}%&quot;)&#10;    print(f&quot;   Test F1-Score:  {results['test_f1_score']*100:.2f}%&quot;)&#10;    print(&quot;=&quot;*80)&#10;&#10;    return model, history, results&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    model, history, results = main()&#10;&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/PROGRESS_BAR_GUIDE.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/PROGRESS_BAR_GUIDE.md" />
              <option name="updatedContent" value="#  Progress Bar - Hướng dẫn sử dụng&#10;&#10;## ✅ ĐÃ THÊM PROGRESS BAR&#10;&#10;Script `preprocess_data.py` đã được cập nhật với **thanh tiến trình (progress bar)** để theo dõi tiến độ xử lý!&#10;&#10;---&#10;&#10;##  Cài đặt (khuyến nghị)&#10;&#10;### Cài tqdm để có progress bar:&#10;&#10;```powershell&#10;pip install tqdm&#10;```&#10;&#10;**Sau khi cài xong, chạy:**&#10;```powershell&#10;python preprocess_data.py&#10;```&#10;&#10;---&#10;&#10;##  Các thanh tiến trình&#10;&#10;### 1. **Loading CSV files**&#10;```&#10; Loading CSV files: 100%|████████| 10/10 [02:30&lt;00:00, 15.0s/file]&#10;```&#10;- Hiển thị số file đã load&#10;- Thời gian còn lại (ETA)&#10;- Tốc độ load (files/second)&#10;&#10;### 2. **Processing inf values**&#10;```&#10; Processing inf values: 100%|████████| 68/68 [00:15&lt;00:00, 4.5col/s]&#10;```&#10;- Số cột đã xử lý&#10;- Thời gian còn lại&#10;&#10;### 3. **Normalizing features**&#10;```&#10; Normalizing: 100%|████████| 2/2 [00:30&lt;00:00, 15.0s/step]&#10;```&#10;- Tiến độ normalization&#10;&#10;### 4. **Saving files**&#10;```&#10; Saving files: 100%|████████| 8/8 [01:00&lt;00:00, 7.5s/file]&#10;```&#10;- Số file đã lưu&#10;- File đang l��u&#10;&#10;---&#10;&#10;##  Minh họa output&#10;&#10;### Với tqdm (CÓ progress bar):&#10;&#10;```&#10;================================================================================&#10;LOADING CICIDS2018 DATA&#10;================================================================================&#10;Tìm thấy 10 file CSV&#10; Loading CSV files:  50%|██████▌       | 5/10 [01:15&lt;01:15, 15.0s/file]&#10;```&#10;&#10;### Không có tqdm (KHÔNG progress bar):&#10;&#10;```&#10;================================================================================&#10;LOADING CICIDS2018 DATA&#10;================================================================================&#10;Tìm thấy 10 file CSV&#10;⚠️  tqdm not installed. Install for progress bars: pip install tqdm&#10;  Đang đọc: Friday-02-03-2018_TrafficForML_CICFlowMeter.csv...&#10;    ✓ 445,909 rows loaded&#10;  Đang đọc: Friday-16-02-2018_TrafficForML_CICFlowMeter.csv...&#10;    ✓ 663,808 rows loaded&#10;  ...&#10;```&#10;&#10;**→ Vẫn chạy được nhưng không có progress bar!**&#10;&#10;---&#10;&#10;##  Lợi ích của Progress Bar&#10;&#10;### 1. **Biết được tiến độ**&#10;- Bao nhiêu % đã hoàn thành&#10;- Còn bao lâu nữa (ETA - Estimated Time of Arrival)&#10;&#10;### 2. **Không lo &quot;script bị treo&quot;**&#10;- Thấy thanh chạy → biết script vẫn hoạt động&#10;- Thấy file/cột đang xử lý&#10;&#10;### 3. **Tối ưu workflow**&#10;- Biết step nào chậm nhất&#10;- Có thể đi làm việc khác khi còn lâu&#10;&#10;---&#10;&#10;##  So sánh&#10;&#10;| Aspect | Không có tqdm | Có tqdm |&#10;|--------|---------------|---------|&#10;| **Progress** | ❌ Không biết % | ✅ Biết rõ % |&#10;| **ETA** | ❌ Không biết còn bao lâu | ✅ Hiển thị thời gian còn lại |&#10;| **Tốc độ** | ❌ Không biết | ✅ Hiển thị items/s |&#10;| **Cài đặt** | ✅ Không cần | ⚠️ Cần: `pip install tqdm` |&#10;| **Hoạt động** | ✅ Vẫn chạy bình thường | ✅ Chạy + có progress bar |&#10;&#10;---&#10;&#10;##  Chi tiết kỹ thuật&#10;&#10;### Auto-detect tqdm:&#10;&#10;```python&#10;try:&#10;    from tqdm import tqdm&#10;    TQDM_AVAILABLE = True&#10;except ImportError:&#10;    TQDM_AVAILABLE = False&#10;    print(&quot;⚠️  tqdm not installed. Install for progress bars: pip install tqdm&quot;)&#10;    tqdm = lambda x, **kwargs: x  # Fallback&#10;```&#10;&#10;**Cách hoạt động:**&#10;- Nếu **CÓ tqdm**: Dùng progress bar&#10;- Nếu **KHÔNG có**: Fallback về in thông thường&#10;&#10;**→ Script luôn chạy được, dù có hay không có tqdm!**&#10;&#10;---&#10;&#10;## ✅ Checklist&#10;&#10;### Để có progress bar đẹp:&#10;- [ ] Cài tqdm: `pip install tqdm`&#10;- [ ] Chạy script: `python preprocess_data.py`&#10;- [ ] Thấy các thanh tiến trình:    &#10;&#10;### Nếu không muốn cài tqdm:&#10;- [ ] Bỏ qua, chạy luôn: `python preprocess_data.py`&#10;- [ ] Thấy text thông thường (không có progress bar)&#10;- [ ] Vẫn hoạt động bình thường!&#10;&#10;---&#10;&#10;##  Demo output (với tqdm)&#10;&#10;```&#10;╔══════════════════════════════════════════════════════════════════════════════╗&#10;║               CICIDS2018 DATA PREPROCESSING PIPELINE                         ║&#10;╚══════════════════════════════════════════════════════════════════════════════╝&#10;&#10;&#10;================================================================================&#10;LOADING CICIDS2018 DATA&#10;================================================================================&#10;Tìm thấy 10 file CSV&#10; Loading CSV files:  30%|███▊           | 3/10 [00:45&lt;01:45, 15.0s/file] {rows: 1,109,717, filtered: 6}&#10;&#10;================================================================================&#10;CLEANING DATA&#10;================================================================================&#10;Dropping inconsistent/unnecessary columns: ['Timestamp']&#10;✓ Remaining columns: 79&#10;✓ Features kept: 78&#10;...&#10; Processing inf values: 100%|████████████| 68/68 [00:15&lt;00:00, 4.5col/s]&#10;&#10;================================================================================&#10;NORMALIZING FEATURES&#10;================================================================================&#10;Normalizing 68 features...&#10; Normalizing: 100%|████████████████████| 2/2 [00:30&lt;00:00, 15.0s/step] {status: completed}&#10;✓ Features normalized&#10;&#10;================================================================================&#10;SAVING PROCESSED DATA&#10;================================================================================&#10; Saving files: 100%|████████████████████| 8/8 [01:00&lt;00:00, 7.5s/file] {file: metadata.pkl}&#10;&#10;✓ All files saved successfully!&#10;  - Feature array: (16232943, 68)&#10;  - Binary labels: (16232943,)&#10;  - Multi-class labels: (16232943,)&#10;&#10;================================================================================&#10;PREPROCESSING COMPLETED!&#10;================================================================================&#10;```&#10;&#10;**→ Rất dễ theo dõi tiến độ!** &#10;&#10;---&#10;&#10;##  Troubleshooting&#10;&#10;### Q: Cài tqdm nhưng vẫn không thấy progress bar?&#10;A: Restart terminal hoặc Python environment&#10;&#10;### Q: Progress bar bị nhấp nháy/lỗi?&#10;A: Đang chạy trong IDE có thể không hỗ trợ tốt. Chạy trong terminal thuần.&#10;&#10;### Q: Có bắt buộc phải cài tqdm không?&#10;A: **KHÔNG!** Script vẫn chạy bình thường, chỉ không có progress bar.&#10;&#10;---&#10;&#10;**Happy preprocessing với progress bar! **&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/extract_labels_no_pandas.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/extract_labels_no_pandas.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Script đơn giản để trích xuất các nhãn (Label) từ tệp CSV&#10;Chỉ sử dụng thư viện chuẩn của Python (không cần pandas)&#10;&quot;&quot;&quot;&#10;&#10;import csv&#10;from collections import Counter&#10;import os&#10;&#10;def extract_labels_no_pandas(csv_file_path):&#10;    &quot;&quot;&quot;&#10;    Trích xuất các nhãn từ một tệp CSV cụ thể&#10;    Không sử dụng pandas - chỉ dùng thư viện chuẩn&#10;    &#10;    Args:&#10;        csv_file_path: Đường dẫn đến tệp CSV&#10;    &quot;&quot;&quot;&#10;    print(f&quot;Đang đọc tệp: {csv_file_path}&quot;)&#10;    &#10;    labels = []&#10;    &#10;    # Đọc tệp CSV&#10;    with open(csv_file_path, 'r', encoding='utf-8') as file:&#10;        csv_reader = csv.DictReader(file)&#10;        &#10;        # Kiểm tra xem có cột 'Label' không&#10;        if 'Label' not in csv_reader.fieldnames:&#10;            print(&quot;Lỗi: Không tìm thấy cột 'Label' trong tệp CSV&quot;)&#10;            print(f&quot;Các cột có sẵn: {csv_reader.fieldnames}&quot;)&#10;            return None, None&#10;        &#10;        # Đọc tất cả các nhãn&#10;        for row in csv_reader:&#10;            labels.append(row['Label'])&#10;    &#10;    # Lấy các nhãn duy nhất&#10;    unique_labels = sorted(set(labels))&#10;    &#10;    # Đếm số lượng mỗi nhãn&#10;    label_counts = Counter(labels)&#10;    &#10;    # Hiển thị kết quả&#10;    print(&quot;\n&quot; + &quot;=&quot; * 60)&#10;    print(&quot;CÁC NHÃN TÌM THẤY&quot;)&#10;    print(&quot;=&quot; * 60)&#10;    print(f&quot;\nTổng số nhãn duy nhất: {len(unique_labels)}&quot;)&#10;    print(f&quot;\nDanh sách các nhãn:&quot;)&#10;    for i, label in enumerate(unique_labels, 1):&#10;        print(f&quot;  {i}. {label}&quot;)&#10;    &#10;    print(&quot;\n&quot; + &quot;=&quot; * 60)&#10;    print(&quot;PHÂN PHỐI SỐ LƯỢNG MỖI NHÃN&quot;)&#10;    print(&quot;=&quot; * 60)&#10;    total = len(labels)&#10;    for label, count in label_counts.most_common():&#10;        percentage = (count / total) * 100&#10;        print(f&quot;{label}: {count:,} ({percentage:.2f}%)&quot;)&#10;    &#10;    print(f&quot;\nTổng số dòng dữ liệu: {total:,}&quot;)&#10;    &#10;    return unique_labels, label_counts&#10;&#10;def extract_labels_from_all_files(directory_path):&#10;    &quot;&quot;&quot;&#10;    Trích xuất nhãn từ tất cả các tệp CSV trong thư mục&#10;    &#10;    Args:&#10;        directory_path: Đường dẫn đến thư mục chứa các tệp CSV&#10;    &quot;&quot;&quot;&#10;    all_labels = set()&#10;    &#10;    print(&quot;\n&quot; + &quot;=&quot; * 80)&#10;    print(&quot;TRÍCH XUẤT NHÃN TỪ TẤT CẢ CÁC TẾP CSV&quot;)&#10;    print(&quot;=&quot; * 80)&#10;    &#10;    # Lấy tất cả các tệp CSV&#10;    csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]&#10;    &#10;    print(f&quot;\nTìm thấy {len(csv_files)} tệp CSV\n&quot;)&#10;    &#10;    for csv_file in sorted(csv_files):&#10;        file_path = os.path.join(directory_path, csv_file)&#10;        print(f&quot;\n{'─' * 80}&quot;)&#10;        print(f&quot;Đang xử lý: {csv_file}&quot;)&#10;        print('─' * 80)&#10;        &#10;        try:&#10;            with open(file_path, 'r', encoding='utf-8') as file:&#10;                csv_reader = csv.DictReader(file)&#10;                &#10;                if 'Label' in csv_reader.fieldnames:&#10;                    file_labels = set()&#10;                    for row in csv_reader:&#10;                        file_labels.add(row['Label'])&#10;                    &#10;                    all_labels.update(file_labels)&#10;                    print(f&quot;Các nhãn tìm thấy ({len(file_labels)}): {sorted(file_labels)}&quot;)&#10;                else:&#10;                    print(&quot;Cảnh báo: Không tìm thấy cột 'Label'&quot;)&#10;        &#10;        except Exception as e:&#10;            print(f&quot;Lỗi khi đọc tệp: {e}&quot;)&#10;    &#10;    # Hiển thị tổng kết&#10;    print(&quot;\n&quot; + &quot;=&quot; * 80)&#10;    print(&quot;KẾT QUẢ TỔNG HỢP&quot;)&#10;    print(&quot;=&quot; * 80)&#10;    print(f&quot;\nTổng số nhãn duy nhất tìm thấy: {len(all_labels)}&quot;)&#10;    print(f&quot;\nDanh sách các nhãn:&quot;)&#10;    for i, label in enumerate(sorted(all_labels), 1):&#10;        print(f&quot;  {i}. {label}&quot;)&#10;    &#10;    return all_labels&#10;&#10;# Chương trình chính&#10;if __name__ == &quot;__main__&quot;:&#10;    # Lựa chọn: phân tích một tệp hoặc tất cả các tệp&#10;    print(&quot;Chọn chế độ:&quot;)&#10;    print(&quot;1. Trích xuất nhãn từ một tệp cụ thể&quot;)&#10;    print(&quot;2. Trích xuất nhãn từ tất cả các tệp CSV trong thư mục&quot;)&#10;    &#10;    choice = input(&quot;\nNhập lựa chọn của bạn (1 hoặc 2): &quot;).strip()&#10;    &#10;    if choice == &quot;1&quot;:&#10;        # Phân tích một tệp cụ thể&#10;        csv_file = r&quot;D:\PROJECT\Machine Learning\IOT\CICIDS2018-CSV\Friday-02-03-2018_TrafficForML_CICFlowMeter.csv&quot;&#10;        &#10;        # Hoặc để người dùng nhập đường dẫn&#10;        custom_path = input(f&quot;\nNhập đường dẫn tệp CSV (Enter để dùng mặc định):\n{csv_file}\n&gt; &quot;).strip()&#10;        if custom_path:&#10;            csv_file = custom_path&#10;        &#10;        try:&#10;            labels, counts = extract_labels_no_pandas(csv_file)&#10;        except FileNotFoundError:&#10;            print(f&quot;\nLỗi: Không tìm thấy tệp {csv_file}&quot;)&#10;        except Exception as e:&#10;            print(f&quot;\nLỗi: {e}&quot;)&#10;    &#10;    elif choice == &quot;2&quot;:&#10;        # Phân tích tất cả các tệp&#10;        directory = r&quot;D:\PROJECT\Machine Learning\IOT\CICIDS2018-CSV&quot;&#10;        &#10;        custom_dir = input(f&quot;\nNhập đường dẫn thư mục (Enter để dùng mặc định):\n{directory}\n&gt; &quot;).strip()&#10;        if custom_dir:&#10;            directory = custom_dir&#10;        &#10;        try:&#10;            all_labels = extract_labels_from_all_files(directory)&#10;            &#10;            # Lưu kết quả ra tệp&#10;            output_file = &quot;labels_summary.txt&quot;&#10;            with open(output_file, 'w', encoding='utf-8') as f:&#10;                f.write(&quot;TỔNG HỢP CÁC NHÃN TỪ CICIDS2018 DATASET\n&quot;)&#10;                f.write(&quot;=&quot; * 80 + &quot;\n\n&quot;)&#10;                f.write(f&quot;Tổng số nhãn duy nhất: {len(all_labels)}\n\n&quot;)&#10;                f.write(&quot;Danh sách các nhãn:\n&quot;)&#10;                for label in sorted(all_labels):&#10;                    f.write(f&quot;  - {label}\n&quot;)&#10;            &#10;            print(f&quot;\nKết quả đã được lưu vào: {output_file}&quot;)&#10;        &#10;        except Exception as e:&#10;            print(f&quot;\nLỗi: {e}&quot;)&#10;    &#10;    else:&#10;        print(&quot;\nLựa chọn không hợp lệ!&quot;)&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>
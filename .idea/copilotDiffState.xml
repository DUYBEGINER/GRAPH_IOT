<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/CNN/prepare_folds.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/CNN/prepare_folds.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;=============================================================================&#10;CHIA DỮ LIỆU THEO K-FOLD CHO MÔ HÌNH CNN&#10;Phát hiện lưu lượng mạng IoT bất thường (Binary Classification)&#10;=============================================================================&#10;Mô tả:&#10;    - Đọc dữ liệu đã được tiền xử lý&#10;    - Chia dữ liệu theo K-Fold Cross Validation&#10;    - Hỗ trợ Stratified K-Fold để giữ tỷ lệ class&#10;    - Lưu các fold để sử dụng khi training&#10;=============================================================================&#10;&quot;&quot;&quot;&#10;&#10;import os&#10;import numpy as np&#10;import pickle&#10;from sklearn.model_selection import StratifiedKFold, train_test_split&#10;from tqdm import tqdm&#10;import warnings&#10;&#10;warnings.filterwarnings('ignore')&#10;&#10;&#10;# =============================================================================&#10;# CẤU HÌNH ĐƯỜNG DẪN&#10;# =============================================================================&#10;&#10;def get_paths():&#10;    &quot;&quot;&quot;&#10;    Tự động xác định đường dẫn dựa trên môi trường (Kaggle/Local)&#10;    &quot;&quot;&quot;&#10;    # Kiểm tra nếu đang chạy trên Kaggle&#10;    if os.path.exists('/kaggle/input'):&#10;        INPUT_DIR = '/kaggle/working/processed_cnn'&#10;        OUTPUT_DIR = '/kaggle/working/folds'&#10;        print(&quot; Đang chạy trên KAGGLE&quot;)&#10;    else:&#10;        # Đường dẫn Local&#10;        BASE_DIR = os.path.dirname(os.path.abspath(__file__))&#10;        INPUT_DIR = os.path.join(BASE_DIR, 'processed_data')&#10;        OUTPUT_DIR = os.path.join(BASE_DIR, 'folds')&#10;        print(&quot; Đang chạy trên LOCAL&quot;)&#10;    &#10;    # Tạo thư mục output nếu chưa tồn tại&#10;    os.makedirs(OUTPUT_DIR, exist_ok=True)&#10;    &#10;    print(f&quot; Thư mục input: {INPUT_DIR}&quot;)&#10;    print(f&quot; Thư mục output: {OUTPUT_DIR}&quot;)&#10;    &#10;    return INPUT_DIR, OUTPUT_DIR&#10;&#10;&#10;# =============================================================================&#10;# HÀM CHIA DỮ LIỆU&#10;# =============================================================================&#10;&#10;def load_processed_data(input_dir):&#10;    &quot;&quot;&quot;&#10;    Đọc dữ liệu đã được tiền xử lý&#10;    &quot;&quot;&quot;&#10;    print(&quot;\n Đang đọc dữ liệu đã xử lý...&quot;)&#10;    &#10;    # Đọc features và labels&#10;    X = np.load(os.path.join(input_dir, 'X_processed.npy'))&#10;    y = np.load(os.path.join(input_dir, 'y_binary.npy'))&#10;    &#10;    # Đọc metadata&#10;    with open(os.path.join(input_dir, 'metadata.pkl'), 'rb') as f:&#10;        metadata = pickle.load(f)&#10;    &#10;    print(f&quot;✅ Đã đọc dữ liệu:&quot;)&#10;    print(f&quot;   - X shape: {X.shape}&quot;)&#10;    print(f&quot;   - y shape: {y.shape}&quot;)&#10;    print(f&quot;   - Số mẫu Benign (0): {np.sum(y == 0):,}&quot;)&#10;    print(f&quot;   - Số mẫu Attack (1): {np.sum(y == 1):,}&quot;)&#10;    &#10;    return X, y, metadata&#10;&#10;&#10;def reshape_for_cnn(X, method='1d'):&#10;    &quot;&quot;&quot;&#10;    Reshape dữ liệu cho CNN&#10;    &#10;    Parameters:&#10;    -----------&#10;    X : numpy array&#10;        Dữ liệu features (n_samples, n_features)&#10;    method : str&#10;        '1d' - Reshape thành (samples, features, 1) cho Conv1D&#10;        '2d' - Reshape thành hình vuông cho Conv2D&#10;    &#10;    Returns:&#10;    --------&#10;    X_reshaped : numpy array&#10;        Dữ liệu đã reshape&#10;    &quot;&quot;&quot;&#10;    n_samples, n_features = X.shape&#10;    &#10;    if method == '1d':&#10;        # Reshape cho Conv1D: (samples, features, 1)&#10;        X_reshaped = X.reshape(n_samples, n_features, 1)&#10;        &#10;    elif method == '2d':&#10;        # Tìm kích thước hình vuông gần nhất&#10;        sqrt_features = int(np.ceil(np.sqrt(n_features)))&#10;        padded_features = sqrt_features ** 2&#10;        &#10;        # Padding nếu cần&#10;        if padded_features &gt; n_features:&#10;            padding = np.zeros((n_samples, padded_features - n_features))&#10;            X_padded = np.hstack([X, padding])&#10;        else:&#10;            X_padded = X&#10;        &#10;        # Reshape cho Conv2D: (samples, height, width, 1)&#10;        X_reshaped = X_padded.reshape(n_samples, sqrt_features, sqrt_features, 1)&#10;    &#10;    else:&#10;        X_reshaped = X&#10;    &#10;    return X_reshaped&#10;&#10;&#10;def create_stratified_kfold(X, y, n_folds=5, shuffle=True, random_state=42):&#10;    &quot;&quot;&quot;&#10;    Tạo Stratified K-Fold để chia dữ liệu&#10;    Stratified giữ nguyên tỷ lệ class trong mỗi fold&#10;    &#10;    Parameters:&#10;    -----------&#10;    X : numpy array&#10;        Features&#10;    y : numpy array&#10;        Labels&#10;    n_folds : int&#10;        Số fold (mặc định 5)&#10;    shuffle : bool&#10;        Có shuffle dữ liệu không&#10;    random_state : int&#10;        Random seed&#10;    &#10;    Returns:&#10;    --------&#10;    folds : list of tuples&#10;        Mỗi tuple chứa (train_indices, val_indices)&#10;    &quot;&quot;&quot;&#10;    &#10;    print(f&quot;\n Đang tạo {n_folds}-Fold Stratified Cross Validation...&quot;)&#10;    &#10;    skf = StratifiedKFold(n_splits=n_folds, shuffle=shuffle, random_state=random_state)&#10;    &#10;    folds = []&#10;    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y)):&#10;        folds.append((train_idx, val_idx))&#10;        &#10;        # Thống kê mỗi fold&#10;        y_train_fold = y[train_idx]&#10;        y_val_fold = y[val_idx]&#10;        &#10;        print(f&quot;\n Fold {fold_idx + 1}:&quot;)&#10;        print(f&quot;   Train: {len(train_idx):,} mẫu | &quot;&#10;              f&quot;Benign: {np.sum(y_train_fold == 0):,} | &quot;&#10;              f&quot;Attack: {np.sum(y_train_fold == 1):,} | &quot;&#10;              f&quot;Ratio: {np.mean(y_train_fold)*100:.2f}%&quot;)&#10;        print(f&quot;   Val:   {len(val_idx):,} mẫu | &quot;&#10;              f&quot;Benign: {np.sum(y_val_fold == 0):,} | &quot;&#10;              f&quot;Attack: {np.sum(y_val_fold == 1):,} | &quot;&#10;              f&quot;Ratio: {np.mean(y_val_fold)*100:.2f}%&quot;)&#10;    &#10;    return folds&#10;&#10;&#10;def create_train_val_test_split(X, y, val_size=0.15, test_size=0.15, random_state=42):&#10;    &quot;&quot;&quot;&#10;    Chia dữ liệu thành Train/Validation/Test&#10;    Sử dụng Stratified split để giữ tỷ lệ class&#10;    &#10;    Parameters:&#10;    -----------&#10;    X : numpy array&#10;        Features&#10;    y : numpy array&#10;        Labels&#10;    val_size : float&#10;        Tỷ lệ validation set&#10;    test_size : float&#10;        Tỷ lệ test set&#10;    random_state : int&#10;        Random seed&#10;    &#10;    Returns:&#10;    --------&#10;    splits : dict&#10;        Dictionary chứa các split&#10;    &quot;&quot;&quot;&#10;    &#10;    print(f&quot;\n Đang chia dữ liệu Train/Val/Test...&quot;)&#10;    print(f&quot;   Tỷ lệ: Train={1-val_size-test_size:.0%} | Val={val_size:.0%} | Test={test_size:.0%}&quot;)&#10;    &#10;    # Chia train+val và test&#10;    X_trainval, X_test, y_trainval, y_test = train_test_split(&#10;        X, y, test_size=test_size, stratify=y, random_state=random_state&#10;    )&#10;    &#10;    # Chia train và val&#10;    val_ratio = val_size / (1 - test_size)&#10;    X_train, X_val, y_train, y_val = train_test_split(&#10;        X_trainval, y_trainval, test_size=val_ratio, stratify=y_trainval, &#10;        random_state=random_state&#10;    )&#10;    &#10;    splits = {&#10;        'X_train': X_train,&#10;        'y_train': y_train,&#10;        'X_val': X_val,&#10;        'y_val': y_val,&#10;        'X_test': X_test,&#10;        'y_test': y_test&#10;    }&#10;    &#10;    # Thống kê&#10;    print(f&quot;\n Kết quả chia dữ liệu:&quot;)&#10;    print(f&quot;   Train: {len(X_train):,} mẫu | &quot;&#10;          f&quot;Benign: {np.sum(y_train == 0):,} | &quot;&#10;          f&quot;Attack: {np.sum(y_train == 1):,}&quot;)&#10;    print(f&quot;   Val:   {len(X_val):,} mẫu | &quot;&#10;          f&quot;Benign: {np.sum(y_val == 0):,} | &quot;&#10;          f&quot;Attack: {np.sum(y_val == 1):,}&quot;)&#10;    print(f&quot;   Test:  {len(X_test):,} mẫu | &quot;&#10;          f&quot;Benign: {np.sum(y_test == 0):,} | &quot;&#10;          f&quot;Attack: {np.sum(y_test == 1):,}&quot;)&#10;    &#10;    return splits&#10;&#10;&#10;def save_folds(folds, X, y, output_dir, reshape_method='1d'):&#10;    &quot;&quot;&quot;&#10;    Lưu các fold đã chia&#10;    &#10;    Parameters:&#10;    -----------&#10;    folds : list of tuples&#10;        Danh sách các fold (train_idx, val_idx)&#10;    X : numpy array&#10;        Features&#10;    y : numpy array&#10;        Labels&#10;    output_dir : str&#10;        Thư mục lưu&#10;    reshape_method : str&#10;        Phương pháp reshape ('1d' hoặc '2d')&#10;    &quot;&quot;&quot;&#10;    &#10;    print(f&quot;\n Đang lưu {len(folds)} folds...&quot;)&#10;    &#10;    # Reshape dữ liệu cho CNN&#10;    X_reshaped = reshape_for_cnn(X, method=reshape_method)&#10;    print(f&quot; Shape sau reshape: {X_reshaped.shape}&quot;)&#10;    &#10;    for fold_idx, (train_idx, val_idx) in enumerate(tqdm(folds, desc=&quot;Lưu folds&quot;)):&#10;        fold_dir = os.path.join(output_dir, f'fold_{fold_idx + 1}')&#10;        os.makedirs(fold_dir, exist_ok=True)&#10;        &#10;        # Lưu train data&#10;        np.save(os.path.join(fold_dir, 'X_train.npy'), X_reshaped[train_idx])&#10;        np.save(os.path.join(fold_dir, 'y_train.npy'), y[train_idx])&#10;        &#10;        # Lưu validation data&#10;        np.save(os.path.join(fold_dir, 'X_val.npy'), X_reshaped[val_idx])&#10;        np.save(os.path.join(fold_dir, 'y_val.npy'), y[val_idx])&#10;    &#10;    # Lưu thông tin về folds&#10;    fold_info = {&#10;        'n_folds': len(folds),&#10;        'n_samples': len(y),&#10;        'n_features': X.shape[1],&#10;        'input_shape': X_reshaped.shape[1:],&#10;        'reshape_method': reshape_method,&#10;        'folds': [(list(train_idx), list(val_idx)) for train_idx, val_idx in folds]&#10;    }&#10;    &#10;    with open(os.path.join(output_dir, 'fold_info.pkl'), 'wb') as f:&#10;        pickle.dump(fold_info, f)&#10;    &#10;    print(f&quot;\n✅ Đã lưu folds vào: {output_dir}&quot;)&#10;&#10;&#10;def save_train_val_test(splits, output_dir, reshape_method='1d'):&#10;    &quot;&quot;&quot;&#10;    Lưu Train/Val/Test splits&#10;    &#10;    Parameters:&#10;    -----------&#10;    splits : dict&#10;        Dictionary chứa các split&#10;    output_dir : str&#10;        Thư mục lưu&#10;    reshape_method : str&#10;        Phương pháp reshape&#10;    &quot;&quot;&quot;&#10;    &#10;    print(f&quot;\n Đang lưu Train/Val/Test splits...&quot;)&#10;    &#10;    split_dir = os.path.join(output_dir, 'train_val_test')&#10;    os.makedirs(split_dir, exist_ok=True)&#10;    &#10;    for key, data in splits.items():&#10;        if key.startswith('X_'):&#10;            # Reshape cho CNN&#10;            data_reshaped = reshape_for_cnn(data, method=reshape_method)&#10;            np.save(os.path.join(split_dir, f'{key}.npy'), data_reshaped)&#10;            print(f&quot;   Saved {key}: {data_reshaped.shape}&quot;)&#10;        else:&#10;            np.save(os.path.join(split_dir, f'{key}.npy'), data)&#10;            print(f&quot;   Saved {key}: {data.shape}&quot;)&#10;    &#10;    # Lưu thông tin về splits&#10;    split_info = {&#10;        'n_train': len(splits['X_train']),&#10;        'n_val': len(splits['X_val']),&#10;        'n_test': len(splits['X_test']),&#10;        'n_features': splits['X_train'].shape[1],&#10;        'input_shape': reshape_for_cnn(splits['X_train'], method=reshape_method).shape[1:],&#10;        'reshape_method': reshape_method&#10;    }&#10;    &#10;    with open(os.path.join(split_dir, 'split_info.pkl'), 'wb') as f:&#10;        pickle.dump(split_info, f)&#10;    &#10;    print(f&quot;\n✅ Đã lưu splits vào: {split_dir}&quot;)&#10;&#10;&#10;# =============================================================================&#10;# HÀM CHÍNH&#10;# =============================================================================&#10;&#10;def prepare_folds(n_folds=5, reshape_method='1d', create_holdout=True, &#10;                  val_size=0.15, test_size=0.15):&#10;    &quot;&quot;&quot;&#10;    Hàm chính để chuẩn bị folds cho training&#10;    &#10;    Parameters:&#10;    -----------&#10;    n_folds : int&#10;        Số fold cho K-Fold CV&#10;    reshape_method : str&#10;        '1d' cho Conv1D, '2d' cho Conv2D&#10;    create_holdout : bool&#10;        Có tạo thêm Train/Val/Test split không&#10;    val_size : float&#10;        Tỷ lệ validation (cho holdout)&#10;    test_size : float&#10;        Tỷ lệ test (cho holdout)&#10;    &quot;&quot;&quot;&#10;    &#10;    print(&quot;=&quot;*70)&#10;    print(&quot; BẮT ĐẦU CHIA FOLDS CHO TRAINING&quot;)&#10;    print(&quot;=&quot;*70)&#10;    &#10;    # Lấy đường dẫn&#10;    INPUT_DIR, OUTPUT_DIR = get_paths()&#10;    &#10;    # Đọc dữ liệu&#10;    X, y, metadata = load_processed_data(INPUT_DIR)&#10;    &#10;    # Tạo K-Fold&#10;    folds = create_stratified_kfold(X, y, n_folds=n_folds)&#10;    &#10;    # Lưu folds&#10;    save_folds(folds, X, y, OUTPUT_DIR, reshape_method=reshape_method)&#10;    &#10;    # Tạo thêm Train/Val/Test split (không dùng K-Fold)&#10;    if create_holdout:&#10;        print(&quot;\n&quot; + &quot;=&quot;*70)&#10;        print(&quot; TẠO THÊM TRAIN/VAL/TEST SPLIT&quot;)&#10;        print(&quot;=&quot;*70)&#10;        &#10;        splits = create_train_val_test_split(X, y, val_size=val_size, test_size=test_size)&#10;        save_train_val_test(splits, OUTPUT_DIR, reshape_method=reshape_method)&#10;    &#10;    print(&quot;\n&quot; + &quot;=&quot;*70)&#10;    print(&quot; HOÀN THÀNH CHIA FOLDS!&quot;)&#10;    print(&quot;=&quot;*70)&#10;    &#10;    # Tóm tắt&#10;    print(f&quot;\n TÓM TẮT:&quot;)&#10;    print(f&quot;   - Số folds: {n_folds}&quot;)&#10;    print(f&quot;   - Reshape method: {reshape_method}&quot;)&#10;    print(f&quot;   - Input shape cho CNN: {reshape_for_cnn(X, method=reshape_method).shape[1:]}&quot;)&#10;    print(f&quot;   - Thư mục output: {OUTPUT_DIR}&quot;)&#10;    &#10;    return folds&#10;&#10;&#10;# =============================================================================&#10;# HÀM TIỆN ÍCH - LOAD FOLD&#10;# =============================================================================&#10;&#10;def load_fold(fold_idx, folds_dir=None):&#10;    &quot;&quot;&quot;&#10;    Load dữ liệu của một fold cụ thể&#10;    &#10;    Parameters:&#10;    -----------&#10;    fold_idx : int&#10;        Index của fold (1-based)&#10;    folds_dir : str&#10;        Thư mục chứa folds (None để tự động detect)&#10;    &#10;    Returns:&#10;    --------&#10;    X_train, y_train, X_val, y_val&#10;    &quot;&quot;&quot;&#10;    &#10;    if folds_dir is None:&#10;        _, folds_dir = get_paths()&#10;    &#10;    fold_dir = os.path.join(folds_dir, f'fold_{fold_idx}')&#10;    &#10;    X_train = np.load(os.path.join(fold_dir, 'X_train.npy'))&#10;    y_train = np.load(os.path.join(fold_dir, 'y_train.npy'))&#10;    X_val = np.load(os.path.join(fold_dir, 'X_val.npy'))&#10;    y_val = np.load(os.path.join(fold_dir, 'y_val.npy'))&#10;    &#10;    print(f&quot; Đã load Fold {fold_idx}:&quot;)&#10;    print(f&quot;   X_train: {X_train.shape}, y_train: {y_train.shape}&quot;)&#10;    print(f&quot;   X_val: {X_val.shape}, y_val: {y_val.shape}&quot;)&#10;    &#10;    return X_train, y_train, X_val, y_val&#10;&#10;&#10;def load_train_val_test(folds_dir=None):&#10;    &quot;&quot;&quot;&#10;    Load Train/Val/Test split&#10;    &#10;    Parameters:&#10;    -----------&#10;    folds_dir : str&#10;        Thư mục chứa folds (None để tự động detect)&#10;    &#10;    Returns:&#10;    --------&#10;    X_train, y_train, X_val, y_val, X_test, y_test&#10;    &quot;&quot;&quot;&#10;    &#10;    if folds_dir is None:&#10;        _, folds_dir = get_paths()&#10;    &#10;    split_dir = os.path.join(folds_dir, 'train_val_test')&#10;    &#10;    X_train = np.load(os.path.join(split_dir, 'X_train.npy'))&#10;    y_train = np.load(os.path.join(split_dir, 'y_train.npy'))&#10;    X_val = np.load(os.path.join(split_dir, 'X_val.npy'))&#10;    y_val = np.load(os.path.join(split_dir, 'y_val.npy'))&#10;    X_test = np.load(os.path.join(split_dir, 'X_test.npy'))&#10;    y_test = np.load(os.path.join(split_dir, 'y_test.npy'))&#10;    &#10;    print(f&quot; Đã load Train/Val/Test split:&quot;)&#10;    print(f&quot;   X_train: {X_train.shape}, y_train: {y_train.shape}&quot;)&#10;    print(f&quot;   X_val: {X_val.shape}, y_val: {y_val.shape}&quot;)&#10;    print(f&quot;   X_test: {X_test.shape}, y_test: {y_test.shape}&quot;)&#10;    &#10;    return X_train, y_train, X_val, y_val, X_test, y_test&#10;&#10;&#10;# =============================================================================&#10;# CHẠY CHƯƠNG TRÌNH&#10;# =============================================================================&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    # Chuẩn bị folds&#10;    folds = prepare_folds(&#10;        n_folds=5,              # Số fold&#10;        reshape_method='1d',    # '1d' cho Conv1D, '2d' cho Conv2D&#10;        create_holdout=True,    # Tạo thêm Train/Val/Test split&#10;        val_size=0.15,          # Tỷ lệ validation&#10;        test_size=0.15          # Tỷ lệ test&#10;    )&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/CNN/preprocess_cicids2018.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/CNN/preprocess_cicids2018.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;=============================================================================&#10;TIỀN XỬ LÝ DATASET CICIDS2018 CHO MÔ HÌNH CNN&#10;Phát hiện lưu lượng mạng IoT bất thường (Binary Classification)&#10;=============================================================================&#10;Tác giả: Auto-generated&#10;Mô tả: &#10;    - Đọc và xử lý từng file CSV trong dataset CICIDS2018&#10;    - Chuyển đổi về binary class (Benign vs Attack)&#10;    - Loại bỏ duplicate, xử lý NaN, Inf&#10;    - Chuẩn hóa dữ liệu và lưu dưới dạng tối ưu (numpy/pickle)&#10;    - Sử dụng chunk để tối ưu bộ nhớ&#10;=============================================================================&#10;&quot;&quot;&quot;&#10;&#10;import os&#10;import numpy as np&#10;import pandas as pd&#10;import pickle&#10;from sklearn.preprocessing import StandardScaler, RobustScaler&#10;from tqdm import tqdm&#10;import warnings&#10;import gc&#10;&#10;warnings.filterwarnings('ignore')&#10;&#10;# =============================================================================&#10;# CẤU HÌNH ĐƯỜNG DẪN - Tự động detect Kaggle hoặc Local&#10;# =============================================================================&#10;def get_paths():&#10;    &quot;&quot;&quot;&#10;    Tự động xác định đường dẫn dựa trên môi trường (Kaggle/Local)&#10;    &quot;&quot;&quot;&#10;    # Kiểm tra nếu đang chạy trên Kaggle&#10;    if os.path.exists('/kaggle/input'):&#10;        # Đường dẫn Kaggle - người dùng cần upload dataset&#10;        DATA_DIR = '/kaggle/input/cicids2018-csv'  # Thay đổi theo tên dataset trên Kaggle&#10;        OUTPUT_DIR = '/kaggle/working/processed_cnn'&#10;        print(&quot; Đang chạy trên KAGGLE&quot;)&#10;    else:&#10;        # Đường dẫn Local&#10;        BASE_DIR = os.path.dirname(os.path.abspath(__file__))&#10;        DATA_DIR = os.path.join(os.path.dirname(BASE_DIR), 'CICIDS2018-CSV')&#10;        OUTPUT_DIR = os.path.join(BASE_DIR, 'processed_data')&#10;        print(&quot; Đang chạy trên LOCAL&quot;)&#10;    &#10;    # Tạo thư mục output nếu chưa tồn tại&#10;    os.makedirs(OUTPUT_DIR, exist_ok=True)&#10;    &#10;    print(f&quot; Thư mục dữ liệu: {DATA_DIR}&quot;)&#10;    print(f&quot; Thư mục output: {OUTPUT_DIR}&quot;)&#10;    &#10;    return DATA_DIR, OUTPUT_DIR&#10;&#10;&#10;# =============================================================================&#10;# DANH SÁCH CÁC FEATURE CẦN LOẠI BỎ&#10;# =============================================================================&#10;# Các feature không cần thiết cho CNN:&#10;# - Timestamp: Thông tin thời gian không liên quan đến pattern traffic&#10;# - Các cột có giá trị hằng số (variance = 0)&#10;# - Các cột trùng lặp thông tin&#10;&#10;COLUMNS_TO_DROP = [&#10;    'Timestamp',           # Thời gian - không cần thiết&#10;    'Fwd Byts/b Avg',      # Thường = 0&#10;    'Fwd Pkts/b Avg',      # Thường = 0&#10;    'Fwd Blk Rate Avg',    # Thường = 0&#10;    'Bwd Byts/b Avg',      # Thường = 0&#10;    'Bwd Pkts/b Avg',      # Thường = 0&#10;    'Bwd Blk Rate Avg',    # Thường = 0&#10;]&#10;&#10;# Cột nhãn&#10;LABEL_COLUMN = 'Label'&#10;&#10;&#10;# =============================================================================&#10;# HÀM XỬ LÝ DỮ LIỆU&#10;# =============================================================================&#10;&#10;def get_csv_files(data_dir):&#10;    &quot;&quot;&quot;&#10;    Lấy danh sách tất cả file CSV trong thư mục&#10;    &quot;&quot;&quot;&#10;    csv_files = []&#10;    for file in os.listdir(data_dir):&#10;        if file.endswith('.csv'):&#10;            csv_files.append(os.path.join(data_dir, file))&#10;    &#10;    print(f&quot;\n Tìm thấy {len(csv_files)} file CSV:&quot;)&#10;    for f in csv_files:&#10;        print(f&quot;   - {os.path.basename(f)}&quot;)&#10;    &#10;    return csv_files&#10;&#10;&#10;def clean_column_names(df):&#10;    &quot;&quot;&quot;&#10;    Làm sạch tên cột: loại bỏ khoảng trắng thừa&#10;    &quot;&quot;&quot;&#10;    df.columns = df.columns.str.strip()&#10;    return df&#10;&#10;&#10;def convert_to_binary_label(label):&#10;    &quot;&quot;&quot;&#10;    Chuyển đổi nhãn multi-class sang binary class&#10;    - Benign -&gt; 0&#10;    - Tất cả các loại Attack -&gt; 1&#10;    &quot;&quot;&quot;&#10;    if isinstance(label, str):&#10;        label = label.strip()&#10;        if label.lower() == 'benign':&#10;            return 0&#10;        else:&#10;            return 1&#10;    return 1  # Mặc định là attack nếu không xác định&#10;&#10;&#10;def process_chunk(chunk, columns_to_drop):&#10;    &quot;&quot;&quot;&#10;    Xử lý từng chunk dữ liệu:&#10;    - Làm sạch tên cột&#10;    - Loại bỏ cột không cần thiết&#10;    - Chuyển đổi nhãn sang binary&#10;    - Xử lý NaN, Inf&#10;    - Loại bỏ duplicate&#10;    &quot;&quot;&quot;&#10;    # Làm sạch tên cột&#10;    chunk = clean_column_names(chunk)&#10;    &#10;    # Kiểm tra và lấy cột Label&#10;    if LABEL_COLUMN not in chunk.columns:&#10;        print(f&quot;⚠️ Không tìm thấy cột '{LABEL_COLUMN}'&quot;)&#10;        return None&#10;    &#10;    # Chuyển đổi nhãn sang binary&#10;    chunk['Label_Binary'] = chunk[LABEL_COLUMN].apply(convert_to_binary_label)&#10;    &#10;    # Loại bỏ cột Label gốc và các cột không cần thiết&#10;    cols_to_remove = [col for col in columns_to_drop + [LABEL_COLUMN] &#10;                      if col in chunk.columns]&#10;    chunk = chunk.drop(columns=cols_to_remove, errors='ignore')&#10;    &#10;    # Lấy các cột số (features)&#10;    feature_cols = chunk.select_dtypes(include=[np.number]).columns.tolist()&#10;    feature_cols = [col for col in feature_cols if col != 'Label_Binary']&#10;    &#10;    # Thay thế Inf bằng NaN, sau đó fill NaN bằng median&#10;    chunk[feature_cols] = chunk[feature_cols].replace([np.inf, -np.inf], np.nan)&#10;    &#10;    # Xử lý NaN - điền bằng 0 (hoặc median nếu cần)&#10;    chunk[feature_cols] = chunk[feature_cols].fillna(0)&#10;    &#10;    # Loại bỏ duplicate&#10;    chunk = chunk.drop_duplicates()&#10;    &#10;    return chunk&#10;&#10;&#10;def process_single_file(file_path, columns_to_drop, chunk_size=100000):&#10;    &quot;&quot;&quot;&#10;    Xử lý một file CSV với chunk&#10;    &quot;&quot;&quot;&#10;    file_name = os.path.basename(file_path)&#10;    print(f&quot;\n{'='*60}&quot;)&#10;    print(f&quot; Đang xử lý: {file_name}&quot;)&#10;    print(f&quot;{'='*60}&quot;)&#10;    &#10;    chunks_list = []&#10;    total_rows = 0&#10;    &#10;    try:&#10;        # Đọc file theo chunk&#10;        chunk_iterator = pd.read_csv(file_path, chunksize=chunk_size, &#10;                                      low_memory=False, encoding='utf-8')&#10;        &#10;        for i, chunk in enumerate(tqdm(chunk_iterator, desc=&quot;Đọc chunks&quot;)):&#10;            processed_chunk = process_chunk(chunk, columns_to_drop)&#10;            if processed_chunk is not None:&#10;                chunks_list.append(processed_chunk)&#10;                total_rows += len(processed_chunk)&#10;            &#10;            # Giải phóng bộ nhớ&#10;            del chunk&#10;            gc.collect()&#10;        &#10;        if chunks_list:&#10;            # Ghép các chunk lại&#10;            df = pd.concat(chunks_list, ignore_index=True)&#10;            &#10;            # Loại bỏ duplicate lần cuối sau khi ghép&#10;            before_dedup = len(df)&#10;            df = df.drop_duplicates()&#10;            after_dedup = len(df)&#10;            &#10;            print(f&quot;✅ Tổng số dòng sau xử lý: {after_dedup:,}&quot;)&#10;            print(f&quot;️ Số dòng duplicate đã loại bỏ: {before_dedup - after_dedup:,}&quot;)&#10;            &#10;            # Thống kê nhãn&#10;            label_counts = df['Label_Binary'].value_counts()&#10;            print(f&quot; Phân bố nhãn:&quot;)&#10;            print(f&quot;   - Benign (0): {label_counts.get(0, 0):,}&quot;)&#10;            print(f&quot;   - Attack (1): {label_counts.get(1, 0):,}&quot;)&#10;            &#10;            return df&#10;        else:&#10;            print(f&quot;❌ Không có dữ liệu hợp lệ trong file&quot;)&#10;            return None&#10;            &#10;    except Exception as e:&#10;        print(f&quot;❌ Lỗi khi xử lý file: {str(e)}&quot;)&#10;        return None&#10;&#10;&#10;def remove_constant_columns(df, feature_cols):&#10;    &quot;&quot;&quot;&#10;    Loại bỏ các cột có giá trị hằng số (variance = 0)&#10;    &quot;&quot;&quot;&#10;    constant_cols = []&#10;    for col in feature_cols:&#10;        if df[col].nunique() &lt;= 1:&#10;            constant_cols.append(col)&#10;    &#10;    if constant_cols:&#10;        print(f&quot;\n️ Loại bỏ {len(constant_cols)} cột hằng số:&quot;)&#10;        for col in constant_cols:&#10;            print(f&quot;   - {col}&quot;)&#10;        df = df.drop(columns=constant_cols)&#10;    &#10;    return df, constant_cols&#10;&#10;&#10;def remove_high_correlation_columns(df, feature_cols, threshold=0.95):&#10;    &quot;&quot;&quot;&#10;    Loại bỏ các cột có tương quan cao (&gt; threshold)&#10;    Giữ lại 1 cột trong mỗi cặp tương quan cao&#10;    &quot;&quot;&quot;&#10;    print(f&quot;\n Kiểm tra tương quan giữa các features (threshold={threshold})...&quot;)&#10;    &#10;    # Tính ma trận tương quan&#10;    corr_matrix = df[feature_cols].corr().abs()&#10;    &#10;    # Lấy tam giác trên của ma trận&#10;    upper_tri = corr_matrix.where(&#10;        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)&#10;    )&#10;    &#10;    # Tìm các cột có tương quan cao&#10;    cols_to_drop = [column for column in upper_tri.columns &#10;                    if any(upper_tri[column] &gt; threshold)]&#10;    &#10;    if cols_to_drop:&#10;        print(f&quot;️ Loại bỏ {len(cols_to_drop)} cột tương quan cao:&quot;)&#10;        for col in cols_to_drop[:10]:  # Chỉ hiển thị 10 cột đầu&#10;            print(f&quot;   - {col}&quot;)&#10;        if len(cols_to_drop) &gt; 10:&#10;            print(f&quot;   ... và {len(cols_to_drop) - 10} cột khác&quot;)&#10;        &#10;        df = df.drop(columns=cols_to_drop)&#10;    &#10;    return df, cols_to_drop&#10;&#10;&#10;def normalize_features(X, scaler=None):&#10;    &quot;&quot;&quot;&#10;    Chuẩn hóa features sử dụng RobustScaler&#10;    RobustScaler ít bị ảnh hưởng bởi outliers&#10;    &quot;&quot;&quot;&#10;    if scaler is None:&#10;        scaler = RobustScaler()&#10;        X_normalized = scaler.fit_transform(X)&#10;    else:&#10;        X_normalized = scaler.transform(X)&#10;    &#10;    return X_normalized, scaler&#10;&#10;&#10;def reshape_for_cnn(X, method='1d'):&#10;    &quot;&quot;&quot;&#10;    Reshape dữ liệu cho CNN&#10;    &#10;    method='1d': Reshape thành (samples, features, 1) cho Conv1D&#10;    method='2d': Reshape thành hình vuông cho Conv2D (nếu có thể)&#10;    &quot;&quot;&quot;&#10;    n_samples, n_features = X.shape&#10;    &#10;    if method == '1d':&#10;        # Reshape cho Conv1D: (samples, features, 1)&#10;        X_reshaped = X.reshape(n_samples, n_features, 1)&#10;        print(f&quot; Reshape cho Conv1D: {X.shape} -&gt; {X_reshaped.shape}&quot;)&#10;        &#10;    elif method == '2d':&#10;        # Tìm kích thước hình vuông gần nhất&#10;        sqrt_features = int(np.ceil(np.sqrt(n_features)))&#10;        padded_features = sqrt_features ** 2&#10;        &#10;        # Padding nếu cần&#10;        if padded_features &gt; n_features:&#10;            padding = np.zeros((n_samples, padded_features - n_features))&#10;            X_padded = np.hstack([X, padding])&#10;        else:&#10;            X_padded = X&#10;        &#10;        # Reshape cho Conv2D: (samples, height, width, 1)&#10;        X_reshaped = X_padded.reshape(n_samples, sqrt_features, sqrt_features, 1)&#10;        print(f&quot; Reshape cho Conv2D: {X.shape} -&gt; {X_reshaped.shape}&quot;)&#10;    &#10;    else:&#10;        X_reshaped = X&#10;    &#10;    return X_reshaped&#10;&#10;&#10;# =============================================================================&#10;# HÀM CHÍNH - TIỀN XỬ LÝ TOÀN BỘ DATASET&#10;# =============================================================================&#10;&#10;def preprocess_dataset(chunk_size=100000, remove_high_corr=True, corr_threshold=0.95):&#10;    &quot;&quot;&quot;&#10;    Hàm chính để tiền xử lý toàn bộ dataset CICIDS2018&#10;    &#10;    Parameters:&#10;    -----------&#10;    chunk_size : int&#10;        Kích thước mỗi chunk khi đọc file&#10;    remove_high_corr : bool&#10;        Có loại bỏ các cột tương quan cao không&#10;    corr_threshold : float&#10;        Ngưỡng tương quan để loại bỏ (mặc định 0.95)&#10;    &#10;    Returns:&#10;    --------&#10;    Lưu các file sau vào thư mục output:&#10;        - X_processed.npy: Features đã chuẩn hóa&#10;        - y_binary.npy: Nhãn binary (0: Benign, 1: Attack)&#10;        - scaler.pkl: Scaler để transform dữ liệu mới&#10;        - feature_names.txt: Tên các features&#10;        - metadata.pkl: Thông tin metadata&#10;    &quot;&quot;&quot;&#10;    &#10;    print(&quot;=&quot;*70)&#10;    print(&quot; BẮT ĐẦU TIỀN XỬ LÝ DATASET CICIDS2018&quot;)&#10;    print(&quot;=&quot;*70)&#10;    &#10;    # Lấy đường dẫn&#10;    DATA_DIR, OUTPUT_DIR = get_paths()&#10;    &#10;    # Lấy danh sách file CSV&#10;    csv_files = get_csv_files(DATA_DIR)&#10;    &#10;    if not csv_files:&#10;        print(&quot;❌ Không tìm thấy file CSV nào!&quot;)&#10;        return&#10;    &#10;    # Xử lý từng file&#10;    all_data = []&#10;    &#10;    for file_path in csv_files:&#10;        df = process_single_file(file_path, COLUMNS_TO_DROP, chunk_size)&#10;        if df is not None:&#10;            all_data.append(df)&#10;        &#10;        # Giải phóng bộ nhớ&#10;        gc.collect()&#10;    &#10;    if not all_data:&#10;        print(&quot;❌ Không có dữ liệu để xử lý!&quot;)&#10;        return&#10;    &#10;    # Ghép tất cả dữ liệu&#10;    print(&quot;\n&quot; + &quot;=&quot;*70)&#10;    print(&quot; GHÉP VÀ XỬ LÝ TOÀN BỘ DỮ LIỆU&quot;)&#10;    print(&quot;=&quot;*70)&#10;    &#10;    full_df = pd.concat(all_data, ignore_index=True)&#10;    del all_data&#10;    gc.collect()&#10;    &#10;    print(f&quot; Tổng số dòng trước khi loại duplicate: {len(full_df):,}&quot;)&#10;    &#10;    # Loại bỏ duplicate toàn cục&#10;    full_df = full_df.drop_duplicates()&#10;    print(f&quot; Tổng số dòng sau khi loại duplicate: {len(full_df):,}&quot;)&#10;    &#10;    # Tách features và labels&#10;    y = full_df['Label_Binary'].values&#10;    X_df = full_df.drop(columns=['Label_Binary'])&#10;    feature_cols = X_df.columns.tolist()&#10;    &#10;    del full_df&#10;    gc.collect()&#10;    &#10;    # Loại bỏ cột hằng số&#10;    X_df, constant_cols = remove_constant_columns(X_df, feature_cols)&#10;    feature_cols = X_df.columns.tolist()&#10;    &#10;    # Loại bỏ cột tương quan cao (tùy chọn)&#10;    if remove_high_corr:&#10;        X_df, high_corr_cols = remove_high_correlation_columns(&#10;            X_df, feature_cols, corr_threshold&#10;        )&#10;        feature_cols = X_df.columns.tolist()&#10;    else:&#10;        high_corr_cols = []&#10;    &#10;    print(f&quot;\n Số features cuối cùng: {len(feature_cols)}&quot;)&#10;    &#10;    # Chuyển sang numpy array&#10;    X = X_df.values.astype(np.float32)&#10;    del X_df&#10;    gc.collect()&#10;    &#10;    # Xử lý NaN và Inf còn sót (nếu có)&#10;    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)&#10;    &#10;    # Chuẩn hóa features&#10;    print(&quot;\n Đang chuẩn hóa features...&quot;)&#10;    X_normalized, scaler = normalize_features(X)&#10;    &#10;    # Thống kê cuối cùng&#10;    print(&quot;\n&quot; + &quot;=&quot;*70)&#10;    print(&quot; THỐNG KÊ CUỐI CÙNG&quot;)&#10;    print(&quot;=&quot;*70)&#10;    print(f&quot;Kích thước X: {X_normalized.shape}&quot;)&#10;    print(f&quot;Kích thước y: {y.shape}&quot;)&#10;    print(f&quot;Số features: {len(feature_cols)}&quot;)&#10;    print(f&quot;Số mẫu Benign (0): {np.sum(y == 0):,}&quot;)&#10;    print(f&quot;Số mẫu Attack (1): {np.sum(y == 1):,}&quot;)&#10;    print(f&quot;Tỷ lệ Attack: {np.mean(y) * 100:.2f}%&quot;)&#10;    &#10;    # Lưu dữ liệu&#10;    print(&quot;\n Đang lưu dữ liệu...&quot;)&#10;    &#10;    # Lưu features và labels&#10;    np.save(os.path.join(OUTPUT_DIR, 'X_processed.npy'), X_normalized)&#10;    np.save(os.path.join(OUTPUT_DIR, 'y_binary.npy'), y)&#10;    &#10;    # Lưu scaler&#10;    with open(os.path.join(OUTPUT_DIR, 'scaler.pkl'), 'wb') as f:&#10;        pickle.dump(scaler, f)&#10;    &#10;    # Lưu tên features&#10;    with open(os.path.join(OUTPUT_DIR, 'feature_names.txt'), 'w') as f:&#10;        for name in feature_cols:&#10;            f.write(f&quot;{name}\n&quot;)&#10;    &#10;    # Lưu metadata&#10;    metadata = {&#10;        'n_samples': X_normalized.shape[0],&#10;        'n_features': X_normalized.shape[1],&#10;        'feature_names': feature_cols,&#10;        'columns_dropped': COLUMNS_TO_DROP,&#10;        'constant_columns_removed': constant_cols,&#10;        'high_corr_columns_removed': high_corr_cols,&#10;        'n_benign': int(np.sum(y == 0)),&#10;        'n_attack': int(np.sum(y == 1)),&#10;        'attack_ratio': float(np.mean(y)),&#10;    }&#10;    &#10;    with open(os.path.join(OUTPUT_DIR, 'metadata.pkl'), 'wb') as f:&#10;        pickle.dump(metadata, f)&#10;    &#10;    print(f&quot;\n✅ ĐÃ LƯU DỮ LIỆU VÀO: {OUTPUT_DIR}&quot;)&#10;    print(&quot;   - X_processed.npy&quot;)&#10;    print(&quot;   - y_binary.npy&quot;)&#10;    print(&quot;   - scaler.pkl&quot;)&#10;    print(&quot;   - feature_names.txt&quot;)&#10;    print(&quot;   - metadata.pkl&quot;)&#10;    &#10;    print(&quot;\n&quot; + &quot;=&quot;*70)&#10;    print(&quot; HOÀN THÀNH TIỀN XỬ LÝ!&quot;)&#10;    print(&quot;=&quot;*70)&#10;    &#10;    return X_normalized, y, scaler, feature_cols, metadata&#10;&#10;&#10;# =============================================================================&#10;# CHẠY CHƯƠNG TRÌNH&#10;# =============================================================================&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    # Chạy tiền xử lý&#10;    result = preprocess_dataset(&#10;        chunk_size=100000,      # Kích thước chunk&#10;        remove_high_corr=True,  # Loại bỏ cột tương quan cao&#10;        corr_threshold=0.95     # Ngưỡng tương quan&#10;    )&#10;    &#10;    if result:&#10;        X, y, scaler, feature_names, metadata = result&#10;        print(f&quot;\n Shape của X: {X.shape}&quot;)&#10;        print(f&quot; Shape của y: {y.shape}&quot;)&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/PROGRESS_BAR_GUIDE.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/PROGRESS_BAR_GUIDE.md" />
              <option name="updatedContent" value="#  Progress Bar - Hướng dẫn sử dụng&#10;&#10;## ✅ ĐÃ THÊM PROGRESS BAR&#10;&#10;Script `preprocess_data.py` đã được cập nhật với **thanh tiến trình (progress bar)** để theo dõi tiến độ xử lý!&#10;&#10;---&#10;&#10;##  Cài đặt (khuyến nghị)&#10;&#10;### Cài tqdm để có progress bar:&#10;&#10;```powershell&#10;pip install tqdm&#10;```&#10;&#10;**Sau khi cài xong, chạy:**&#10;```powershell&#10;python preprocess_data.py&#10;```&#10;&#10;---&#10;&#10;##  Các thanh tiến trình&#10;&#10;### 1. **Loading CSV files**&#10;```&#10; Loading CSV files: 100%|████████| 10/10 [02:30&lt;00:00, 15.0s/file]&#10;```&#10;- Hiển thị số file đã load&#10;- Thời gian còn lại (ETA)&#10;- Tốc độ load (files/second)&#10;&#10;### 2. **Processing inf values**&#10;```&#10; Processing inf values: 100%|████████| 68/68 [00:15&lt;00:00, 4.5col/s]&#10;```&#10;- Số cột đã xử lý&#10;- Thời gian còn lại&#10;&#10;### 3. **Normalizing features**&#10;```&#10; Normalizing: 100%|████████| 2/2 [00:30&lt;00:00, 15.0s/step]&#10;```&#10;- Tiến độ normalization&#10;&#10;### 4. **Saving files**&#10;```&#10; Saving files: 100%|████████| 8/8 [01:00&lt;00:00, 7.5s/file]&#10;```&#10;- Số file đã lưu&#10;- File đang l��u&#10;&#10;---&#10;&#10;##  Minh họa output&#10;&#10;### Với tqdm (CÓ progress bar):&#10;&#10;```&#10;================================================================================&#10;LOADING CICIDS2018 DATA&#10;================================================================================&#10;Tìm thấy 10 file CSV&#10; Loading CSV files:  50%|██████▌       | 5/10 [01:15&lt;01:15, 15.0s/file]&#10;```&#10;&#10;### Không có tqdm (KHÔNG progress bar):&#10;&#10;```&#10;================================================================================&#10;LOADING CICIDS2018 DATA&#10;================================================================================&#10;Tìm thấy 10 file CSV&#10;⚠️  tqdm not installed. Install for progress bars: pip install tqdm&#10;  Đang đọc: Friday-02-03-2018_TrafficForML_CICFlowMeter.csv...&#10;    ✓ 445,909 rows loaded&#10;  Đang đọc: Friday-16-02-2018_TrafficForML_CICFlowMeter.csv...&#10;    ✓ 663,808 rows loaded&#10;  ...&#10;```&#10;&#10;**→ Vẫn chạy được nhưng không có progress bar!**&#10;&#10;---&#10;&#10;##  Lợi ích của Progress Bar&#10;&#10;### 1. **Biết được tiến độ**&#10;- Bao nhiêu % đã hoàn thành&#10;- Còn bao lâu nữa (ETA - Estimated Time of Arrival)&#10;&#10;### 2. **Không lo &quot;script bị treo&quot;**&#10;- Thấy thanh chạy → biết script vẫn hoạt động&#10;- Thấy file/cột đang xử lý&#10;&#10;### 3. **Tối ưu workflow**&#10;- Biết step nào chậm nhất&#10;- Có thể đi làm việc khác khi còn lâu&#10;&#10;---&#10;&#10;##  So sánh&#10;&#10;| Aspect | Không có tqdm | Có tqdm |&#10;|--------|---------------|---------|&#10;| **Progress** | ❌ Không biết % | ✅ Biết rõ % |&#10;| **ETA** | ❌ Không biết còn bao lâu | ✅ Hiển thị thời gian còn lại |&#10;| **Tốc độ** | ❌ Không biết | ✅ Hiển thị items/s |&#10;| **Cài đặt** | ✅ Không cần | ⚠️ Cần: `pip install tqdm` |&#10;| **Hoạt động** | ✅ Vẫn chạy bình thường | ✅ Chạy + có progress bar |&#10;&#10;---&#10;&#10;##  Chi tiết kỹ thuật&#10;&#10;### Auto-detect tqdm:&#10;&#10;```python&#10;try:&#10;    from tqdm import tqdm&#10;    TQDM_AVAILABLE = True&#10;except ImportError:&#10;    TQDM_AVAILABLE = False&#10;    print(&quot;⚠️  tqdm not installed. Install for progress bars: pip install tqdm&quot;)&#10;    tqdm = lambda x, **kwargs: x  # Fallback&#10;```&#10;&#10;**Cách hoạt động:**&#10;- Nếu **CÓ tqdm**: Dùng progress bar&#10;- Nếu **KHÔNG có**: Fallback về in thông thường&#10;&#10;**→ Script luôn chạy được, dù có hay không có tqdm!**&#10;&#10;---&#10;&#10;## ✅ Checklist&#10;&#10;### Để có progress bar đẹp:&#10;- [ ] Cài tqdm: `pip install tqdm`&#10;- [ ] Chạy script: `python preprocess_data.py`&#10;- [ ] Thấy các thanh tiến trình:    &#10;&#10;### Nếu không muốn cài tqdm:&#10;- [ ] Bỏ qua, chạy luôn: `python preprocess_data.py`&#10;- [ ] Thấy text thông thường (không có progress bar)&#10;- [ ] Vẫn hoạt động bình thường!&#10;&#10;---&#10;&#10;##  Demo output (với tqdm)&#10;&#10;```&#10;╔══════════════════════════════════════════════════════════════════════════════╗&#10;║               CICIDS2018 DATA PREPROCESSING PIPELINE                         ║&#10;╚══════════════════════════════════════════════════════════════════════════════╝&#10;&#10;&#10;================================================================================&#10;LOADING CICIDS2018 DATA&#10;================================================================================&#10;Tìm thấy 10 file CSV&#10; Loading CSV files:  30%|███▊           | 3/10 [00:45&lt;01:45, 15.0s/file] {rows: 1,109,717, filtered: 6}&#10;&#10;================================================================================&#10;CLEANING DATA&#10;================================================================================&#10;Dropping inconsistent/unnecessary columns: ['Timestamp']&#10;✓ Remaining columns: 79&#10;✓ Features kept: 78&#10;...&#10; Processing inf values: 100%|████████████| 68/68 [00:15&lt;00:00, 4.5col/s]&#10;&#10;================================================================================&#10;NORMALIZING FEATURES&#10;================================================================================&#10;Normalizing 68 features...&#10; Normalizing: 100%|████████████████████| 2/2 [00:30&lt;00:00, 15.0s/step] {status: completed}&#10;✓ Features normalized&#10;&#10;================================================================================&#10;SAVING PROCESSED DATA&#10;================================================================================&#10; Saving files: 100%|████████████████████| 8/8 [01:00&lt;00:00, 7.5s/file] {file: metadata.pkl}&#10;&#10;✓ All files saved successfully!&#10;  - Feature array: (16232943, 68)&#10;  - Binary labels: (16232943,)&#10;  - Multi-class labels: (16232943,)&#10;&#10;================================================================================&#10;PREPROCESSING COMPLETED!&#10;================================================================================&#10;```&#10;&#10;**→ Rất dễ theo dõi tiến độ!** &#10;&#10;---&#10;&#10;##  Troubleshooting&#10;&#10;### Q: Cài tqdm nhưng vẫn không thấy progress bar?&#10;A: Restart terminal hoặc Python environment&#10;&#10;### Q: Progress bar bị nhấp nháy/lỗi?&#10;A: Đang chạy trong IDE có thể không hỗ trợ tốt. Chạy trong terminal thuần.&#10;&#10;### Q: Có bắt buộc phải cài tqdm không?&#10;A: **KHÔNG!** Script vẫn chạy bình thường, chỉ không có progress bar.&#10;&#10;---&#10;&#10;**Happy preprocessing với progress bar! **&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/extract_labels_no_pandas.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/extract_labels_no_pandas.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Script đơn giản để trích xuất các nhãn (Label) từ tệp CSV&#10;Chỉ sử dụng thư viện chuẩn của Python (không cần pandas)&#10;&quot;&quot;&quot;&#10;&#10;import csv&#10;from collections import Counter&#10;import os&#10;&#10;def extract_labels_no_pandas(csv_file_path):&#10;    &quot;&quot;&quot;&#10;    Trích xuất các nhãn từ một tệp CSV cụ thể&#10;    Không sử dụng pandas - chỉ dùng thư viện chuẩn&#10;    &#10;    Args:&#10;        csv_file_path: Đường dẫn đến tệp CSV&#10;    &quot;&quot;&quot;&#10;    print(f&quot;Đang đọc tệp: {csv_file_path}&quot;)&#10;    &#10;    labels = []&#10;    &#10;    # Đọc tệp CSV&#10;    with open(csv_file_path, 'r', encoding='utf-8') as file:&#10;        csv_reader = csv.DictReader(file)&#10;        &#10;        # Kiểm tra xem có cột 'Label' không&#10;        if 'Label' not in csv_reader.fieldnames:&#10;            print(&quot;Lỗi: Không tìm thấy cột 'Label' trong tệp CSV&quot;)&#10;            print(f&quot;Các cột có sẵn: {csv_reader.fieldnames}&quot;)&#10;            return None, None&#10;        &#10;        # Đọc tất cả các nhãn&#10;        for row in csv_reader:&#10;            labels.append(row['Label'])&#10;    &#10;    # Lấy các nhãn duy nhất&#10;    unique_labels = sorted(set(labels))&#10;    &#10;    # Đếm số lượng mỗi nhãn&#10;    label_counts = Counter(labels)&#10;    &#10;    # Hiển thị kết quả&#10;    print(&quot;\n&quot; + &quot;=&quot; * 60)&#10;    print(&quot;CÁC NHÃN TÌM THẤY&quot;)&#10;    print(&quot;=&quot; * 60)&#10;    print(f&quot;\nTổng số nhãn duy nhất: {len(unique_labels)}&quot;)&#10;    print(f&quot;\nDanh sách các nhãn:&quot;)&#10;    for i, label in enumerate(unique_labels, 1):&#10;        print(f&quot;  {i}. {label}&quot;)&#10;    &#10;    print(&quot;\n&quot; + &quot;=&quot; * 60)&#10;    print(&quot;PHÂN PHỐI SỐ LƯỢNG MỖI NHÃN&quot;)&#10;    print(&quot;=&quot; * 60)&#10;    total = len(labels)&#10;    for label, count in label_counts.most_common():&#10;        percentage = (count / total) * 100&#10;        print(f&quot;{label}: {count:,} ({percentage:.2f}%)&quot;)&#10;    &#10;    print(f&quot;\nTổng số dòng dữ liệu: {total:,}&quot;)&#10;    &#10;    return unique_labels, label_counts&#10;&#10;def extract_labels_from_all_files(directory_path):&#10;    &quot;&quot;&quot;&#10;    Trích xuất nhãn từ tất cả các tệp CSV trong thư mục&#10;    &#10;    Args:&#10;        directory_path: Đường dẫn đến thư mục chứa các tệp CSV&#10;    &quot;&quot;&quot;&#10;    all_labels = set()&#10;    &#10;    print(&quot;\n&quot; + &quot;=&quot; * 80)&#10;    print(&quot;TRÍCH XUẤT NHÃN TỪ TẤT CẢ CÁC TẾP CSV&quot;)&#10;    print(&quot;=&quot; * 80)&#10;    &#10;    # Lấy tất cả các tệp CSV&#10;    csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]&#10;    &#10;    print(f&quot;\nTìm thấy {len(csv_files)} tệp CSV\n&quot;)&#10;    &#10;    for csv_file in sorted(csv_files):&#10;        file_path = os.path.join(directory_path, csv_file)&#10;        print(f&quot;\n{'─' * 80}&quot;)&#10;        print(f&quot;Đang xử lý: {csv_file}&quot;)&#10;        print('─' * 80)&#10;        &#10;        try:&#10;            with open(file_path, 'r', encoding='utf-8') as file:&#10;                csv_reader = csv.DictReader(file)&#10;                &#10;                if 'Label' in csv_reader.fieldnames:&#10;                    file_labels = set()&#10;                    for row in csv_reader:&#10;                        file_labels.add(row['Label'])&#10;                    &#10;                    all_labels.update(file_labels)&#10;                    print(f&quot;Các nhãn tìm thấy ({len(file_labels)}): {sorted(file_labels)}&quot;)&#10;                else:&#10;                    print(&quot;Cảnh báo: Không tìm thấy cột 'Label'&quot;)&#10;        &#10;        except Exception as e:&#10;            print(f&quot;Lỗi khi đọc tệp: {e}&quot;)&#10;    &#10;    # Hiển thị tổng kết&#10;    print(&quot;\n&quot; + &quot;=&quot; * 80)&#10;    print(&quot;KẾT QUẢ TỔNG HỢP&quot;)&#10;    print(&quot;=&quot; * 80)&#10;    print(f&quot;\nTổng số nhãn duy nhất tìm thấy: {len(all_labels)}&quot;)&#10;    print(f&quot;\nDanh sách các nhãn:&quot;)&#10;    for i, label in enumerate(sorted(all_labels), 1):&#10;        print(f&quot;  {i}. {label}&quot;)&#10;    &#10;    return all_labels&#10;&#10;# Chương trình chính&#10;if __name__ == &quot;__main__&quot;:&#10;    # Lựa chọn: phân tích một tệp hoặc tất cả các tệp&#10;    print(&quot;Chọn chế độ:&quot;)&#10;    print(&quot;1. Trích xuất nhãn từ một tệp cụ thể&quot;)&#10;    print(&quot;2. Trích xuất nhãn từ tất cả các tệp CSV trong thư mục&quot;)&#10;    &#10;    choice = input(&quot;\nNhập lựa chọn của bạn (1 hoặc 2): &quot;).strip()&#10;    &#10;    if choice == &quot;1&quot;:&#10;        # Phân tích một tệp cụ thể&#10;        csv_file = r&quot;D:\PROJECT\Machine Learning\IOT\CICIDS2018-CSV\Friday-02-03-2018_TrafficForML_CICFlowMeter.csv&quot;&#10;        &#10;        # Hoặc để người dùng nhập đường dẫn&#10;        custom_path = input(f&quot;\nNhập đường dẫn tệp CSV (Enter để dùng mặc định):\n{csv_file}\n&gt; &quot;).strip()&#10;        if custom_path:&#10;            csv_file = custom_path&#10;        &#10;        try:&#10;            labels, counts = extract_labels_no_pandas(csv_file)&#10;        except FileNotFoundError:&#10;            print(f&quot;\nLỗi: Không tìm thấy tệp {csv_file}&quot;)&#10;        except Exception as e:&#10;            print(f&quot;\nLỗi: {e}&quot;)&#10;    &#10;    elif choice == &quot;2&quot;:&#10;        # Phân tích tất cả các tệp&#10;        directory = r&quot;D:\PROJECT\Machine Learning\IOT\CICIDS2018-CSV&quot;&#10;        &#10;        custom_dir = input(f&quot;\nNhập đường dẫn thư mục (Enter để dùng mặc định):\n{directory}\n&gt; &quot;).strip()&#10;        if custom_dir:&#10;            directory = custom_dir&#10;        &#10;        try:&#10;            all_labels = extract_labels_from_all_files(directory)&#10;            &#10;            # Lưu kết quả ra tệp&#10;            output_file = &quot;labels_summary.txt&quot;&#10;            with open(output_file, 'w', encoding='utf-8') as f:&#10;                f.write(&quot;TỔNG HỢP CÁC NHÃN TỪ CICIDS2018 DATASET\n&quot;)&#10;                f.write(&quot;=&quot; * 80 + &quot;\n\n&quot;)&#10;                f.write(f&quot;Tổng số nhãn duy nhất: {len(all_labels)}\n\n&quot;)&#10;                f.write(&quot;Danh sách các nhãn:\n&quot;)&#10;                for label in sorted(all_labels):&#10;                    f.write(f&quot;  - {label}\n&quot;)&#10;            &#10;            print(f&quot;\nKết quả đã được lưu vào: {output_file}&quot;)&#10;        &#10;        except Exception as e:&#10;            print(f&quot;\nLỗi: {e}&quot;)&#10;    &#10;    else:&#10;        print(&quot;\nLựa chọn không hợp lệ!&quot;)&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>
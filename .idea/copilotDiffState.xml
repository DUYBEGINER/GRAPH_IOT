<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/CNN/cnn_cicids2018_full_pipeline.ipynb">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/CNN/cnn_cicids2018_full_pipeline.ipynb" />
              <option name="updatedContent" value="{&#10; &quot;cells&quot;: [&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;#  PHÁT HIỆN LƯU LƯỢNG MẠNG IOT BẤT THƯỜNG BẰNG CNN\n&quot;,&#10;    &quot;## Binary Classification: Benign vs Attack\n&quot;,&#10;    &quot;### Dataset: CSE-CIC-IDS2018\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;---\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;**Các bước thực hiện:**\n&quot;,&#10;    &quot;1. **Bước 1:** Clean dữ liệu (loại bỏ cột ID, xử lý NaN/Inf, loại duplicate)\n&quot;,&#10;    &quot;2. **Bước 2:** Cân bằng dữ liệu và chuẩn bị training (70% Benign, 30% Attack)\n&quot;,&#10;    &quot;3. **Bước 3:** Train mô hình CNN\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;**Kiến trúc CNN:**\n&quot;,&#10;    &quot;- 5 lớp Conv1D với MaxPooling\n&quot;,&#10;    &quot;- BatchNormalization + Dropout\n&quot;,&#10;    &quot;- Output: Dense(1, sigmoid) cho binary classification&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;##  IMPORT THƯ VIỆN&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;import os\n&quot;,&#10;    &quot;import numpy as np\n&quot;,&#10;    &quot;import pandas as pd\n&quot;,&#10;    &quot;import pickle\n&quot;,&#10;    &quot;import json\n&quot;,&#10;    &quot;import gc\n&quot;,&#10;    &quot;from pathlib import Path\n&quot;,&#10;    &quot;from datetime import datetime\n&quot;,&#10;    &quot;import warnings\n&quot;,&#10;    &quot;warnings.filterwarnings('ignore')\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Sklearn\n&quot;,&#10;    &quot;from sklearn.preprocessing import StandardScaler\n&quot;,&#10;    &quot;from sklearn.model_selection import train_test_split\n&quot;,&#10;    &quot;from sklearn.utils.class_weight import compute_class_weight\n&quot;,&#10;    &quot;from sklearn.metrics import confusion_matrix, classification_report\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# TensorFlow/Keras\n&quot;,&#10;    &quot;import tensorflow as tf\n&quot;,&#10;    &quot;from tensorflow import keras\n&quot;,&#10;    &quot;from tensorflow.keras.models import Sequential\n&quot;,&#10;    &quot;from tensorflow.keras.layers import (\n&quot;,&#10;    &quot;    Conv1D, MaxPooling1D, Flatten, Dense,\n&quot;,&#10;    &quot;    Dropout, BatchNormalization, Input\n&quot;,&#10;    &quot;)\n&quot;,&#10;    &quot;from tensorflow.keras.callbacks import (\n&quot;,&#10;    &quot;    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n&quot;,&#10;    &quot;)\n&quot;,&#10;    &quot;from tensorflow.keras.metrics import Precision, Recall\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Visualization\n&quot;,&#10;    &quot;import matplotlib.pyplot as plt\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Progress bar\n&quot;,&#10;    &quot;from tqdm import tqdm\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;TensorFlow version: {tf.__version__}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;GPU available: {tf.config.list_physical_devices('GPU')}\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;## ⚙️ CẤU HÌNH&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# ============================================================================\n&quot;,&#10;    &quot;# CẤU HÌNH ĐƯỜNG DẪN\n&quot;,&#10;    &quot;# ============================================================================\n&quot;,&#10;    &quot;IS_KAGGLE = os.path.exists('/kaggle/input')\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;if IS_KAGGLE:\n&quot;,&#10;    &quot;    # Đường dẫn Kaggle - THAY ĐỔI TÊN DATASET NẾU CẦN\n&quot;,&#10;    &quot;    DATA_DIR = \&quot;/kaggle/input/cicids2018\&quot;  # Thay đổi theo tên dataset của bạn\n&quot;,&#10;    &quot;    OUTPUT_DIR = \&quot;/kaggle/working\&quot;\n&quot;,&#10;    &quot;    print(\&quot; Đang chạy trên KAGGLE\&quot;)\n&quot;,&#10;    &quot;else:\n&quot;,&#10;    &quot;    # Đường dẫn Local\n&quot;,&#10;    &quot;    DATA_DIR = r\&quot;D:\\PROJECT\\Machine Learning\\IOT\\CICIDS2018-CSV\&quot;\n&quot;,&#10;    &quot;    OUTPUT_DIR = r\&quot;D:\\PROJECT\\Machine Learning\\IOT\\CNN\&quot;\n&quot;,&#10;    &quot;    print(\&quot; Đang chạy trên LOCAL\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Tạo các thư mục con\n&quot;,&#10;    &quot;CLEANED_DATA_DIR = os.path.join(OUTPUT_DIR, \&quot;cleaned_data\&quot;)\n&quot;,&#10;    &quot;TRAINING_DATA_DIR = os.path.join(OUTPUT_DIR, \&quot;training_data\&quot;)\n&quot;,&#10;    &quot;MODEL_DIR = os.path.join(OUTPUT_DIR, \&quot;models\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;for d in [CLEANED_DATA_DIR, TRAINING_DATA_DIR, MODEL_DIR]:\n&quot;,&#10;    &quot;    os.makedirs(d, exist_ok=True)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# ============================================================================\n&quot;,&#10;    &quot;# CẤU HÌNH XỬ LÝ DỮ LIỆU\n&quot;,&#10;    &quot;# ============================================================================\n&quot;,&#10;    &quot;CHUNK_SIZE = 300000        # Số dòng mỗi chunk khi đọc CSV\n&quot;,&#10;    &quot;RANDOM_STATE = 42          # Random seed\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# ============================================================================\n&quot;,&#10;    &quot;# CẤU HÌNH CÂN BẰNG DỮ LIỆU\n&quot;,&#10;    &quot;# ============================================================================\n&quot;,&#10;    &quot;TOTAL_SAMPLES = 3000000    # Tổng số mẫu mong muốn (3 triệu)\n&quot;,&#10;    &quot;BENIGN_RATIO = 0.70        # 70% Benign\n&quot;,&#10;    &quot;ATTACK_RATIO = 0.30        # 30% Attack\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;TARGET_BENIGN = int(TOTAL_SAMPLES * BENIGN_RATIO)  # 2,100,000\n&quot;,&#10;    &quot;TARGET_ATTACK = int(TOTAL_SAMPLES * ATTACK_RATIO)  # 900,000\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Tỷ lệ chia train/val/test\n&quot;,&#10;    &quot;TEST_SIZE = 0.20   # 20% test\n&quot;,&#10;    &quot;VAL_SIZE = 0.10    # 10% validation\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# ============================================================================\n&quot;,&#10;    &quot;# CẤU HÌNH TRAINING\n&quot;,&#10;    &quot;# ============================================================================\n&quot;,&#10;    &quot;BATCH_SIZE = 256\n&quot;,&#10;    &quot;EPOCHS = 50\n&quot;,&#10;    &quot;LEARNING_RATE = 0.001\n&quot;,&#10;    &quot;DROPOUT_RATE = 0.5\n&quot;,&#10;    &quot;PATIENCE = 10\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# ============================================================================\n&quot;,&#10;    &quot;# CÁC CỘT CẦN LOẠI BỎ\n&quot;,&#10;    &quot;# ============================================================================\n&quot;,&#10;    &quot;COLUMNS_TO_DROP = [\n&quot;,&#10;    &quot;    'Flow ID', 'Src IP', 'Dst IP', 'Src Port', 'Dst Port', 'Timestamp'\n&quot;,&#10;    &quot;]\n&quot;,&#10;    &quot;LABEL_COLUMN = 'Label'\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;\\n CẤU HÌNH:\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Tổng mẫu: {TOTAL_SAMPLES:,}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Benign: {TARGET_BENIGN:,} ({BENIGN_RATIO*100:.0f}%)\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Attack: {TARGET_ATTACK:,} ({ATTACK_RATIO*100:.0f}%)\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;---\n&quot;,&#10;    &quot;#  BƯỚC 1: CLEAN DỮ LIỆU\n&quot;,&#10;    &quot;---&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def get_csv_files(data_dir):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;Lấy danh sách các file CSV\&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    data_dir = Path(data_dir)\n&quot;,&#10;    &quot;    csv_files = list(data_dir.glob(\&quot;*_TrafficForML_CICFlowMeter.csv\&quot;))\n&quot;,&#10;    &quot;    if not csv_files:\n&quot;,&#10;    &quot;        csv_files = [f for f in data_dir.glob(\&quot;*.csv\&quot;) if not f.name.endswith('.zip')]\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    if not csv_files:\n&quot;,&#10;    &quot;        raise FileNotFoundError(f\&quot;Không tìm thấy file CSV trong {data_dir}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;\\n Tìm thấy {len(csv_files)} file CSV:\&quot;)\n&quot;,&#10;    &quot;    for f in sorted(csv_files):\n&quot;,&#10;    &quot;        print(f\&quot;   - {f.name}\&quot;)\n&quot;,&#10;    &quot;    return sorted(csv_files)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;csv_files = get_csv_files(DATA_DIR)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def first_pass_collect_info(csv_files, chunk_size=CHUNK_SIZE):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    Lần đọc đầu tiên: Thu thập thông tin về columns và tính mode\n&quot;,&#10;    &quot;    Mục đích:\n&quot;,&#10;    &quot;    - Xác định các cột có variance = 0\n&quot;,&#10;    &quot;    - Tính mode của từng cột để thay thế NaN/Inf\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    print(\&quot;\\n\&quot; + \&quot;=\&quot;*80)\n&quot;,&#10;    &quot;    print(\&quot; BƯỚC 1.1: THU THẬP THÔNG TIN TỪ DỮ LIỆU\&quot;)\n&quot;,&#10;    &quot;    print(\&quot;=\&quot;*80)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    all_columns = None\n&quot;,&#10;    &quot;    column_value_counts = {}  # Để tính mode\n&quot;,&#10;    &quot;    column_min_max = {}  # Để kiểm tra variance\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    for csv_file in csv_files:\n&quot;,&#10;    &quot;        print(f\&quot;\\n   Đang scan: {csv_file.name}\&quot;)\n&quot;,&#10;    &quot;        chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size,\n&quot;,&#10;    &quot;                                    low_memory=False, encoding='utf-8')\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        for chunk in tqdm(chunk_iterator, desc=\&quot;   Chunks\&quot;):\n&quot;,&#10;    &quot;            # Chuẩn hóa tên cột\n&quot;,&#10;    &quot;            chunk.columns = chunk.columns.str.strip()\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Loại bỏ cột identification\n&quot;,&#10;    &quot;            cols_to_drop = [c for c in COLUMNS_TO_DROP if c in chunk.columns]\n&quot;,&#10;    &quot;            chunk = chunk.drop(columns=cols_to_drop)\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Chuyển sang numeric\n&quot;,&#10;    &quot;            feature_cols = [c for c in chunk.columns if c != LABEL_COLUMN]\n&quot;,&#10;    &quot;            for col in feature_cols:\n&quot;,&#10;    &quot;                if chunk[col].dtype == 'object':\n&quot;,&#10;    &quot;                    chunk[col] = pd.to_numeric(chunk[col], errors='coerce')\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            if all_columns is None:\n&quot;,&#10;    &quot;                all_columns = feature_cols\n&quot;,&#10;    &quot;                for col in all_columns:\n&quot;,&#10;    &quot;                    column_value_counts[col] = {}\n&quot;,&#10;    &quot;                    column_min_max[col] = {'min': np.inf, 'max': -np.inf}\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Thu thập thông tin cho mỗi cột\n&quot;,&#10;    &quot;            for col in all_columns:\n&quot;,&#10;    &quot;                if col in chunk.columns:\n&quot;,&#10;    &quot;                    col_data = chunk[col].replace([np.inf, -np.inf], np.nan)\n&quot;,&#10;    &quot;                    valid_data = col_data.dropna()\n&quot;,&#10;    &quot;                    \n&quot;,&#10;    &quot;                    if len(valid_data) &gt; 0:\n&quot;,&#10;    &quot;                        # Cập nhật min/max\n&quot;,&#10;    &quot;                        column_min_max[col]['min'] = min(column_min_max[col]['min'], valid_data.min())\n&quot;,&#10;    &quot;                        column_min_max[col]['max'] = max(column_min_max[col]['max'], valid_data.max())\n&quot;,&#10;    &quot;                        \n&quot;,&#10;    &quot;                        # Thu thập value counts cho mode\n&quot;,&#10;    &quot;                        vc = valid_data.value_counts().head(10).to_dict()\n&quot;,&#10;    &quot;                        for val, count in vc.items():\n&quot;,&#10;    &quot;                            if val not in column_value_counts[col]:\n&quot;,&#10;    &quot;                                column_value_counts[col][val] = 0\n&quot;,&#10;    &quot;                            column_value_counts[col][val] += count\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            gc.collect()\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Xác định zero-variance columns\n&quot;,&#10;    &quot;    zero_variance_cols = []\n&quot;,&#10;    &quot;    for col in all_columns:\n&quot;,&#10;    &quot;        if column_min_max[col]['min'] == column_min_max[col]['max']:\n&quot;,&#10;    &quot;            zero_variance_cols.append(col)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Tính mode cho mỗi cột\n&quot;,&#10;    &quot;    column_modes = {}\n&quot;,&#10;    &quot;    for col in all_columns:\n&quot;,&#10;    &quot;        if col not in zero_variance_cols:\n&quot;,&#10;    &quot;            if column_value_counts[col]:\n&quot;,&#10;    &quot;                mode_val = max(column_value_counts[col], key=column_value_counts[col].get)\n&quot;,&#10;    &quot;                column_modes[col] = mode_val\n&quot;,&#10;    &quot;            else:\n&quot;,&#10;    &quot;                column_modes[col] = 0\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;\\n   ✅ Số cột zero-variance sẽ loại bỏ: {len(zero_variance_cols)}\&quot;)\n&quot;,&#10;    &quot;    if zero_variance_cols:\n&quot;,&#10;    &quot;        print(f\&quot;      Các cột: {zero_variance_cols}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   ✅ Số cột sẽ giữ lại: {len(all_columns) - len(zero_variance_cols)}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    return all_columns, zero_variance_cols, column_modes\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;all_columns, zero_variance_cols, column_modes = first_pass_collect_info(csv_files)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def clean_all_files(csv_files, zero_variance_cols, column_modes, chunk_size=CHUNK_SIZE):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    Clean tất cả các file CSV\n&quot;,&#10;    &quot;    - Loại bỏ cột identification\n&quot;,&#10;    &quot;    - Loại bỏ zero-variance columns\n&quot;,&#10;    &quot;    - Xử lý NaN/Inf bằng mode\n&quot;,&#10;    &quot;    - Chuyển nhãn sang binary\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    print(\&quot;\\n\&quot; + \&quot;=\&quot;*80)\n&quot;,&#10;    &quot;    print(\&quot; BƯỚC 1.2: CLEAN DỮ LIỆU\&quot;)\n&quot;,&#10;    &quot;    print(\&quot;=\&quot;*80)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    all_dataframes = []\n&quot;,&#10;    &quot;    stats = {\n&quot;,&#10;    &quot;        'total_rows_read': 0,\n&quot;,&#10;    &quot;        'nan_replaced': 0,\n&quot;,&#10;    &quot;        'inf_replaced': 0\n&quot;,&#10;    &quot;    }\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    for csv_file in csv_files:\n&quot;,&#10;    &quot;        print(f\&quot;\\n Đang xử lý: {csv_file.name}\&quot;)\n&quot;,&#10;    &quot;        processed_chunks = []\n&quot;,&#10;    &quot;        chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size,\n&quot;,&#10;    &quot;                                     low_memory=False, encoding='utf-8')\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        for chunk in tqdm(chunk_iterator, desc=\&quot;   Chunks\&quot;):\n&quot;,&#10;    &quot;            stats['total_rows_read'] += len(chunk)\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Chuẩn hóa tên cột\n&quot;,&#10;    &quot;            chunk.columns = chunk.columns.str.strip()\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Loại bỏ cột identification\n&quot;,&#10;    &quot;            cols_to_drop = [c for c in COLUMNS_TO_DROP if c in chunk.columns]\n&quot;,&#10;    &quot;            chunk = chunk.drop(columns=cols_to_drop)\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Chuyển sang numeric\n&quot;,&#10;    &quot;            feature_cols = [c for c in chunk.columns if c != LABEL_COLUMN]\n&quot;,&#10;    &quot;            for col in feature_cols:\n&quot;,&#10;    &quot;                if chunk[col].dtype == 'object':\n&quot;,&#10;    &quot;                    chunk[col] = pd.to_numeric(chunk[col], errors='coerce')\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Chuyển đổi nhãn sang binary\n&quot;,&#10;    &quot;            chunk[LABEL_COLUMN] = chunk[LABEL_COLUMN].astype(str).str.strip().str.lower()\n&quot;,&#10;    &quot;            chunk = chunk[chunk[LABEL_COLUMN] != 'label']  # Loại header lẫn vào\n&quot;,&#10;    &quot;            chunk['binary_label'] = (chunk[LABEL_COLUMN] != 'benign').astype(int)\n&quot;,&#10;    &quot;            chunk = chunk.drop(columns=[LABEL_COLUMN])\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Loại bỏ zero-variance columns\n&quot;,&#10;    &quot;            cols_zv = [c for c in zero_variance_cols if c in chunk.columns]\n&quot;,&#10;    &quot;            chunk = chunk.drop(columns=cols_zv)\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            # Xử lý NaN/Inf bằng mode\n&quot;,&#10;    &quot;            feature_cols = [c for c in chunk.columns if c != 'binary_label']\n&quot;,&#10;    &quot;            for col in feature_cols:\n&quot;,&#10;    &quot;                if col in column_modes:\n&quot;,&#10;    &quot;                    mode_val = column_modes[col]\n&quot;,&#10;    &quot;                    \n&quot;,&#10;    &quot;                    # Đếm inf và nan\n&quot;,&#10;    &quot;                    inf_mask = np.isinf(chunk[col])\n&quot;,&#10;    &quot;                    nan_mask = chunk[col].isna()\n&quot;,&#10;    &quot;                    stats['inf_replaced'] += inf_mask.sum()\n&quot;,&#10;    &quot;                    stats['nan_replaced'] += nan_mask.sum()\n&quot;,&#10;    &quot;                    \n&quot;,&#10;    &quot;                    # Thay thế\n&quot;,&#10;    &quot;                    chunk[col] = chunk[col].replace([np.inf, -np.inf], np.nan)\n&quot;,&#10;    &quot;                    chunk[col] = chunk[col].fillna(mode_val)\n&quot;,&#10;    &quot;            \n&quot;,&#10;    &quot;            processed_chunks.append(chunk)\n&quot;,&#10;    &quot;            gc.collect()\n&quot;,&#10;    &quot;        \n&quot;,&#10;    &quot;        if processed_chunks:\n&quot;,&#10;    &quot;            df_file = pd.concat(processed_chunks, ignore_index=True)\n&quot;,&#10;    &quot;            all_dataframes.append(df_file)\n&quot;,&#10;    &quot;            print(f\&quot;   ✅ Đã xử lý: {len(df_file):,} mẫu\&quot;)\n&quot;,&#10;    &quot;            del processed_chunks, df_file\n&quot;,&#10;    &quot;            gc.collect()\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Gộp tất cả\n&quot;,&#10;    &quot;    print(\&quot;\\n   Đang gộp dữ liệu...\&quot;)\n&quot;,&#10;    &quot;    df_combined = pd.concat(all_dataframes, ignore_index=True)\n&quot;,&#10;    &quot;    del all_dataframes\n&quot;,&#10;    &quot;    gc.collect()\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;   Tổng số mẫu sau khi gộp: {len(df_combined):,}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Loại bỏ duplicate\n&quot;,&#10;    &quot;    print(\&quot;   Đang loại bỏ duplicate...\&quot;)\n&quot;,&#10;    &quot;    rows_before = len(df_combined)\n&quot;,&#10;    &quot;    df_combined = df_combined.drop_duplicates()\n&quot;,&#10;    &quot;    rows_after = len(df_combined)\n&quot;,&#10;    &quot;    duplicates_removed = rows_before - rows_after\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;   Số mẫu sau khi loại duplicate: {len(df_combined):,}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   Số duplicate đã loại: {duplicates_removed:,}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Thống kê\n&quot;,&#10;    &quot;    benign_count = (df_combined['binary_label'] == 0).sum()\n&quot;,&#10;    &quot;    attack_count = (df_combined['binary_label'] == 1).sum()\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;\\n    PHÂN BỐ NHÃN SAU CLEAN:\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   Benign: {benign_count:,} ({benign_count/len(df_combined)*100:.1f}%)\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   Attack: {attack_count:,} ({attack_count/len(df_combined)*100:.1f}%)\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    return df_combined, stats\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;df_cleaned, clean_stats = clean_all_files(csv_files, zero_variance_cols, column_modes)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Lưu dữ liệu đã clean\n&quot;,&#10;    &quot;print(\&quot;\\n ĐANG LƯU DỮ LIỆU ĐÃ CLEAN...\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Lưu parquet\n&quot;,&#10;    &quot;parquet_path = os.path.join(CLEANED_DATA_DIR, 'cleaned_data.parquet')\n&quot;,&#10;    &quot;df_cleaned.to_parquet(parquet_path, index=False)\n&quot;,&#10;    &quot;print(f\&quot;   ✅ Đã lưu: {parquet_path}\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Lưu feature names\n&quot;,&#10;    &quot;feature_names = [c for c in df_cleaned.columns if c != 'binary_label']\n&quot;,&#10;    &quot;with open(os.path.join(CLEANED_DATA_DIR, 'feature_names.txt'), 'w') as f:\n&quot;,&#10;    &quot;    for name in feature_names:\n&quot;,&#10;    &quot;        f.write(name + '\\n')\n&quot;,&#10;    &quot;print(f\&quot;   ✅ Đã lưu: feature_names.txt ({len(feature_names)} features)\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Lưu column modes\n&quot;,&#10;    &quot;with open(os.path.join(CLEANED_DATA_DIR, 'column_modes.pkl'), 'wb') as f:\n&quot;,&#10;    &quot;    pickle.dump(column_modes, f)\n&quot;,&#10;    &quot;print(f\&quot;   ✅ Đã lưu: column_modes.pkl\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;\\n✅ HOÀN THÀNH BƯỚC 1 - CLEAN DỮ LIỆU\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;---\n&quot;,&#10;    &quot;# ⚖️ BƯỚC 2: CÂN BẰNG VÀ CHUẨN BỊ DỮ LIỆU TRAINING\n&quot;,&#10;    &quot;---&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def balanced_sample(df, target_benign, target_attack):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    Sample dữ liệu với tỷ lệ cân bằng mong muốn\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    print(\&quot;\\n\&quot; + \&quot;=\&quot;*80)\n&quot;,&#10;    &quot;    print(\&quot;⚖️ ĐANG CÂN BẰNG DỮ LIỆU\&quot;)\n&quot;,&#10;    &quot;    print(\&quot;=\&quot;*80)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Tách theo class\n&quot;,&#10;    &quot;    df_benign = df[df['binary_label'] == 0]\n&quot;,&#10;    &quot;    df_attack = df[df['binary_label'] == 1]\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    n_benign = len(df_benign)\n&quot;,&#10;    &quot;    n_attack = len(df_attack)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;\\n   Dữ liệu gốc:\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   - Benign: {n_benign:,}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   - Attack: {n_attack:,}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;\\n   Target mong muốn:\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   - Benign: {target_benign:,} ({BENIGN_RATIO*100:.0f}%)\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   - Attack: {target_attack:,} ({ATTACK_RATIO*100:.0f}%)\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Xác định số lượng thực tế\n&quot;,&#10;    &quot;    actual_attack = min(target_attack, n_attack)\n&quot;,&#10;    &quot;    actual_benign = int(actual_attack * (BENIGN_RATIO / ATTACK_RATIO))\n&quot;,&#10;    &quot;    actual_benign = min(actual_benign, n_benign)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Điều chỉnh nếu cần\n&quot;,&#10;    &quot;    if actual_benign &lt; int(actual_attack * (BENIGN_RATIO / ATTACK_RATIO)):\n&quot;,&#10;    &quot;        actual_attack = int(actual_benign * (ATTACK_RATIO / BENIGN_RATIO))\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(f\&quot;\\n   Số lượng sẽ lấy:\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   - Benign: {actual_benign:,}\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   - Attack: {actual_attack:,}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Random sample\n&quot;,&#10;    &quot;    df_benign_sampled = df_benign.sample(n=actual_benign, random_state=RANDOM_STATE)\n&quot;,&#10;    &quot;    df_attack_sampled = df_attack.sample(n=actual_attack, random_state=RANDOM_STATE)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Gộp và shuffle\n&quot;,&#10;    &quot;    df_balanced = pd.concat([df_benign_sampled, df_attack_sampled], ignore_index=True)\n&quot;,&#10;    &quot;    df_balanced = df_balanced.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    total = len(df_balanced)\n&quot;,&#10;    &quot;    print(f\&quot;\\n   ✅ Kết quả:\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   - Benign: {actual_benign:,} ({actual_benign/total*100:.1f}%)\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   - Attack: {actual_attack:,} ({actual_attack/total*100:.1f}%)\&quot;)\n&quot;,&#10;    &quot;    print(f\&quot;   - Tổng: {total:,}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    del df_benign, df_attack, df_benign_sampled, df_attack_sampled\n&quot;,&#10;    &quot;    gc.collect()\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    return df_balanced\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;df_balanced = balanced_sample(df_cleaned, TARGET_BENIGN, TARGET_ATTACK)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Giải phóng bộ nhớ\n&quot;,&#10;    &quot;del df_cleaned\n&quot;,&#10;    &quot;gc.collect()&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Tách features và labels\n&quot;,&#10;    &quot;X = df_balanced.drop(columns=['binary_label']).values\n&quot;,&#10;    &quot;y = df_balanced['binary_label'].values\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;del df_balanced\n&quot;,&#10;    &quot;gc.collect()\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;\\n Shape:\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   X: {X.shape}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   y: {y.shape}\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Áp dụng Log Transform: log_e(1+x)\n&quot;,&#10;    &quot;print(\&quot;\\n ĐANG ÁP DỤNG LOG TRANSFORM: log_e(1+x)...\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Đảm bảo không có giá trị âm\n&quot;,&#10;    &quot;min_val = X.min()\n&quot;,&#10;    &quot;if min_val &lt; 0:\n&quot;,&#10;    &quot;    print(f\&quot;   ⚠️ Phát hiện giá trị âm (min={min_val:.4f}), đang shift...\&quot;)\n&quot;,&#10;    &quot;    X = X - min_val\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Áp dụng log(1+x)\n&quot;,&#10;    &quot;X = np.log1p(X)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;   ✅ Log transform hoàn tất\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;      Range: [{X.min():.4f}, {X.max():.4f}]\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Chuẩn hóa bằng StandardScaler\n&quot;,&#10;    &quot;print(\&quot;\\n ĐANG CHUẨN HÓA BẰNG STANDARDSCALER...\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;scaler = StandardScaler()\n&quot;,&#10;    &quot;X = scaler.fit_transform(X)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;   ✅ StandardScaler hoàn tất\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;      Mean: {X.mean():.6f}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;      Std:  {X.std():.6f}\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Reshape cho CNN 1D\n&quot;,&#10;    &quot;print(\&quot;\\n ĐANG RESHAPE CHO CNN 1D...\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;X = X.reshape(X.shape[0], X.shape[1], 1)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;   ✅ Shape: {X.shape}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;      (samples, features, channels)\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Chia train/val/test\n&quot;,&#10;    &quot;print(\&quot;\\n ĐANG CHIA DỮ LIỆU TRAIN/VAL/TEST...\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Chia train+val / test\n&quot;,&#10;    &quot;X_temp, X_test, y_temp, y_test = train_test_split(\n&quot;,&#10;    &quot;    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n&quot;,&#10;    &quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Chia train / val\n&quot;,&#10;    &quot;val_ratio = VAL_SIZE / (1 - TEST_SIZE)\n&quot;,&#10;    &quot;X_train, X_val, y_train, y_val = train_test_split(\n&quot;,&#10;    &quot;    X_temp, y_temp, test_size=val_ratio, random_state=RANDOM_STATE, stratify=y_temp\n&quot;,&#10;    &quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;del X, y, X_temp, y_temp\n&quot;,&#10;    &quot;gc.collect()\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;\\n    KẾT QUẢ CHIA DỮ LIỆU:\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   {'='*60}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   {'Set':&lt;10} {'Samples':&gt;12} {'Benign':&gt;12} {'Attack':&gt;12} {'Attack%':&gt;10}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   {'-'*60}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   {'Train':&lt;10} {len(X_train):&gt;12,} {(y_train==0).sum():&gt;12,} {(y_train==1).sum():&gt;12,} {(y_train==1).sum()/len(y_train)*100:&gt;9.1f}%\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   {'Val':&lt;10} {len(X_val):&gt;12,} {(y_val==0).sum():&gt;12,} {(y_val==1).sum():&gt;12,} {(y_val==1).sum()/len(y_val)*100:&gt;9.1f}%\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   {'Test':&lt;10} {len(X_test):&gt;12,} {(y_test==0).sum():&gt;12,} {(y_test==1).sum():&gt;12,} {(y_test==1).sum()/len(y_test)*100:&gt;9.1f}%\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   {'-'*60}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   {'Total':&lt;10} {len(X_train)+len(X_val)+len(X_test):&gt;12,}\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Tính class weights\n&quot;,&#10;    &quot;print(\&quot;\\n⚖️ ĐANG TÍNH CLASS WEIGHTS...\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;classes = np.unique(y_train)\n&quot;,&#10;    &quot;weights = compute_class_weight('balanced', classes=classes, y=y_train)\n&quot;,&#10;    &quot;class_weights = dict(zip(classes, weights))\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;   Class 0 (Benign): {class_weights[0]:.4f}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Class 1 (Attack): {class_weights[1]:.4f}\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Lưu dữ liệu training\n&quot;,&#10;    &quot;print(\&quot;\\n ĐANG LƯU DỮ LIỆU TRAINING...\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;np.save(os.path.join(TRAINING_DATA_DIR, 'X_train.npy'), X_train)\n&quot;,&#10;    &quot;np.save(os.path.join(TRAINING_DATA_DIR, 'X_val.npy'), X_val)\n&quot;,&#10;    &quot;np.save(os.path.join(TRAINING_DATA_DIR, 'X_test.npy'), X_test)\n&quot;,&#10;    &quot;np.save(os.path.join(TRAINING_DATA_DIR, 'y_train.npy'), y_train)\n&quot;,&#10;    &quot;np.save(os.path.join(TRAINING_DATA_DIR, 'y_val.npy'), y_val)\n&quot;,&#10;    &quot;np.save(os.path.join(TRAINING_DATA_DIR, 'y_test.npy'), y_test)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;with open(os.path.join(TRAINING_DATA_DIR, 'scaler.pkl'), 'wb') as f:\n&quot;,&#10;    &quot;    pickle.dump(scaler, f)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;with open(os.path.join(TRAINING_DATA_DIR, 'class_weights.pkl'), 'wb') as f:\n&quot;,&#10;    &quot;    pickle.dump(class_weights, f)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;   ✅ Đã lưu tất cả dữ liệu training\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;\\n✅ HOÀN THÀNH BƯỚC 2 - CHUẨN BỊ DỮ LIỆU\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;markdown&quot;,&#10;   &quot;metadata&quot;: {},&#10;   &quot;source&quot;: [&#10;    &quot;---\n&quot;,&#10;    &quot;#  BƯỚC 3: XÂY DỰNG VÀ TRAIN MÔ HÌNH CNN\n&quot;,&#10;    &quot;---&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;def build_cnn_model(input_shape):\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    Xây dựng mô hình CNN cho phân loại binary\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    Kiến trúc:\n&quot;,&#10;    &quot;    - 5 lớp Conv1D với MaxPooling\n&quot;,&#10;    &quot;    - BatchNormalization + Dropout trước Flatten\n&quot;,&#10;    &quot;    - Output: Dense(1, sigmoid)\n&quot;,&#10;    &quot;    \&quot;\&quot;\&quot;\n&quot;,&#10;    &quot;    print(\&quot;\\n\&quot; + \&quot;=\&quot;*80)\n&quot;,&#10;    &quot;    print(\&quot;️ ĐANG XÂY DỰNG MÔ HÌNH CNN\&quot;)\n&quot;,&#10;    &quot;    print(\&quot;=\&quot;*80)\n&quot;,&#10;    &quot;    print(f\&quot;   Input shape: {input_shape}\&quot;)\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    model = Sequential(name='CNN_Binary_Classification')\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Input\n&quot;,&#10;    &quot;    model.add(Input(shape=input_shape))\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Conv Block 1: 32 filters\n&quot;,&#10;    &quot;    model.add(Conv1D(32, kernel_size=2, activation='relu', padding='same', name='conv1d_1'))\n&quot;,&#10;    &quot;    model.add(MaxPooling1D(pool_size=2, name='maxpool_1'))\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Conv Block 2: 32 filters\n&quot;,&#10;    &quot;    model.add(Conv1D(32, kernel_size=2, activation='relu', padding='same', name='conv1d_2'))\n&quot;,&#10;    &quot;    model.add(MaxPooling1D(pool_size=2, name='maxpool_2'))\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Conv Block 3: 64 filters\n&quot;,&#10;    &quot;    model.add(Conv1D(64, kernel_size=2, activation='relu', padding='same', name='conv1d_3'))\n&quot;,&#10;    &quot;    model.add(MaxPooling1D(pool_size=2, name='maxpool_3'))\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Conv Block 4: 64 filters\n&quot;,&#10;    &quot;    model.add(Conv1D(64, kernel_size=2, activation='relu', padding='same', name='conv1d_4'))\n&quot;,&#10;    &quot;    model.add(MaxPooling1D(pool_size=2, name='maxpool_4'))\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Conv Block 5: 64 filters\n&quot;,&#10;    &quot;    model.add(Conv1D(64, kernel_size=2, activation='relu', padding='same', name='conv1d_5'))\n&quot;,&#10;    &quot;    model.add(MaxPooling1D(pool_size=2, name='maxpool_5'))\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Regularization\n&quot;,&#10;    &quot;    model.add(BatchNormalization(name='batch_norm'))\n&quot;,&#10;    &quot;    model.add(Dropout(DROPOUT_RATE, name='dropout'))\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Flatten và Output\n&quot;,&#10;    &quot;    model.add(Flatten(name='flatten'))\n&quot;,&#10;    &quot;    model.add(Dense(1, activation='sigmoid', name='output'))\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Compile\n&quot;,&#10;    &quot;    model.compile(\n&quot;,&#10;    &quot;        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n&quot;,&#10;    &quot;        loss='binary_crossentropy',\n&quot;,&#10;    &quot;        metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n&quot;,&#10;    &quot;    )\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    print(\&quot;\\n    KIẾN TRÚC MÔ HÌNH:\&quot;)\n&quot;,&#10;    &quot;    model.summary()\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    return model\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Xây dựng model\n&quot;,&#10;    &quot;input_shape = (X_train.shape[1], X_train.shape[2])\n&quot;,&#10;    &quot;model = build_cnn_model(input_shape)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Tạo callbacks\n&quot;,&#10;    &quot;print(\&quot;\\n ĐANG CẤU HÌNH CALLBACKS...\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;callbacks = [\n&quot;,&#10;    &quot;    # Early Stopping\n&quot;,&#10;    &quot;    EarlyStopping(\n&quot;,&#10;    &quot;        monitor='val_loss',\n&quot;,&#10;    &quot;        patience=PATIENCE,\n&quot;,&#10;    &quot;        verbose=1,\n&quot;,&#10;    &quot;        mode='min',\n&quot;,&#10;    &quot;        restore_best_weights=True\n&quot;,&#10;    &quot;    ),\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Model Checkpoint\n&quot;,&#10;    &quot;    ModelCheckpoint(\n&quot;,&#10;    &quot;        filepath=os.path.join(MODEL_DIR, 'best_model.keras'),\n&quot;,&#10;    &quot;        monitor='val_loss',\n&quot;,&#10;    &quot;        verbose=1,\n&quot;,&#10;    &quot;        save_best_only=True,\n&quot;,&#10;    &quot;        mode='min'\n&quot;,&#10;    &quot;    ),\n&quot;,&#10;    &quot;    \n&quot;,&#10;    &quot;    # Reduce LR\n&quot;,&#10;    &quot;    ReduceLROnPlateau(\n&quot;,&#10;    &quot;        monitor='val_loss',\n&quot;,&#10;    &quot;        factor=0.5,\n&quot;,&#10;    &quot;        patience=5,\n&quot;,&#10;    &quot;        min_lr=1e-7,\n&quot;,&#10;    &quot;        verbose=1\n&quot;,&#10;    &quot;    )\n&quot;,&#10;    &quot;]\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;   ✅ EarlyStopping: patience=10\&quot;)\n&quot;,&#10;    &quot;print(\&quot;   ✅ ModelCheckpoint: save best model\&quot;)\n&quot;,&#10;    &quot;print(\&quot;   ✅ ReduceLROnPlateau: factor=0.5, patience=5\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Train model\n&quot;,&#10;    &quot;print(\&quot;\\n\&quot; + \&quot;=\&quot;*80)\n&quot;,&#10;    &quot;print(\&quot; BẮT ĐẦU HUẤN LUYỆN MÔ HÌNH\&quot;)\n&quot;,&#10;    &quot;print(\&quot;=\&quot;*80)\n&quot;,&#10;    &quot;print(f\&quot;   Epochs: {EPOCHS}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Batch size: {BATCH_SIZE}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Learning rate: {LEARNING_RATE}\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;start_time = datetime.now()\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;history = model.fit(\n&quot;,&#10;    &quot;    X_train, y_train,\n&quot;,&#10;    &quot;    batch_size=BATCH_SIZE,\n&quot;,&#10;    &quot;    epochs=EPOCHS,\n&quot;,&#10;    &quot;    validation_data=(X_val, y_val),\n&quot;,&#10;    &quot;    class_weight=class_weights,\n&quot;,&#10;    &quot;    callbacks=callbacks,\n&quot;,&#10;    &quot;    verbose=1\n&quot;,&#10;    &quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;end_time = datetime.now()\n&quot;,&#10;    &quot;training_time = (end_time - start_time).total_seconds()\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;\\n   ⏱️ Thời gian training: {training_time/60:.2f} phút\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;    Best val_loss: {min(history.history['val_loss']):.4f}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;    Best val_accuracy: {max(history.history['val_accuracy']):.4f}\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Đánh giá trên test set\n&quot;,&#10;    &quot;print(\&quot;\\n\&quot; + \&quot;=\&quot;*80)\n&quot;,&#10;    &quot;print(\&quot; ĐÁNH GIÁ MÔ HÌNH TRÊN TEST SET\&quot;)\n&quot;,&#10;    &quot;print(\&quot;=\&quot;*80)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;loss, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=1)\n&quot;,&#10;    &quot;f1_score = 2 * (precision * recall) / (precision + recall + 1e-7)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;\\n    KẾT QUẢ:\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   {'='*40}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Loss:      {loss:.4f}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Precision: {precision:.4f}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Recall:    {recall:.4f}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   F1-Score:  {f1_score:.4f}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   {'='*40}\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Confusion Matrix và Classification Report\n&quot;,&#10;    &quot;y_pred_prob = model.predict(X_test, verbose=0)\n&quot;,&#10;    &quot;y_pred = (y_pred_prob &gt; 0.5).astype(int).flatten()\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;\\n CONFUSION MATRIX:\&quot;)\n&quot;,&#10;    &quot;cm = confusion_matrix(y_test, y_pred)\n&quot;,&#10;    &quot;print(f\&quot;                 Predicted\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;                 Benign  Attack\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Actual Benign  {cm[0,0]:&gt;6}  {cm[0,1]:&gt;6}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Actual Attack  {cm[1,0]:&gt;6}  {cm[1,1]:&gt;6}\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;\\n CLASSIFICATION REPORT:\&quot;)\n&quot;,&#10;    &quot;print(classification_report(y_test, y_pred, target_names=['Benign', 'Attack']))&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Vẽ biểu đồ training history\n&quot;,&#10;    &quot;fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Loss\n&quot;,&#10;    &quot;axes[0, 0].plot(history.history['loss'], label='Train Loss')\n&quot;,&#10;    &quot;axes[0, 0].plot(history.history['val_loss'], label='Val Loss')\n&quot;,&#10;    &quot;axes[0, 0].set_title('Model Loss')\n&quot;,&#10;    &quot;axes[0, 0].set_xlabel('Epoch')\n&quot;,&#10;    &quot;axes[0, 0].set_ylabel('Loss')\n&quot;,&#10;    &quot;axes[0, 0].legend()\n&quot;,&#10;    &quot;axes[0, 0].grid(True)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Accuracy\n&quot;,&#10;    &quot;axes[0, 1].plot(history.history['accuracy'], label='Train Accuracy')\n&quot;,&#10;    &quot;axes[0, 1].plot(history.history['val_accuracy'], label='Val Accuracy')\n&quot;,&#10;    &quot;axes[0, 1].set_title('Model Accuracy')\n&quot;,&#10;    &quot;axes[0, 1].set_xlabel('Epoch')\n&quot;,&#10;    &quot;axes[0, 1].set_ylabel('Accuracy')\n&quot;,&#10;    &quot;axes[0, 1].legend()\n&quot;,&#10;    &quot;axes[0, 1].grid(True)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Precision\n&quot;,&#10;    &quot;axes[1, 0].plot(history.history['precision'], label='Train Precision')\n&quot;,&#10;    &quot;axes[1, 0].plot(history.history['val_precision'], label='Val Precision')\n&quot;,&#10;    &quot;axes[1, 0].set_title('Model Precision')\n&quot;,&#10;    &quot;axes[1, 0].set_xlabel('Epoch')\n&quot;,&#10;    &quot;axes[1, 0].set_ylabel('Precision')\n&quot;,&#10;    &quot;axes[1, 0].legend()\n&quot;,&#10;    &quot;axes[1, 0].grid(True)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Recall\n&quot;,&#10;    &quot;axes[1, 1].plot(history.history['recall'], label='Train Recall')\n&quot;,&#10;    &quot;axes[1, 1].plot(history.history['val_recall'], label='Val Recall')\n&quot;,&#10;    &quot;axes[1, 1].set_title('Model Recall')\n&quot;,&#10;    &quot;axes[1, 1].set_xlabel('Epoch')\n&quot;,&#10;    &quot;axes[1, 1].set_ylabel('Recall')\n&quot;,&#10;    &quot;axes[1, 1].legend()\n&quot;,&#10;    &quot;axes[1, 1].grid(True)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;plt.tight_layout()\n&quot;,&#10;    &quot;plt.savefig(os.path.join(MODEL_DIR, 'training_history.png'), dpi=150)\n&quot;,&#10;    &quot;plt.show()\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(\&quot;   ✅ Đã lưu training_history.png\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Lưu model và kết quả\n&quot;,&#10;    &quot;print(\&quot;\\n ĐANG LƯU MODEL VÀ KẾT QUẢ...\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Lưu model\n&quot;,&#10;    &quot;model.save(os.path.join(MODEL_DIR, 'final_model.keras'))\n&quot;,&#10;    &quot;print(\&quot;   ✅ final_model.keras\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Lưu weights\n&quot;,&#10;    &quot;model.save_weights(os.path.join(MODEL_DIR, 'model_weights.h5'))\n&quot;,&#10;    &quot;print(\&quot;   ✅ model_weights.h5\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Lưu history\n&quot;,&#10;    &quot;history_dict = {key: [float(v) for v in values] for key, values in history.history.items()}\n&quot;,&#10;    &quot;with open(os.path.join(MODEL_DIR, 'training_history.json'), 'w') as f:\n&quot;,&#10;    &quot;    json.dump(history_dict, f, indent=4)\n&quot;,&#10;    &quot;print(\&quot;   ✅ training_history.json\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;# Lưu kết quả\n&quot;,&#10;    &quot;results = {\n&quot;,&#10;    &quot;    'test_loss': float(loss),\n&quot;,&#10;    &quot;    'test_accuracy': float(accuracy),\n&quot;,&#10;    &quot;    'test_precision': float(precision),\n&quot;,&#10;    &quot;    'test_recall': float(recall),\n&quot;,&#10;    &quot;    'test_f1_score': float(f1_score),\n&quot;,&#10;    &quot;    'training_time_minutes': training_time / 60,\n&quot;,&#10;    &quot;    'epochs_trained': len(history.history['loss']),\n&quot;,&#10;    &quot;    'confusion_matrix': cm.tolist()\n&quot;,&#10;    &quot;}\n&quot;,&#10;    &quot;with open(os.path.join(MODEL_DIR, 'evaluation_results.json'), 'w') as f:\n&quot;,&#10;    &quot;    json.dump(results, f, indent=4)\n&quot;,&#10;    &quot;print(\&quot;   ✅ evaluation_results.json\&quot;)\n&quot;,&#10;    &quot;\n&quot;,&#10;    &quot;print(f\&quot;\\n Tất cả file được lưu tại: {MODEL_DIR}\&quot;)&quot;&#10;   ]&#10;  },&#10;  {&#10;   &quot;cell_type&quot;: &quot;code&quot;,&#10;   &quot;execution_count&quot;: null,&#10;   &quot;metadata&quot;: {},&#10;   &quot;outputs&quot;: [],&#10;   &quot;source&quot;: [&#10;    &quot;# Tóm tắt kết quả\n&quot;,&#10;    &quot;print(\&quot;\\n\&quot; + \&quot;=\&quot;*80)\n&quot;,&#10;    &quot;print(\&quot;✅ HOÀN THÀNH HUẤN LUYỆN MÔ HÌNH CNN!\&quot;)\n&quot;,&#10;    &quot;print(\&quot;=\&quot;*80)\n&quot;,&#10;    &quot;print(f\&quot;\\n    KẾT QUẢ CUỐI CÙNG:\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   {'='*40}\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Test Accuracy:  {accuracy*100:.2f}%\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Test Precision: {precision*100:.2f}%\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Test Recall:    {recall*100:.2f}%\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Test F1-Score:  {f1_score*100:.2f}%\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   Training Time:  {training_time/60:.2f} phút\&quot;)\n&quot;,&#10;    &quot;print(f\&quot;   {'='*40}\&quot;)\n&quot;,&#10;    &quot;print(\&quot;\\n   Mô hình đã được lưu và sẵn sàng sử dụng!\&quot;)&quot;&#10;   ]&#10;  }&#10; ],&#10; &quot;metadata&quot;: {&#10;  &quot;kernelspec&quot;: {&#10;   &quot;display_name&quot;: &quot;Python 3&quot;,&#10;   &quot;language&quot;: &quot;python&quot;,&#10;   &quot;name&quot;: &quot;python3&quot;&#10;  },&#10;  &quot;language_info&quot;: {&#10;   &quot;codemirror_mode&quot;: {&#10;    &quot;name&quot;: &quot;ipython&quot;,&#10;    &quot;version&quot;: 3&#10;   },&#10;   &quot;file_extension&quot;: &quot;.py&quot;,&#10;   &quot;mimetype&quot;: &quot;text/x-python&quot;,&#10;   &quot;name&quot;: &quot;python&quot;,&#10;   &quot;nbconvert_exporter&quot;: &quot;python&quot;,&#10;   &quot;pygments_lexer&quot;: &quot;ipython3&quot;,&#10;   &quot;version&quot;: &quot;3.10.0&quot;&#10;  }&#10; },&#10; &quot;nbformat&quot;: 4,&#10; &quot;nbformat_minor&quot;: 4&#10;}&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/CNN/prepare_folds.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/CNN/prepare_folds.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;=============================================================================&#10;CHIA DỮ LIỆU THEO K-FOLD CHO MÔ HÌNH CNN&#10;Phát hiện lưu lượng mạng IoT bất thường (Binary Classification)&#10;=============================================================================&#10;Mô tả:&#10;    - Đọc dữ liệu đã được tiền xử lý&#10;    - Chia dữ liệu theo K-Fold Cross Validation&#10;    - Hỗ trợ Stratified K-Fold để giữ tỷ lệ class&#10;    - Lưu các fold để sử dụng khi training&#10;=============================================================================&#10;&quot;&quot;&quot;&#10;&#10;import os&#10;import numpy as np&#10;import pickle&#10;from sklearn.model_selection import StratifiedKFold, train_test_split&#10;from tqdm import tqdm&#10;import warnings&#10;&#10;warnings.filterwarnings('ignore')&#10;&#10;&#10;# =============================================================================&#10;# CẤU HÌNH ĐƯỜNG DẪN&#10;# =============================================================================&#10;&#10;def get_paths():&#10;    &quot;&quot;&quot;&#10;    Tự động xác định đường dẫn dựa trên môi trường (Kaggle/Local)&#10;    &quot;&quot;&quot;&#10;    # Kiểm tra nếu đang chạy trên Kaggle&#10;    if os.path.exists('/kaggle/input'):&#10;        INPUT_DIR = '/kaggle/working/processed_cnn'&#10;        OUTPUT_DIR = '/kaggle/working/folds'&#10;        print(&quot; Đang chạy trên KAGGLE&quot;)&#10;    else:&#10;        # Đường dẫn Local&#10;        BASE_DIR = os.path.dirname(os.path.abspath(__file__))&#10;        INPUT_DIR = os.path.join(BASE_DIR, 'processed_data')&#10;        OUTPUT_DIR = os.path.join(BASE_DIR, 'folds')&#10;        print(&quot; Đang chạy trên LOCAL&quot;)&#10;    &#10;    # Tạo thư mục output nếu chưa tồn tại&#10;    os.makedirs(OUTPUT_DIR, exist_ok=True)&#10;    &#10;    print(f&quot; Thư mục input: {INPUT_DIR}&quot;)&#10;    print(f&quot; Thư mục output: {OUTPUT_DIR}&quot;)&#10;    &#10;    return INPUT_DIR, OUTPUT_DIR&#10;&#10;&#10;# =============================================================================&#10;# HÀM CHIA DỮ LIỆU&#10;# =============================================================================&#10;&#10;def load_processed_data(input_dir):&#10;    &quot;&quot;&quot;&#10;    Đọc dữ liệu đã được tiền xử lý&#10;    &quot;&quot;&quot;&#10;    print(&quot;\n Đang đọc dữ liệu đã xử lý...&quot;)&#10;    &#10;    # Đọc features và labels&#10;    X = np.load(os.path.join(input_dir, 'X_processed.npy'))&#10;    y = np.load(os.path.join(input_dir, 'y_binary.npy'))&#10;    &#10;    # Đọc metadata&#10;    with open(os.path.join(input_dir, 'metadata.pkl'), 'rb') as f:&#10;        metadata = pickle.load(f)&#10;    &#10;    print(f&quot;✅ Đã đọc dữ liệu:&quot;)&#10;    print(f&quot;   - X shape: {X.shape}&quot;)&#10;    print(f&quot;   - y shape: {y.shape}&quot;)&#10;    print(f&quot;   - Số mẫu Benign (0): {np.sum(y == 0):,}&quot;)&#10;    print(f&quot;   - Số mẫu Attack (1): {np.sum(y == 1):,}&quot;)&#10;    &#10;    return X, y, metadata&#10;&#10;&#10;def reshape_for_cnn(X, method='1d'):&#10;    &quot;&quot;&quot;&#10;    Reshape dữ liệu cho CNN&#10;    &#10;    Parameters:&#10;    -----------&#10;    X : numpy array&#10;        Dữ liệu features (n_samples, n_features)&#10;    method : str&#10;        '1d' - Reshape thành (samples, features, 1) cho Conv1D&#10;        '2d' - Reshape thành hình vuông cho Conv2D&#10;    &#10;    Returns:&#10;    --------&#10;    X_reshaped : numpy array&#10;        Dữ liệu đã reshape&#10;    &quot;&quot;&quot;&#10;    n_samples, n_features = X.shape&#10;    &#10;    if method == '1d':&#10;        # Reshape cho Conv1D: (samples, features, 1)&#10;        X_reshaped = X.reshape(n_samples, n_features, 1)&#10;        &#10;    elif method == '2d':&#10;        # Tìm kích thước hình vuông gần nhất&#10;        sqrt_features = int(np.ceil(np.sqrt(n_features)))&#10;        padded_features = sqrt_features ** 2&#10;        &#10;        # Padding nếu cần&#10;        if padded_features &gt; n_features:&#10;            padding = np.zeros((n_samples, padded_features - n_features))&#10;            X_padded = np.hstack([X, padding])&#10;        else:&#10;            X_padded = X&#10;        &#10;        # Reshape cho Conv2D: (samples, height, width, 1)&#10;        X_reshaped = X_padded.reshape(n_samples, sqrt_features, sqrt_features, 1)&#10;    &#10;    else:&#10;        X_reshaped = X&#10;    &#10;    return X_reshaped&#10;&#10;&#10;def create_stratified_kfold(X, y, n_folds=5, shuffle=True, random_state=42):&#10;    &quot;&quot;&quot;&#10;    Tạo Stratified K-Fold để chia dữ liệu&#10;    Stratified giữ nguyên tỷ lệ class trong mỗi fold&#10;    &#10;    Parameters:&#10;    -----------&#10;    X : numpy array&#10;        Features&#10;    y : numpy array&#10;        Labels&#10;    n_folds : int&#10;        Số fold (mặc định 5)&#10;    shuffle : bool&#10;        Có shuffle dữ liệu không&#10;    random_state : int&#10;        Random seed&#10;    &#10;    Returns:&#10;    --------&#10;    folds : list of tuples&#10;        Mỗi tuple chứa (train_indices, val_indices)&#10;    &quot;&quot;&quot;&#10;    &#10;    print(f&quot;\n Đang tạo {n_folds}-Fold Stratified Cross Validation...&quot;)&#10;    &#10;    skf = StratifiedKFold(n_splits=n_folds, shuffle=shuffle, random_state=random_state)&#10;    &#10;    folds = []&#10;    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y)):&#10;        folds.append((train_idx, val_idx))&#10;        &#10;        # Thống kê mỗi fold&#10;        y_train_fold = y[train_idx]&#10;        y_val_fold = y[val_idx]&#10;        &#10;        print(f&quot;\n Fold {fold_idx + 1}:&quot;)&#10;        print(f&quot;   Train: {len(train_idx):,} mẫu | &quot;&#10;              f&quot;Benign: {np.sum(y_train_fold == 0):,} | &quot;&#10;              f&quot;Attack: {np.sum(y_train_fold == 1):,} | &quot;&#10;              f&quot;Ratio: {np.mean(y_train_fold)*100:.2f}%&quot;)&#10;        print(f&quot;   Val:   {len(val_idx):,} mẫu | &quot;&#10;              f&quot;Benign: {np.sum(y_val_fold == 0):,} | &quot;&#10;              f&quot;Attack: {np.sum(y_val_fold == 1):,} | &quot;&#10;              f&quot;Ratio: {np.mean(y_val_fold)*100:.2f}%&quot;)&#10;    &#10;    return folds&#10;&#10;&#10;def create_train_val_test_split(X, y, val_size=0.15, test_size=0.15, random_state=42):&#10;    &quot;&quot;&quot;&#10;    Chia dữ liệu thành Train/Validation/Test&#10;    Sử dụng Stratified split để giữ tỷ lệ class&#10;    &#10;    Parameters:&#10;    -----------&#10;    X : numpy array&#10;        Features&#10;    y : numpy array&#10;        Labels&#10;    val_size : float&#10;        Tỷ lệ validation set&#10;    test_size : float&#10;        Tỷ lệ test set&#10;    random_state : int&#10;        Random seed&#10;    &#10;    Returns:&#10;    --------&#10;    splits : dict&#10;        Dictionary chứa các split&#10;    &quot;&quot;&quot;&#10;    &#10;    print(f&quot;\n Đang chia dữ liệu Train/Val/Test...&quot;)&#10;    print(f&quot;   Tỷ lệ: Train={1-val_size-test_size:.0%} | Val={val_size:.0%} | Test={test_size:.0%}&quot;)&#10;    &#10;    # Chia train+val và test&#10;    X_trainval, X_test, y_trainval, y_test = train_test_split(&#10;        X, y, test_size=test_size, stratify=y, random_state=random_state&#10;    )&#10;    &#10;    # Chia train và val&#10;    val_ratio = val_size / (1 - test_size)&#10;    X_train, X_val, y_train, y_val = train_test_split(&#10;        X_trainval, y_trainval, test_size=val_ratio, stratify=y_trainval, &#10;        random_state=random_state&#10;    )&#10;    &#10;    splits = {&#10;        'X_train': X_train,&#10;        'y_train': y_train,&#10;        'X_val': X_val,&#10;        'y_val': y_val,&#10;        'X_test': X_test,&#10;        'y_test': y_test&#10;    }&#10;    &#10;    # Thống kê&#10;    print(f&quot;\n Kết quả chia dữ liệu:&quot;)&#10;    print(f&quot;   Train: {len(X_train):,} mẫu | &quot;&#10;          f&quot;Benign: {np.sum(y_train == 0):,} | &quot;&#10;          f&quot;Attack: {np.sum(y_train == 1):,}&quot;)&#10;    print(f&quot;   Val:   {len(X_val):,} mẫu | &quot;&#10;          f&quot;Benign: {np.sum(y_val == 0):,} | &quot;&#10;          f&quot;Attack: {np.sum(y_val == 1):,}&quot;)&#10;    print(f&quot;   Test:  {len(X_test):,} mẫu | &quot;&#10;          f&quot;Benign: {np.sum(y_test == 0):,} | &quot;&#10;          f&quot;Attack: {np.sum(y_test == 1):,}&quot;)&#10;    &#10;    return splits&#10;&#10;&#10;def save_folds(folds, X, y, output_dir, reshape_method='1d'):&#10;    &quot;&quot;&quot;&#10;    Lưu các fold đã chia&#10;    &#10;    Parameters:&#10;    -----------&#10;    folds : list of tuples&#10;        Danh sách các fold (train_idx, val_idx)&#10;    X : numpy array&#10;        Features&#10;    y : numpy array&#10;        Labels&#10;    output_dir : str&#10;        Thư mục lưu&#10;    reshape_method : str&#10;        Phương pháp reshape ('1d' hoặc '2d')&#10;    &quot;&quot;&quot;&#10;    &#10;    print(f&quot;\n Đang lưu {len(folds)} folds...&quot;)&#10;    &#10;    # Reshape dữ liệu cho CNN&#10;    X_reshaped = reshape_for_cnn(X, method=reshape_method)&#10;    print(f&quot; Shape sau reshape: {X_reshaped.shape}&quot;)&#10;    &#10;    for fold_idx, (train_idx, val_idx) in enumerate(tqdm(folds, desc=&quot;Lưu folds&quot;)):&#10;        fold_dir = os.path.join(output_dir, f'fold_{fold_idx + 1}')&#10;        os.makedirs(fold_dir, exist_ok=True)&#10;        &#10;        # Lưu train data&#10;        np.save(os.path.join(fold_dir, 'X_train.npy'), X_reshaped[train_idx])&#10;        np.save(os.path.join(fold_dir, 'y_train.npy'), y[train_idx])&#10;        &#10;        # Lưu validation data&#10;        np.save(os.path.join(fold_dir, 'X_val.npy'), X_reshaped[val_idx])&#10;        np.save(os.path.join(fold_dir, 'y_val.npy'), y[val_idx])&#10;    &#10;    # Lưu thông tin về folds&#10;    fold_info = {&#10;        'n_folds': len(folds),&#10;        'n_samples': len(y),&#10;        'n_features': X.shape[1],&#10;        'input_shape': X_reshaped.shape[1:],&#10;        'reshape_method': reshape_method,&#10;        'folds': [(list(train_idx), list(val_idx)) for train_idx, val_idx in folds]&#10;    }&#10;    &#10;    with open(os.path.join(output_dir, 'fold_info.pkl'), 'wb') as f:&#10;        pickle.dump(fold_info, f)&#10;    &#10;    print(f&quot;\n✅ Đã lưu folds vào: {output_dir}&quot;)&#10;&#10;&#10;def save_train_val_test(splits, output_dir, reshape_method='1d'):&#10;    &quot;&quot;&quot;&#10;    Lưu Train/Val/Test splits&#10;    &#10;    Parameters:&#10;    -----------&#10;    splits : dict&#10;        Dictionary chứa các split&#10;    output_dir : str&#10;        Thư mục lưu&#10;    reshape_method : str&#10;        Phương pháp reshape&#10;    &quot;&quot;&quot;&#10;    &#10;    print(f&quot;\n Đang lưu Train/Val/Test splits...&quot;)&#10;    &#10;    split_dir = os.path.join(output_dir, 'train_val_test')&#10;    os.makedirs(split_dir, exist_ok=True)&#10;    &#10;    for key, data in splits.items():&#10;        if key.startswith('X_'):&#10;            # Reshape cho CNN&#10;            data_reshaped = reshape_for_cnn(data, method=reshape_method)&#10;            np.save(os.path.join(split_dir, f'{key}.npy'), data_reshaped)&#10;            print(f&quot;   Saved {key}: {data_reshaped.shape}&quot;)&#10;        else:&#10;            np.save(os.path.join(split_dir, f'{key}.npy'), data)&#10;            print(f&quot;   Saved {key}: {data.shape}&quot;)&#10;    &#10;    # Lưu thông tin về splits&#10;    split_info = {&#10;        'n_train': len(splits['X_train']),&#10;        'n_val': len(splits['X_val']),&#10;        'n_test': len(splits['X_test']),&#10;        'n_features': splits['X_train'].shape[1],&#10;        'input_shape': reshape_for_cnn(splits['X_train'], method=reshape_method).shape[1:],&#10;        'reshape_method': reshape_method&#10;    }&#10;    &#10;    with open(os.path.join(split_dir, 'split_info.pkl'), 'wb') as f:&#10;        pickle.dump(split_info, f)&#10;    &#10;    print(f&quot;\n✅ Đã lưu splits vào: {split_dir}&quot;)&#10;&#10;&#10;# =============================================================================&#10;# HÀM CHÍNH&#10;# =============================================================================&#10;&#10;def prepare_folds(n_folds=5, reshape_method='1d', create_holdout=True, &#10;                  val_size=0.15, test_size=0.15):&#10;    &quot;&quot;&quot;&#10;    Hàm chính để chuẩn bị folds cho training&#10;    &#10;    Parameters:&#10;    -----------&#10;    n_folds : int&#10;        Số fold cho K-Fold CV&#10;    reshape_method : str&#10;        '1d' cho Conv1D, '2d' cho Conv2D&#10;    create_holdout : bool&#10;        Có tạo thêm Train/Val/Test split không&#10;    val_size : float&#10;        Tỷ lệ validation (cho holdout)&#10;    test_size : float&#10;        Tỷ lệ test (cho holdout)&#10;    &quot;&quot;&quot;&#10;    &#10;    print(&quot;=&quot;*70)&#10;    print(&quot; BẮT ĐẦU CHIA FOLDS CHO TRAINING&quot;)&#10;    print(&quot;=&quot;*70)&#10;    &#10;    # Lấy đường dẫn&#10;    INPUT_DIR, OUTPUT_DIR = get_paths()&#10;    &#10;    # Đọc dữ liệu&#10;    X, y, metadata = load_processed_data(INPUT_DIR)&#10;    &#10;    # Tạo K-Fold&#10;    folds = create_stratified_kfold(X, y, n_folds=n_folds)&#10;    &#10;    # Lưu folds&#10;    save_folds(folds, X, y, OUTPUT_DIR, reshape_method=reshape_method)&#10;    &#10;    # Tạo thêm Train/Val/Test split (không dùng K-Fold)&#10;    if create_holdout:&#10;        print(&quot;\n&quot; + &quot;=&quot;*70)&#10;        print(&quot; TẠO THÊM TRAIN/VAL/TEST SPLIT&quot;)&#10;        print(&quot;=&quot;*70)&#10;        &#10;        splits = create_train_val_test_split(X, y, val_size=val_size, test_size=test_size)&#10;        save_train_val_test(splits, OUTPUT_DIR, reshape_method=reshape_method)&#10;    &#10;    print(&quot;\n&quot; + &quot;=&quot;*70)&#10;    print(&quot; HOÀN THÀNH CHIA FOLDS!&quot;)&#10;    print(&quot;=&quot;*70)&#10;    &#10;    # Tóm tắt&#10;    print(f&quot;\n TÓM TẮT:&quot;)&#10;    print(f&quot;   - Số folds: {n_folds}&quot;)&#10;    print(f&quot;   - Reshape method: {reshape_method}&quot;)&#10;    print(f&quot;   - Input shape cho CNN: {reshape_for_cnn(X, method=reshape_method).shape[1:]}&quot;)&#10;    print(f&quot;   - Thư mục output: {OUTPUT_DIR}&quot;)&#10;    &#10;    return folds&#10;&#10;&#10;# =============================================================================&#10;# HÀM TIỆN ÍCH - LOAD FOLD&#10;# =============================================================================&#10;&#10;def load_fold(fold_idx, folds_dir=None):&#10;    &quot;&quot;&quot;&#10;    Load dữ liệu của một fold cụ thể&#10;    &#10;    Parameters:&#10;    -----------&#10;    fold_idx : int&#10;        Index của fold (1-based)&#10;    folds_dir : str&#10;        Thư mục chứa folds (None để tự động detect)&#10;    &#10;    Returns:&#10;    --------&#10;    X_train, y_train, X_val, y_val&#10;    &quot;&quot;&quot;&#10;    &#10;    if folds_dir is None:&#10;        _, folds_dir = get_paths()&#10;    &#10;    fold_dir = os.path.join(folds_dir, f'fold_{fold_idx}')&#10;    &#10;    X_train = np.load(os.path.join(fold_dir, 'X_train.npy'))&#10;    y_train = np.load(os.path.join(fold_dir, 'y_train.npy'))&#10;    X_val = np.load(os.path.join(fold_dir, 'X_val.npy'))&#10;    y_val = np.load(os.path.join(fold_dir, 'y_val.npy'))&#10;    &#10;    print(f&quot; Đã load Fold {fold_idx}:&quot;)&#10;    print(f&quot;   X_train: {X_train.shape}, y_train: {y_train.shape}&quot;)&#10;    print(f&quot;   X_val: {X_val.shape}, y_val: {y_val.shape}&quot;)&#10;    &#10;    return X_train, y_train, X_val, y_val&#10;&#10;&#10;def load_train_val_test(folds_dir=None):&#10;    &quot;&quot;&quot;&#10;    Load Train/Val/Test split&#10;    &#10;    Parameters:&#10;    -----------&#10;    folds_dir : str&#10;        Thư mục chứa folds (None để tự động detect)&#10;    &#10;    Returns:&#10;    --------&#10;    X_train, y_train, X_val, y_val, X_test, y_test&#10;    &quot;&quot;&quot;&#10;    &#10;    if folds_dir is None:&#10;        _, folds_dir = get_paths()&#10;    &#10;    split_dir = os.path.join(folds_dir, 'train_val_test')&#10;    &#10;    X_train = np.load(os.path.join(split_dir, 'X_train.npy'))&#10;    y_train = np.load(os.path.join(split_dir, 'y_train.npy'))&#10;    X_val = np.load(os.path.join(split_dir, 'X_val.npy'))&#10;    y_val = np.load(os.path.join(split_dir, 'y_val.npy'))&#10;    X_test = np.load(os.path.join(split_dir, 'X_test.npy'))&#10;    y_test = np.load(os.path.join(split_dir, 'y_test.npy'))&#10;    &#10;    print(f&quot; Đã load Train/Val/Test split:&quot;)&#10;    print(f&quot;   X_train: {X_train.shape}, y_train: {y_train.shape}&quot;)&#10;    print(f&quot;   X_val: {X_val.shape}, y_val: {y_val.shape}&quot;)&#10;    print(f&quot;   X_test: {X_test.shape}, y_test: {y_test.shape}&quot;)&#10;    &#10;    return X_train, y_train, X_val, y_val, X_test, y_test&#10;&#10;&#10;# =============================================================================&#10;# CHẠY CHƯƠNG TRÌNH&#10;# =============================================================================&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    # Chuẩn bị folds&#10;    folds = prepare_folds(&#10;        n_folds=5,              # Số fold&#10;        reshape_method='1d',    # '1d' cho Conv1D, '2d' cho Conv2D&#10;        create_holdout=True,    # Tạo thêm Train/Val/Test split&#10;        val_size=0.15,          # Tỷ lệ validation&#10;        test_size=0.15          # Tỷ lệ test&#10;    )&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/CNN/preprocess_cicids2018.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/CNN/preprocess_cicids2018.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;=============================================================================&#10;TIỀN XỬ LÝ DATASET CICIDS2018 CHO MÔ HÌNH CNN&#10;Phát hiện lưu lượng mạng IoT bất thường (Binary Classification)&#10;=============================================================================&#10;Tác giả: Auto-generated&#10;Mô tả: &#10;    - Đọc và xử lý từng file CSV trong dataset CICIDS2018&#10;    - Chuyển đổi về binary class (Benign vs Attack)&#10;    - Loại bỏ duplicate, xử lý NaN, Inf&#10;    - Chuẩn hóa dữ liệu và lưu dưới dạng tối ưu (numpy/pickle)&#10;    - Sử dụng chunk để tối ưu bộ nhớ&#10;=============================================================================&#10;&quot;&quot;&quot;&#10;&#10;import os&#10;import numpy as np&#10;import pandas as pd&#10;import pickle&#10;from sklearn.preprocessing import StandardScaler, RobustScaler&#10;from tqdm import tqdm&#10;import warnings&#10;import gc&#10;&#10;warnings.filterwarnings('ignore')&#10;&#10;# =============================================================================&#10;# CẤU HÌNH ĐƯỜNG DẪN - Tự động detect Kaggle hoặc Local&#10;# =============================================================================&#10;def get_paths():&#10;    &quot;&quot;&quot;&#10;    Tự động xác định đường dẫn dựa trên môi trường (Kaggle/Local)&#10;    &quot;&quot;&quot;&#10;    # Kiểm tra nếu đang chạy trên Kaggle&#10;    if os.path.exists('/kaggle/input'):&#10;        # Đường dẫn Kaggle - người dùng cần upload dataset&#10;        DATA_DIR = '/kaggle/input/cicids2018-csv'  # Thay đổi theo tên dataset trên Kaggle&#10;        OUTPUT_DIR = '/kaggle/working/processed_cnn'&#10;        print(&quot; Đang chạy trên KAGGLE&quot;)&#10;    else:&#10;        # Đường dẫn Local&#10;        BASE_DIR = os.path.dirname(os.path.abspath(__file__))&#10;        DATA_DIR = os.path.join(os.path.dirname(BASE_DIR), 'CICIDS2018-CSV')&#10;        OUTPUT_DIR = os.path.join(BASE_DIR, 'processed_data')&#10;        print(&quot; Đang chạy trên LOCAL&quot;)&#10;    &#10;    # Tạo thư mục output nếu chưa tồn tại&#10;    os.makedirs(OUTPUT_DIR, exist_ok=True)&#10;    &#10;    print(f&quot; Thư mục dữ liệu: {DATA_DIR}&quot;)&#10;    print(f&quot; Thư mục output: {OUTPUT_DIR}&quot;)&#10;    &#10;    return DATA_DIR, OUTPUT_DIR&#10;&#10;&#10;# =============================================================================&#10;# DANH SÁCH CÁC FEATURE CẦN LOẠI BỎ&#10;# =============================================================================&#10;# Các feature không cần thiết cho CNN:&#10;# - Timestamp: Thông tin thời gian không liên quan đến pattern traffic&#10;# - Các cột có giá trị hằng số (variance = 0)&#10;# - Các cột trùng lặp thông tin&#10;&#10;COLUMNS_TO_DROP = [&#10;    'Timestamp',           # Thời gian - không cần thiết&#10;    'Fwd Byts/b Avg',      # Thường = 0&#10;    'Fwd Pkts/b Avg',      # Thường = 0&#10;    'Fwd Blk Rate Avg',    # Thường = 0&#10;    'Bwd Byts/b Avg',      # Thường = 0&#10;    'Bwd Pkts/b Avg',      # Thường = 0&#10;    'Bwd Blk Rate Avg',    # Thường = 0&#10;]&#10;&#10;# Cột nhãn&#10;LABEL_COLUMN = 'Label'&#10;&#10;&#10;# =============================================================================&#10;# HÀM XỬ LÝ DỮ LIỆU&#10;# =============================================================================&#10;&#10;def get_csv_files(data_dir):&#10;    &quot;&quot;&quot;&#10;    Lấy danh sách tất cả file CSV trong thư mục&#10;    &quot;&quot;&quot;&#10;    csv_files = []&#10;    for file in os.listdir(data_dir):&#10;        if file.endswith('.csv'):&#10;            csv_files.append(os.path.join(data_dir, file))&#10;    &#10;    print(f&quot;\n Tìm thấy {len(csv_files)} file CSV:&quot;)&#10;    for f in csv_files:&#10;        print(f&quot;   - {os.path.basename(f)}&quot;)&#10;    &#10;    return csv_files&#10;&#10;&#10;def clean_column_names(df):&#10;    &quot;&quot;&quot;&#10;    Làm sạch tên cột: loại bỏ khoảng trắng thừa&#10;    &quot;&quot;&quot;&#10;    df.columns = df.columns.str.strip()&#10;    return df&#10;&#10;&#10;def convert_to_binary_label(label):&#10;    &quot;&quot;&quot;&#10;    Chuyển đổi nhãn multi-class sang binary class&#10;    - Benign -&gt; 0&#10;    - Tất cả các loại Attack -&gt; 1&#10;    &quot;&quot;&quot;&#10;    if isinstance(label, str):&#10;        label = label.strip()&#10;        if label.lower() == 'benign':&#10;            return 0&#10;        else:&#10;            return 1&#10;    return 1  # Mặc định là attack nếu không xác định&#10;&#10;&#10;def process_chunk(chunk, columns_to_drop):&#10;    &quot;&quot;&quot;&#10;    Xử lý từng chunk dữ liệu:&#10;    - Làm sạch tên cột&#10;    - Loại bỏ cột không cần thiết&#10;    - Chuyển đổi nhãn sang binary&#10;    - Xử lý NaN, Inf&#10;    - Loại bỏ duplicate&#10;    &quot;&quot;&quot;&#10;    # Làm sạch tên cột&#10;    chunk = clean_column_names(chunk)&#10;    &#10;    # Kiểm tra và lấy cột Label&#10;    if LABEL_COLUMN not in chunk.columns:&#10;        print(f&quot;⚠️ Không tìm thấy cột '{LABEL_COLUMN}'&quot;)&#10;        return None&#10;    &#10;    # Chuyển đổi nhãn sang binary&#10;    chunk['Label_Binary'] = chunk[LABEL_COLUMN].apply(convert_to_binary_label)&#10;    &#10;    # Loại bỏ cột Label gốc và các cột không cần thiết&#10;    cols_to_remove = [col for col in columns_to_drop + [LABEL_COLUMN] &#10;                      if col in chunk.columns]&#10;    chunk = chunk.drop(columns=cols_to_remove, errors='ignore')&#10;    &#10;    # Lấy các cột số (features)&#10;    feature_cols = chunk.select_dtypes(include=[np.number]).columns.tolist()&#10;    feature_cols = [col for col in feature_cols if col != 'Label_Binary']&#10;    &#10;    # Thay thế Inf bằng NaN, sau đó fill NaN bằng median&#10;    chunk[feature_cols] = chunk[feature_cols].replace([np.inf, -np.inf], np.nan)&#10;    &#10;    # Xử lý NaN - điền bằng 0 (hoặc median nếu cần)&#10;    chunk[feature_cols] = chunk[feature_cols].fillna(0)&#10;    &#10;    # Loại bỏ duplicate&#10;    chunk = chunk.drop_duplicates()&#10;    &#10;    return chunk&#10;&#10;&#10;def process_single_file(file_path, columns_to_drop, chunk_size=100000):&#10;    &quot;&quot;&quot;&#10;    Xử lý một file CSV với chunk&#10;    &quot;&quot;&quot;&#10;    file_name = os.path.basename(file_path)&#10;    print(f&quot;\n{'='*60}&quot;)&#10;    print(f&quot; Đang xử lý: {file_name}&quot;)&#10;    print(f&quot;{'='*60}&quot;)&#10;    &#10;    chunks_list = []&#10;    total_rows = 0&#10;    &#10;    try:&#10;        # Đọc file theo chunk&#10;        chunk_iterator = pd.read_csv(file_path, chunksize=chunk_size, &#10;                                      low_memory=False, encoding='utf-8')&#10;        &#10;        for i, chunk in enumerate(tqdm(chunk_iterator, desc=&quot;Đọc chunks&quot;)):&#10;            processed_chunk = process_chunk(chunk, columns_to_drop)&#10;            if processed_chunk is not None:&#10;                chunks_list.append(processed_chunk)&#10;                total_rows += len(processed_chunk)&#10;            &#10;            # Giải phóng bộ nhớ&#10;            del chunk&#10;            gc.collect()&#10;        &#10;        if chunks_list:&#10;            # Ghép các chunk lại&#10;            df = pd.concat(chunks_list, ignore_index=True)&#10;            &#10;            # Loại bỏ duplicate lần cuối sau khi ghép&#10;            before_dedup = len(df)&#10;            df = df.drop_duplicates()&#10;            after_dedup = len(df)&#10;            &#10;            print(f&quot;✅ Tổng số dòng sau xử lý: {after_dedup:,}&quot;)&#10;            print(f&quot;️ Số dòng duplicate đã loại bỏ: {before_dedup - after_dedup:,}&quot;)&#10;            &#10;            # Thống kê nhãn&#10;            label_counts = df['Label_Binary'].value_counts()&#10;            print(f&quot; Phân bố nhãn:&quot;)&#10;            print(f&quot;   - Benign (0): {label_counts.get(0, 0):,}&quot;)&#10;            print(f&quot;   - Attack (1): {label_counts.get(1, 0):,}&quot;)&#10;            &#10;            return df&#10;        else:&#10;            print(f&quot;❌ Không có dữ liệu hợp lệ trong file&quot;)&#10;            return None&#10;            &#10;    except Exception as e:&#10;        print(f&quot;❌ Lỗi khi xử lý file: {str(e)}&quot;)&#10;        return None&#10;&#10;&#10;def remove_constant_columns(df, feature_cols):&#10;    &quot;&quot;&quot;&#10;    Loại bỏ các cột có giá trị hằng số (variance = 0)&#10;    &quot;&quot;&quot;&#10;    constant_cols = []&#10;    for col in feature_cols:&#10;        if df[col].nunique() &lt;= 1:&#10;            constant_cols.append(col)&#10;    &#10;    if constant_cols:&#10;        print(f&quot;\n️ Loại bỏ {len(constant_cols)} cột hằng số:&quot;)&#10;        for col in constant_cols:&#10;            print(f&quot;   - {col}&quot;)&#10;        df = df.drop(columns=constant_cols)&#10;    &#10;    return df, constant_cols&#10;&#10;&#10;def remove_high_correlation_columns(df, feature_cols, threshold=0.95):&#10;    &quot;&quot;&quot;&#10;    Loại bỏ các cột có tương quan cao (&gt; threshold)&#10;    Giữ lại 1 cột trong mỗi cặp tương quan cao&#10;    &quot;&quot;&quot;&#10;    print(f&quot;\n Kiểm tra tương quan giữa các features (threshold={threshold})...&quot;)&#10;    &#10;    # Tính ma trận tương quan&#10;    corr_matrix = df[feature_cols].corr().abs()&#10;    &#10;    # Lấy tam giác trên của ma trận&#10;    upper_tri = corr_matrix.where(&#10;        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)&#10;    )&#10;    &#10;    # Tìm các cột có tương quan cao&#10;    cols_to_drop = [column for column in upper_tri.columns &#10;                    if any(upper_tri[column] &gt; threshold)]&#10;    &#10;    if cols_to_drop:&#10;        print(f&quot;️ Loại bỏ {len(cols_to_drop)} cột tương quan cao:&quot;)&#10;        for col in cols_to_drop[:10]:  # Chỉ hiển thị 10 cột đầu&#10;            print(f&quot;   - {col}&quot;)&#10;        if len(cols_to_drop) &gt; 10:&#10;            print(f&quot;   ... và {len(cols_to_drop) - 10} cột khác&quot;)&#10;        &#10;        df = df.drop(columns=cols_to_drop)&#10;    &#10;    return df, cols_to_drop&#10;&#10;&#10;def normalize_features(X, scaler=None):&#10;    &quot;&quot;&quot;&#10;    Chuẩn hóa features sử dụng RobustScaler&#10;    RobustScaler ít bị ảnh hưởng bởi outliers&#10;    &quot;&quot;&quot;&#10;    if scaler is None:&#10;        scaler = RobustScaler()&#10;        X_normalized = scaler.fit_transform(X)&#10;    else:&#10;        X_normalized = scaler.transform(X)&#10;    &#10;    return X_normalized, scaler&#10;&#10;&#10;def reshape_for_cnn(X, method='1d'):&#10;    &quot;&quot;&quot;&#10;    Reshape dữ liệu cho CNN&#10;    &#10;    method='1d': Reshape thành (samples, features, 1) cho Conv1D&#10;    method='2d': Reshape thành hình vuông cho Conv2D (nếu có thể)&#10;    &quot;&quot;&quot;&#10;    n_samples, n_features = X.shape&#10;    &#10;    if method == '1d':&#10;        # Reshape cho Conv1D: (samples, features, 1)&#10;        X_reshaped = X.reshape(n_samples, n_features, 1)&#10;        print(f&quot; Reshape cho Conv1D: {X.shape} -&gt; {X_reshaped.shape}&quot;)&#10;        &#10;    elif method == '2d':&#10;        # Tìm kích thước hình vuông gần nhất&#10;        sqrt_features = int(np.ceil(np.sqrt(n_features)))&#10;        padded_features = sqrt_features ** 2&#10;        &#10;        # Padding nếu cần&#10;        if padded_features &gt; n_features:&#10;            padding = np.zeros((n_samples, padded_features - n_features))&#10;            X_padded = np.hstack([X, padding])&#10;        else:&#10;            X_padded = X&#10;        &#10;        # Reshape cho Conv2D: (samples, height, width, 1)&#10;        X_reshaped = X_padded.reshape(n_samples, sqrt_features, sqrt_features, 1)&#10;        print(f&quot; Reshape cho Conv2D: {X.shape} -&gt; {X_reshaped.shape}&quot;)&#10;    &#10;    else:&#10;        X_reshaped = X&#10;    &#10;    return X_reshaped&#10;&#10;&#10;# =============================================================================&#10;# HÀM CHÍNH - TIỀN XỬ LÝ TOÀN BỘ DATASET&#10;# =============================================================================&#10;&#10;def preprocess_dataset(chunk_size=100000, remove_high_corr=True, corr_threshold=0.95):&#10;    &quot;&quot;&quot;&#10;    Hàm chính để tiền xử lý toàn bộ dataset CICIDS2018&#10;    &#10;    Parameters:&#10;    -----------&#10;    chunk_size : int&#10;        Kích thước mỗi chunk khi đọc file&#10;    remove_high_corr : bool&#10;        Có loại bỏ các cột tương quan cao không&#10;    corr_threshold : float&#10;        Ngưỡng tương quan để loại bỏ (mặc định 0.95)&#10;    &#10;    Returns:&#10;    --------&#10;    Lưu các file sau vào thư mục output:&#10;        - X_processed.npy: Features đã chuẩn hóa&#10;        - y_binary.npy: Nhãn binary (0: Benign, 1: Attack)&#10;        - scaler.pkl: Scaler để transform dữ liệu mới&#10;        - feature_names.txt: Tên các features&#10;        - metadata.pkl: Thông tin metadata&#10;    &quot;&quot;&quot;&#10;    &#10;    print(&quot;=&quot;*70)&#10;    print(&quot; BẮT ĐẦU TIỀN XỬ LÝ DATASET CICIDS2018&quot;)&#10;    print(&quot;=&quot;*70)&#10;    &#10;    # Lấy đường dẫn&#10;    DATA_DIR, OUTPUT_DIR = get_paths()&#10;    &#10;    # Lấy danh sách file CSV&#10;    csv_files = get_csv_files(DATA_DIR)&#10;    &#10;    if not csv_files:&#10;        print(&quot;❌ Không tìm thấy file CSV nào!&quot;)&#10;        return&#10;    &#10;    # Xử lý từng file&#10;    all_data = []&#10;    &#10;    for file_path in csv_files:&#10;        df = process_single_file(file_path, COLUMNS_TO_DROP, chunk_size)&#10;        if df is not None:&#10;            all_data.append(df)&#10;        &#10;        # Giải phóng bộ nhớ&#10;        gc.collect()&#10;    &#10;    if not all_data:&#10;        print(&quot;❌ Không có dữ liệu để xử lý!&quot;)&#10;        return&#10;    &#10;    # Ghép tất cả dữ liệu&#10;    print(&quot;\n&quot; + &quot;=&quot;*70)&#10;    print(&quot; GHÉP VÀ XỬ LÝ TOÀN BỘ DỮ LIỆU&quot;)&#10;    print(&quot;=&quot;*70)&#10;    &#10;    full_df = pd.concat(all_data, ignore_index=True)&#10;    del all_data&#10;    gc.collect()&#10;    &#10;    print(f&quot; Tổng số dòng trước khi loại duplicate: {len(full_df):,}&quot;)&#10;    &#10;    # Loại bỏ duplicate toàn cục&#10;    full_df = full_df.drop_duplicates()&#10;    print(f&quot; Tổng số dòng sau khi loại duplicate: {len(full_df):,}&quot;)&#10;    &#10;    # Tách features và labels&#10;    y = full_df['Label_Binary'].values&#10;    X_df = full_df.drop(columns=['Label_Binary'])&#10;    feature_cols = X_df.columns.tolist()&#10;    &#10;    del full_df&#10;    gc.collect()&#10;    &#10;    # Loại bỏ cột hằng số&#10;    X_df, constant_cols = remove_constant_columns(X_df, feature_cols)&#10;    feature_cols = X_df.columns.tolist()&#10;    &#10;    # Loại bỏ cột tương quan cao (tùy chọn)&#10;    if remove_high_corr:&#10;        X_df, high_corr_cols = remove_high_correlation_columns(&#10;            X_df, feature_cols, corr_threshold&#10;        )&#10;        feature_cols = X_df.columns.tolist()&#10;    else:&#10;        high_corr_cols = []&#10;    &#10;    print(f&quot;\n Số features cuối cùng: {len(feature_cols)}&quot;)&#10;    &#10;    # Chuyển sang numpy array&#10;    X = X_df.values.astype(np.float32)&#10;    del X_df&#10;    gc.collect()&#10;    &#10;    # Xử lý NaN và Inf còn sót (nếu có)&#10;    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)&#10;    &#10;    # Chuẩn hóa features&#10;    print(&quot;\n Đang chuẩn hóa features...&quot;)&#10;    X_normalized, scaler = normalize_features(X)&#10;    &#10;    # Thống kê cuối cùng&#10;    print(&quot;\n&quot; + &quot;=&quot;*70)&#10;    print(&quot; THỐNG KÊ CUỐI CÙNG&quot;)&#10;    print(&quot;=&quot;*70)&#10;    print(f&quot;Kích thước X: {X_normalized.shape}&quot;)&#10;    print(f&quot;Kích thước y: {y.shape}&quot;)&#10;    print(f&quot;Số features: {len(feature_cols)}&quot;)&#10;    print(f&quot;Số mẫu Benign (0): {np.sum(y == 0):,}&quot;)&#10;    print(f&quot;Số mẫu Attack (1): {np.sum(y == 1):,}&quot;)&#10;    print(f&quot;Tỷ lệ Attack: {np.mean(y) * 100:.2f}%&quot;)&#10;    &#10;    # Lưu dữ liệu&#10;    print(&quot;\n Đang lưu dữ liệu...&quot;)&#10;    &#10;    # Lưu features và labels&#10;    np.save(os.path.join(OUTPUT_DIR, 'X_processed.npy'), X_normalized)&#10;    np.save(os.path.join(OUTPUT_DIR, 'y_binary.npy'), y)&#10;    &#10;    # Lưu scaler&#10;    with open(os.path.join(OUTPUT_DIR, 'scaler.pkl'), 'wb') as f:&#10;        pickle.dump(scaler, f)&#10;    &#10;    # Lưu tên features&#10;    with open(os.path.join(OUTPUT_DIR, 'feature_names.txt'), 'w') as f:&#10;        for name in feature_cols:&#10;            f.write(f&quot;{name}\n&quot;)&#10;    &#10;    # Lưu metadata&#10;    metadata = {&#10;        'n_samples': X_normalized.shape[0],&#10;        'n_features': X_normalized.shape[1],&#10;        'feature_names': feature_cols,&#10;        'columns_dropped': COLUMNS_TO_DROP,&#10;        'constant_columns_removed': constant_cols,&#10;        'high_corr_columns_removed': high_corr_cols,&#10;        'n_benign': int(np.sum(y == 0)),&#10;        'n_attack': int(np.sum(y == 1)),&#10;        'attack_ratio': float(np.mean(y)),&#10;    }&#10;    &#10;    with open(os.path.join(OUTPUT_DIR, 'metadata.pkl'), 'wb') as f:&#10;        pickle.dump(metadata, f)&#10;    &#10;    print(f&quot;\n✅ ĐÃ LƯU DỮ LIỆU VÀO: {OUTPUT_DIR}&quot;)&#10;    print(&quot;   - X_processed.npy&quot;)&#10;    print(&quot;   - y_binary.npy&quot;)&#10;    print(&quot;   - scaler.pkl&quot;)&#10;    print(&quot;   - feature_names.txt&quot;)&#10;    print(&quot;   - metadata.pkl&quot;)&#10;    &#10;    print(&quot;\n&quot; + &quot;=&quot;*70)&#10;    print(&quot; HOÀN THÀNH TIỀN XỬ LÝ!&quot;)&#10;    print(&quot;=&quot;*70)&#10;    &#10;    return X_normalized, y, scaler, feature_cols, metadata&#10;&#10;&#10;# =============================================================================&#10;# CHẠY CHƯƠNG TRÌNH&#10;# =============================================================================&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    # Chạy tiền xử lý&#10;    result = preprocess_dataset(&#10;        chunk_size=100000,      # Kích thước chunk&#10;        remove_high_corr=True,  # Loại bỏ cột tương quan cao&#10;        corr_threshold=0.95     # Ngưỡng tương quan&#10;    )&#10;    &#10;    if result:&#10;        X, y, scaler, feature_names, metadata = result&#10;        print(f&quot;\n Shape của X: {X.shape}&quot;)&#10;        print(f&quot; Shape của y: {y.shape}&quot;)&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/GNN/kaggle/check_model.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/GNN/kaggle/check_model.py" />
              <option name="updatedContent" value="import torch&#10;&#10;checkpoint = torch.load('D:/PROJECT/Machine Learning/IOT/GNN/kaggle/best_model_binary.pt', map_location='cpu')&#10;state_dict = checkpoint['model_state_dict']&#10;&#10;print('Model layers:')&#10;for key in sorted(state_dict.keys()):&#10;    print(f'  {key}: {state_dict[key].shape}')&#10;&#10;print(f&quot;\nBest Val Acc: {checkpoint['best_val_acc']}&quot;)&#10;print(f&quot;Best Epoch: {checkpoint['best_epoch']}&quot;)&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/PROGRESS_BAR_GUIDE.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/PROGRESS_BAR_GUIDE.md" />
              <option name="updatedContent" value="#  Progress Bar - Hướng dẫn sử dụng&#10;&#10;## ✅ ĐÃ THÊM PROGRESS BAR&#10;&#10;Script `preprocess_data.py` đã được cập nhật với **thanh tiến trình (progress bar)** để theo dõi tiến độ xử lý!&#10;&#10;---&#10;&#10;##  Cài đặt (khuyến nghị)&#10;&#10;### Cài tqdm để có progress bar:&#10;&#10;```powershell&#10;pip install tqdm&#10;```&#10;&#10;**Sau khi cài xong, chạy:**&#10;```powershell&#10;python preprocess_data.py&#10;```&#10;&#10;---&#10;&#10;##  Các thanh tiến trình&#10;&#10;### 1. **Loading CSV files**&#10;```&#10; Loading CSV files: 100%|████████| 10/10 [02:30&lt;00:00, 15.0s/file]&#10;```&#10;- Hiển thị số file đã load&#10;- Thời gian còn lại (ETA)&#10;- Tốc độ load (files/second)&#10;&#10;### 2. **Processing inf values**&#10;```&#10; Processing inf values: 100%|████████| 68/68 [00:15&lt;00:00, 4.5col/s]&#10;```&#10;- Số cột đã xử lý&#10;- Thời gian còn lại&#10;&#10;### 3. **Normalizing features**&#10;```&#10; Normalizing: 100%|████████| 2/2 [00:30&lt;00:00, 15.0s/step]&#10;```&#10;- Tiến độ normalization&#10;&#10;### 4. **Saving files**&#10;```&#10; Saving files: 100%|████████| 8/8 [01:00&lt;00:00, 7.5s/file]&#10;```&#10;- Số file đã lưu&#10;- File đang l��u&#10;&#10;---&#10;&#10;##  Minh họa output&#10;&#10;### Với tqdm (CÓ progress bar):&#10;&#10;```&#10;================================================================================&#10;LOADING CICIDS2018 DATA&#10;================================================================================&#10;Tìm thấy 10 file CSV&#10; Loading CSV files:  50%|██████▌       | 5/10 [01:15&lt;01:15, 15.0s/file]&#10;```&#10;&#10;### Không có tqdm (KHÔNG progress bar):&#10;&#10;```&#10;================================================================================&#10;LOADING CICIDS2018 DATA&#10;================================================================================&#10;Tìm thấy 10 file CSV&#10;⚠️  tqdm not installed. Install for progress bars: pip install tqdm&#10;  Đang đọc: Friday-02-03-2018_TrafficForML_CICFlowMeter.csv...&#10;    ✓ 445,909 rows loaded&#10;  Đang đọc: Friday-16-02-2018_TrafficForML_CICFlowMeter.csv...&#10;    ✓ 663,808 rows loaded&#10;  ...&#10;```&#10;&#10;**→ Vẫn chạy được nhưng không có progress bar!**&#10;&#10;---&#10;&#10;##  Lợi ích của Progress Bar&#10;&#10;### 1. **Biết được tiến độ**&#10;- Bao nhiêu % đã hoàn thành&#10;- Còn bao lâu nữa (ETA - Estimated Time of Arrival)&#10;&#10;### 2. **Không lo &quot;script bị treo&quot;**&#10;- Thấy thanh chạy → biết script vẫn hoạt động&#10;- Thấy file/cột đang xử lý&#10;&#10;### 3. **Tối ưu workflow**&#10;- Biết step nào chậm nhất&#10;- Có thể đi làm việc khác khi còn lâu&#10;&#10;---&#10;&#10;##  So sánh&#10;&#10;| Aspect | Không có tqdm | Có tqdm |&#10;|--------|---------------|---------|&#10;| **Progress** | ❌ Không biết % | ✅ Biết rõ % |&#10;| **ETA** | ❌ Không biết còn bao lâu | ✅ Hiển thị thời gian còn lại |&#10;| **Tốc độ** | ❌ Không biết | ✅ Hiển thị items/s |&#10;| **Cài đặt** | ✅ Không cần | ⚠️ Cần: `pip install tqdm` |&#10;| **Hoạt động** | ✅ Vẫn chạy bình thường | ✅ Chạy + có progress bar |&#10;&#10;---&#10;&#10;##  Chi tiết kỹ thuật&#10;&#10;### Auto-detect tqdm:&#10;&#10;```python&#10;try:&#10;    from tqdm import tqdm&#10;    TQDM_AVAILABLE = True&#10;except ImportError:&#10;    TQDM_AVAILABLE = False&#10;    print(&quot;⚠️  tqdm not installed. Install for progress bars: pip install tqdm&quot;)&#10;    tqdm = lambda x, **kwargs: x  # Fallback&#10;```&#10;&#10;**Cách hoạt động:**&#10;- Nếu **CÓ tqdm**: Dùng progress bar&#10;- Nếu **KHÔNG có**: Fallback về in thông thường&#10;&#10;**→ Script luôn chạy được, dù có hay không có tqdm!**&#10;&#10;---&#10;&#10;## ✅ Checklist&#10;&#10;### Để có progress bar đẹp:&#10;- [ ] Cài tqdm: `pip install tqdm`&#10;- [ ] Chạy script: `python preprocess_data.py`&#10;- [ ] Thấy các thanh tiến trình:    &#10;&#10;### Nếu không muốn cài tqdm:&#10;- [ ] Bỏ qua, chạy luôn: `python preprocess_data.py`&#10;- [ ] Thấy text thông thường (không có progress bar)&#10;- [ ] Vẫn hoạt động bình thường!&#10;&#10;---&#10;&#10;##  Demo output (với tqdm)&#10;&#10;```&#10;╔══════════════════════════════════════════════════════════════════════════════╗&#10;║               CICIDS2018 DATA PREPROCESSING PIPELINE                         ║&#10;╚══════════════════════════════════════════════════════════════════════════════╝&#10;&#10;&#10;================================================================================&#10;LOADING CICIDS2018 DATA&#10;================================================================================&#10;Tìm thấy 10 file CSV&#10; Loading CSV files:  30%|███▊           | 3/10 [00:45&lt;01:45, 15.0s/file] {rows: 1,109,717, filtered: 6}&#10;&#10;================================================================================&#10;CLEANING DATA&#10;================================================================================&#10;Dropping inconsistent/unnecessary columns: ['Timestamp']&#10;✓ Remaining columns: 79&#10;✓ Features kept: 78&#10;...&#10; Processing inf values: 100%|████████████| 68/68 [00:15&lt;00:00, 4.5col/s]&#10;&#10;================================================================================&#10;NORMALIZING FEATURES&#10;================================================================================&#10;Normalizing 68 features...&#10; Normalizing: 100%|████████████████████| 2/2 [00:30&lt;00:00, 15.0s/step] {status: completed}&#10;✓ Features normalized&#10;&#10;================================================================================&#10;SAVING PROCESSED DATA&#10;================================================================================&#10; Saving files: 100%|████████████████████| 8/8 [01:00&lt;00:00, 7.5s/file] {file: metadata.pkl}&#10;&#10;✓ All files saved successfully!&#10;  - Feature array: (16232943, 68)&#10;  - Binary labels: (16232943,)&#10;  - Multi-class labels: (16232943,)&#10;&#10;================================================================================&#10;PREPROCESSING COMPLETED!&#10;================================================================================&#10;```&#10;&#10;**→ Rất dễ theo dõi tiến độ!** &#10;&#10;---&#10;&#10;##  Troubleshooting&#10;&#10;### Q: Cài tqdm nhưng vẫn không thấy progress bar?&#10;A: Restart terminal hoặc Python environment&#10;&#10;### Q: Progress bar bị nhấp nháy/lỗi?&#10;A: Đang chạy trong IDE có thể không hỗ trợ tốt. Chạy trong terminal thuần.&#10;&#10;### Q: Có bắt buộc phải cài tqdm không?&#10;A: **KHÔNG!** Script vẫn chạy bình thường, chỉ không có progress bar.&#10;&#10;---&#10;&#10;**Happy preprocessing với progress bar! **&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/extract_labels_no_pandas.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/extract_labels_no_pandas.py" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Script đơn giản để trích xuất các nhãn (Label) từ tệp CSV&#10;Chỉ sử dụng thư viện chuẩn của Python (không cần pandas)&#10;&quot;&quot;&quot;&#10;&#10;import csv&#10;from collections import Counter&#10;import os&#10;&#10;def extract_labels_no_pandas(csv_file_path):&#10;    &quot;&quot;&quot;&#10;    Trích xuất các nhãn từ một tệp CSV cụ thể&#10;    Không sử dụng pandas - chỉ dùng thư viện chuẩn&#10;    &#10;    Args:&#10;        csv_file_path: Đường dẫn đến tệp CSV&#10;    &quot;&quot;&quot;&#10;    print(f&quot;Đang đọc tệp: {csv_file_path}&quot;)&#10;    &#10;    labels = []&#10;    &#10;    # Đọc tệp CSV&#10;    with open(csv_file_path, 'r', encoding='utf-8') as file:&#10;        csv_reader = csv.DictReader(file)&#10;        &#10;        # Kiểm tra xem có cột 'Label' không&#10;        if 'Label' not in csv_reader.fieldnames:&#10;            print(&quot;Lỗi: Không tìm thấy cột 'Label' trong tệp CSV&quot;)&#10;            print(f&quot;Các cột có sẵn: {csv_reader.fieldnames}&quot;)&#10;            return None, None&#10;        &#10;        # Đọc tất cả các nhãn&#10;        for row in csv_reader:&#10;            labels.append(row['Label'])&#10;    &#10;    # Lấy các nhãn duy nhất&#10;    unique_labels = sorted(set(labels))&#10;    &#10;    # Đếm số lượng mỗi nhãn&#10;    label_counts = Counter(labels)&#10;    &#10;    # Hiển thị kết quả&#10;    print(&quot;\n&quot; + &quot;=&quot; * 60)&#10;    print(&quot;CÁC NHÃN TÌM THẤY&quot;)&#10;    print(&quot;=&quot; * 60)&#10;    print(f&quot;\nTổng số nhãn duy nhất: {len(unique_labels)}&quot;)&#10;    print(f&quot;\nDanh sách các nhãn:&quot;)&#10;    for i, label in enumerate(unique_labels, 1):&#10;        print(f&quot;  {i}. {label}&quot;)&#10;    &#10;    print(&quot;\n&quot; + &quot;=&quot; * 60)&#10;    print(&quot;PHÂN PHỐI SỐ LƯỢNG MỖI NHÃN&quot;)&#10;    print(&quot;=&quot; * 60)&#10;    total = len(labels)&#10;    for label, count in label_counts.most_common():&#10;        percentage = (count / total) * 100&#10;        print(f&quot;{label}: {count:,} ({percentage:.2f}%)&quot;)&#10;    &#10;    print(f&quot;\nTổng số dòng dữ liệu: {total:,}&quot;)&#10;    &#10;    return unique_labels, label_counts&#10;&#10;def extract_labels_from_all_files(directory_path):&#10;    &quot;&quot;&quot;&#10;    Trích xuất nhãn từ tất cả các tệp CSV trong thư mục&#10;    &#10;    Args:&#10;        directory_path: Đường dẫn đến thư mục chứa các tệp CSV&#10;    &quot;&quot;&quot;&#10;    all_labels = set()&#10;    &#10;    print(&quot;\n&quot; + &quot;=&quot; * 80)&#10;    print(&quot;TRÍCH XUẤT NHÃN TỪ TẤT CẢ CÁC TẾP CSV&quot;)&#10;    print(&quot;=&quot; * 80)&#10;    &#10;    # Lấy tất cả các tệp CSV&#10;    csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]&#10;    &#10;    print(f&quot;\nTìm thấy {len(csv_files)} tệp CSV\n&quot;)&#10;    &#10;    for csv_file in sorted(csv_files):&#10;        file_path = os.path.join(directory_path, csv_file)&#10;        print(f&quot;\n{'─' * 80}&quot;)&#10;        print(f&quot;Đang xử lý: {csv_file}&quot;)&#10;        print('─' * 80)&#10;        &#10;        try:&#10;            with open(file_path, 'r', encoding='utf-8') as file:&#10;                csv_reader = csv.DictReader(file)&#10;                &#10;                if 'Label' in csv_reader.fieldnames:&#10;                    file_labels = set()&#10;                    for row in csv_reader:&#10;                        file_labels.add(row['Label'])&#10;                    &#10;                    all_labels.update(file_labels)&#10;                    print(f&quot;Các nhãn tìm thấy ({len(file_labels)}): {sorted(file_labels)}&quot;)&#10;                else:&#10;                    print(&quot;Cảnh báo: Không tìm thấy cột 'Label'&quot;)&#10;        &#10;        except Exception as e:&#10;            print(f&quot;Lỗi khi đọc tệp: {e}&quot;)&#10;    &#10;    # Hiển thị tổng kết&#10;    print(&quot;\n&quot; + &quot;=&quot; * 80)&#10;    print(&quot;KẾT QUẢ TỔNG HỢP&quot;)&#10;    print(&quot;=&quot; * 80)&#10;    print(f&quot;\nTổng số nhãn duy nhất tìm thấy: {len(all_labels)}&quot;)&#10;    print(f&quot;\nDanh sách các nhãn:&quot;)&#10;    for i, label in enumerate(sorted(all_labels), 1):&#10;        print(f&quot;  {i}. {label}&quot;)&#10;    &#10;    return all_labels&#10;&#10;# Chương trình chính&#10;if __name__ == &quot;__main__&quot;:&#10;    # Lựa chọn: phân tích một tệp hoặc tất cả các tệp&#10;    print(&quot;Chọn chế độ:&quot;)&#10;    print(&quot;1. Trích xuất nhãn từ một tệp cụ thể&quot;)&#10;    print(&quot;2. Trích xuất nhãn từ tất cả các tệp CSV trong thư mục&quot;)&#10;    &#10;    choice = input(&quot;\nNhập lựa chọn của bạn (1 hoặc 2): &quot;).strip()&#10;    &#10;    if choice == &quot;1&quot;:&#10;        # Phân tích một tệp cụ thể&#10;        csv_file = r&quot;D:\PROJECT\Machine Learning\IOT\CICIDS2018-CSV\Friday-02-03-2018_TrafficForML_CICFlowMeter.csv&quot;&#10;        &#10;        # Hoặc để người dùng nhập đường dẫn&#10;        custom_path = input(f&quot;\nNhập đường dẫn tệp CSV (Enter để dùng mặc định):\n{csv_file}\n&gt; &quot;).strip()&#10;        if custom_path:&#10;            csv_file = custom_path&#10;        &#10;        try:&#10;            labels, counts = extract_labels_no_pandas(csv_file)&#10;        except FileNotFoundError:&#10;            print(f&quot;\nLỗi: Không tìm thấy tệp {csv_file}&quot;)&#10;        except Exception as e:&#10;            print(f&quot;\nLỗi: {e}&quot;)&#10;    &#10;    elif choice == &quot;2&quot;:&#10;        # Phân tích tất cả các tệp&#10;        directory = r&quot;D:\PROJECT\Machine Learning\IOT\CICIDS2018-CSV&quot;&#10;        &#10;        custom_dir = input(f&quot;\nNhập đường dẫn thư mục (Enter để dùng mặc định):\n{directory}\n&gt; &quot;).strip()&#10;        if custom_dir:&#10;            directory = custom_dir&#10;        &#10;        try:&#10;            all_labels = extract_labels_from_all_files(directory)&#10;            &#10;            # Lưu kết quả ra tệp&#10;            output_file = &quot;labels_summary.txt&quot;&#10;            with open(output_file, 'w', encoding='utf-8') as f:&#10;                f.write(&quot;TỔNG HỢP CÁC NHÃN TỪ CICIDS2018 DATASET\n&quot;)&#10;                f.write(&quot;=&quot; * 80 + &quot;\n\n&quot;)&#10;                f.write(f&quot;Tổng số nhãn duy nhất: {len(all_labels)}\n\n&quot;)&#10;                f.write(&quot;Danh sách các nhãn:\n&quot;)&#10;                for label in sorted(all_labels):&#10;                    f.write(f&quot;  - {label}\n&quot;)&#10;            &#10;            print(f&quot;\nKết quả đã được lưu vào: {output_file}&quot;)&#10;        &#10;        except Exception as e:&#10;            print(f&quot;\nLỗi: {e}&quot;)&#10;    &#10;    else:&#10;        print(&quot;\nLựa chọn không hợp lệ!&quot;)&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f64b155",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3463a434",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch-geometric scikit-learn matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb1a591",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92479ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# KAGGLE PATH CONFIGURATION - UPDATE THIS!\n",
    "# ============================================================================\n",
    "# Example: If you added dataset named 'graph-iot-dataset', use:\n",
    "# DATA_DIR = \"/kaggle/input/graph-iot-dataset/dataset-processed/ip_gnn\"\n",
    "DATA_DIR = \"/kaggle/input/YOUR-DATASET-NAME/dataset-processed/ip_gnn\"\n",
    "OUTPUT_DIR = \"/kaggle/working/output/ip_gnn\"\n",
    "\n",
    "# ============================================================================\n",
    "# PROJECT SETTINGS\n",
    "# ============================================================================\n",
    "PROJECT_NAME = \"IP-GNN\"\n",
    "SEED = 42\n",
    "DEVICE = \"auto\"  # auto, cuda, mps, cpu\n",
    "\n",
    "# ============================================================================\n",
    "# DATA SETTINGS\n",
    "# ============================================================================\n",
    "SRC_IP_COL = \"Src IP\"\n",
    "SRC_PORT_COL = \"Src Port\"\n",
    "DST_IP_COL = \"Dst IP\"\n",
    "DST_PORT_COL = \"Dst Port\"\n",
    "LABEL_COL = \"Label\"\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL SETTINGS\n",
    "# ============================================================================\n",
    "HIDDEN_DIM = 128\n",
    "NUM_CLASSES = 2\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "AGGR = \"mean\"  # mean or sum\n",
    "\n",
    "# ============================================================================\n",
    "# GRAPH SETTINGS\n",
    "# ============================================================================\n",
    "MAPPING_MODE = \"ip_port\"  # ip_port or ip_only\n",
    "ANTI_LEAKAGE_ENABLED = True\n",
    "ANTI_LEAKAGE_SCOPE = \"src_ip_only\"  # all_ips or src_ip_only\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING SETTINGS\n",
    "# ============================================================================\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "PATIENCE = 10\n",
    "MIN_DELTA = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fbefb6",
   "metadata": {},
   "source": [
    "## 3. Imports and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482d62c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_curve, auc, classification_report,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "print(\"âœ… Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cc2dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"Set random seed for reproducibility.\"\"\"\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_device(device_str: str = \"auto\") -> torch.device:\n",
    "    \"\"\"Get PyTorch device.\"\"\"\n",
    "    if device_str == \"auto\":\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "            print(f\"Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "            device = torch.device(\"mps\")\n",
    "            print(\"Using Apple Metal (MPS)\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"Using CPU\")\n",
    "    else:\n",
    "        device = torch.device(device_str)\n",
    "        print(f\"Using device: {device}\")\n",
    "    return device\n",
    "\n",
    "def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray,\n",
    "                    y_probs: Optional[np.ndarray] = None) -> Dict[str, float]:\n",
    "    \"\"\"Compute comprehensive metrics.\"\"\"\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, pos_label=1, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, pos_label=1, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
    "    }\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "    metrics[\"true_positive\"] = int(tp)\n",
    "    metrics[\"true_negative\"] = int(tn)\n",
    "    metrics[\"false_positive\"] = int(fp)\n",
    "    metrics[\"false_negative\"] = int(fn)\n",
    "    metrics[\"far\"] = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "    metrics[\"detection_rate\"] = metrics[\"recall\"]\n",
    "    \n",
    "    if y_probs is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "        metrics[\"auc\"] = auc(fpr, tpr)\n",
    "        metrics[\"average_precision\"] = average_precision_score(y_true, y_probs)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping callback to prevent overfitting.\"\"\"\n",
    "    def __init__(self, patience: int = 10, min_delta: float = 1e-4):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "    \n",
    "    def __call__(self, score: float) -> bool:\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            return False\n",
    "        if score > self.best_score + self.min_delta:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "class RandomEdgeSampler:\n",
    "    \"\"\"Simple random edge sampler for mini-batch training on full graph.\"\"\"\n",
    "    \n",
    "    def __init__(self, edge_indices: torch.Tensor, batch_size: int, shuffle: bool = True):\n",
    "        self.edge_indices = edge_indices\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        indices = self.edge_indices.clone()\n",
    "        if self.shuffle:\n",
    "            perm = torch.randperm(len(indices))\n",
    "            indices = indices[perm]\n",
    "        \n",
    "        for i in range(0, len(indices), self.batch_size):\n",
    "            yield indices[i:i + self.batch_size]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (len(self.edge_indices) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "print(\"âœ… Utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a77758",
   "metadata": {},
   "source": [
    "## 4. Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8af3b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_endpoint_graph(\n",
    "    df: pd.DataFrame,\n",
    "    feature_cols: list,\n",
    "    src_ip_col: str = SRC_IP_COL,\n",
    "    dst_ip_col: str = DST_IP_COL,\n",
    "    label_col: str = LABEL_COL,\n",
    "    mapping_mode: str = MAPPING_MODE,\n",
    "    anti_leakage: bool = ANTI_LEAKAGE_ENABLED\n",
    ") -> Tuple[Data, int]:\n",
    "    \"\"\"Create PyG Data object for endpoint-based graph.\"\"\"\n",
    "    print(f\"\\nðŸ”¨ Building Endpoint Graph (E-GraphSAGE)...\")\n",
    "    print(f\"   Mapping mode: {mapping_mode}\")\n",
    "    print(f\"   Anti-leakage: {anti_leakage}\")\n",
    "    \n",
    "    with tqdm(total=4, desc=\"ðŸ“ Graph construction\", ncols=100) as pbar:\n",
    "        # Step 1: Build endpoint mapping\n",
    "        endpoint_to_idx, src_indices, dst_indices = _build_endpoint_mapping(\n",
    "            df, src_ip_col, dst_ip_col, mapping_mode, anti_leakage\n",
    "        )\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix_str(\"Built endpoint mapping\")\n",
    "        \n",
    "        num_nodes = len(endpoint_to_idx)\n",
    "        num_edges = len(df)\n",
    "        \n",
    "        # Step 2: Build edge_index\n",
    "        edge_index = torch.tensor(\n",
    "            np.stack([src_indices, dst_indices], axis=0),\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix_str(\"Built edge index\")\n",
    "        \n",
    "        # Step 3: Extract edge features\n",
    "        edge_attr = df[feature_cols].values.astype(np.float32)\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix_str(\"Extracted edge features\")\n",
    "        \n",
    "        # Step 4: Create edge labels\n",
    "        edge_y = (df[label_col] != 0).astype(int).values\n",
    "        edge_y = torch.tensor(edge_y, dtype=torch.long)\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix_str(\"Created edge labels\")\n",
    "    \n",
    "    # Node features: ones vector (as per E-GraphSAGE)\n",
    "    num_edge_features = edge_attr.shape[1]\n",
    "    x = torch.ones((num_nodes, num_edge_features), dtype=torch.float)\n",
    "    \n",
    "    # Create PyG Data\n",
    "    data = Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        edge_y=edge_y,\n",
    "        num_nodes=num_nodes\n",
    "    )\n",
    "    \n",
    "    # Statistics\n",
    "    benign = (edge_y == 0).sum().item()\n",
    "    attack = (edge_y == 1).sum().item()\n",
    "    \n",
    "    print(f\"\\nâœ… Endpoint graph built:\")\n",
    "    print(f\"   Nodes (endpoints): {num_nodes:,}\")\n",
    "    print(f\"   Edges (flows):     {num_edges:,}\")\n",
    "    print(f\"   Edge features:     {num_edge_features}\")\n",
    "    print(f\"   Benign edges:      {benign:,} ({benign/num_edges*100:.1f}%)\")\n",
    "    print(f\"   Attack edges:      {attack:,} ({attack/num_edges*100:.1f}%)\")\n",
    "    \n",
    "    return data, num_nodes\n",
    "\n",
    "def _build_endpoint_mapping(\n",
    "    df, src_ip_col, dst_ip_col, mapping_mode, anti_leakage\n",
    ") -> Tuple[Dict, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Build endpoint mapping and edge indices.\"\"\"\n",
    "    \n",
    "    # Extract IP columns\n",
    "    src_ips = df[src_ip_col].astype(str).values\n",
    "    dst_ips = df[dst_ip_col].astype(str).values\n",
    "    \n",
    "    # Apply IP random mapping if enabled (anti-leakage)\n",
    "    if anti_leakage:\n",
    "        rng = np.random.RandomState(SEED)\n",
    "        \n",
    "        if ANTI_LEAKAGE_SCOPE == \"all_ips\":\n",
    "            unique_ips = np.unique(np.concatenate([src_ips, dst_ips]))\n",
    "        else:  # src_ip_only\n",
    "            unique_ips = np.unique(src_ips)\n",
    "        \n",
    "        ip_map = {ip: f\"IP_{i:06d}\" for i, ip in enumerate(unique_ips)}\n",
    "        \n",
    "        src_ips = np.array([ip_map.get(ip, ip) for ip in src_ips])\n",
    "        if ANTI_LEAKAGE_SCOPE == \"all_ips\":\n",
    "            dst_ips = np.array([ip_map.get(ip, ip) for ip in dst_ips])\n",
    "    \n",
    "    # Build endpoint strings\n",
    "    if mapping_mode == \"ip_port\":\n",
    "        src_ports = df[SRC_PORT_COL].astype(str).values\n",
    "        dst_ports = df[DST_PORT_COL].astype(str).values\n",
    "        src_endpoints = [f\"{ip}:{port}\" for ip, port in zip(src_ips, src_ports)]\n",
    "        dst_endpoints = [f\"{ip}:{port}\" for ip, port in zip(dst_ips, dst_ports)]\n",
    "    else:  # ip_only\n",
    "        src_endpoints = src_ips.tolist()\n",
    "        dst_endpoints = dst_ips.tolist()\n",
    "    \n",
    "    # Create mapping\n",
    "    unique_endpoints = sorted(set(src_endpoints + dst_endpoints))\n",
    "    endpoint_to_idx = {ep: idx for idx, ep in enumerate(unique_endpoints)}\n",
    "    \n",
    "    # Convert to indices\n",
    "    src_indices = np.array([endpoint_to_idx[ep] for ep in src_endpoints], dtype=np.int64)\n",
    "    dst_indices = np.array([endpoint_to_idx[ep] for ep in dst_endpoints], dtype=np.int64)\n",
    "    \n",
    "    return endpoint_to_idx, src_indices, dst_indices\n",
    "\n",
    "print(\"âœ… Graph construction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503791cc",
   "metadata": {},
   "source": [
    "## 5. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac92206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeFeatureSAGEConv(nn.Module):\n",
    "    \"\"\"SAGEConv layer that incorporates edge features during aggregation.\n",
    "    \n",
    "    HÆ°á»›ng 1: Má»—i layer tá»± project edge_attr\n",
    "    - LuÃ´n giá»¯ edge_attr gá»‘c (in_edge_dim)\n",
    "    - ThÃªm lin_edge: Linear(in_edge_dim -> out_dim)\n",
    "    - Aggregate lin_edge(edge_attr) thay vÃ¬ edge_attr raw\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim: int, out_dim: int, in_edge_dim: int, aggr: str = \"mean\"):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.in_edge_dim = in_edge_dim\n",
    "        self.aggr = aggr\n",
    "        \n",
    "        self.lin_self = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        self.lin_neigh = nn.Linear(out_dim, out_dim, bias=False)  # Now takes projected edge dim\n",
    "        self.lin_edge = nn.Linear(in_edge_dim, out_dim, bias=False)  # Project edge_attr to out_dim\n",
    "        self.bias = nn.Parameter(torch.zeros(out_dim))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, \n",
    "                edge_attr: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward with edge features.\n",
    "        \n",
    "        Args:\n",
    "            x: Node features [num_nodes, in_dim]\n",
    "            edge_index: Edges [2, num_edges]\n",
    "            edge_attr: Original edge features [num_edges, in_edge_dim] (always raw)\n",
    "            \n",
    "        Returns:\n",
    "            Updated node features [num_nodes, out_dim]\n",
    "        \"\"\"\n",
    "        src, dst = edge_index\n",
    "        num_nodes = x.size(0)\n",
    "        \n",
    "        # Self transformation\n",
    "        out_self = self.lin_self(x)\n",
    "        \n",
    "        # Project edge features: edge_attr (in_edge_dim) -> projected (out_dim)\n",
    "        edge_projected = self.lin_edge(edge_attr)\n",
    "        \n",
    "        # Aggregate projected edge features\n",
    "        aggregated = torch.zeros(num_nodes, self.out_dim, device=x.device)\n",
    "        \n",
    "        if self.aggr == \"mean\":\n",
    "            ones = torch.ones(edge_index.size(1), device=x.device)\n",
    "            degree = torch.zeros(num_nodes, device=x.device)\n",
    "            degree.scatter_add_(0, dst, ones)\n",
    "            degree = degree.clamp(min=1)\n",
    "            \n",
    "            aggregated.scatter_add_(0, dst.unsqueeze(1).expand_as(edge_projected), edge_projected)\n",
    "            aggregated = aggregated / degree.unsqueeze(1)\n",
    "        elif self.aggr == \"sum\":\n",
    "            aggregated.scatter_add_(0, dst.unsqueeze(1).expand_as(edge_projected), edge_projected)\n",
    "        \n",
    "        out_neigh = self.lin_neigh(aggregated)\n",
    "        out = out_self + out_neigh + self.bias\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class EGraphSAGE(nn.Module):\n",
    "    \"\"\"E-GraphSAGE for edge classification.\n",
    "    \n",
    "    Architecture:\n",
    "    - K layers of EdgeFeatureSAGEConv\n",
    "    - Edge representation: concat(z_u, z_v)\n",
    "    - Edge classifier: Linear(2*hidden_dim -> num_classes)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        hidden_dim: int = 128,\n",
    "        num_classes: int = 2,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.2,\n",
    "        aggr: str = \"mean\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = dropout\n",
    "        self.in_edge_dim = in_dim  # Store original edge dimension\n",
    "        \n",
    "        # Build layers - all layers receive original edge_attr (in_dim)\n",
    "        self.convs = nn.ModuleList()\n",
    "        # First layer: node in_dim -> hidden_dim, edge in_dim -> hidden_dim\n",
    "        self.convs.append(EdgeFeatureSAGEConv(in_dim, hidden_dim, in_edge_dim=in_dim, aggr=aggr))\n",
    "        \n",
    "        # Subsequent layers: node hidden_dim -> hidden_dim, edge still in_dim -> hidden_dim\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(EdgeFeatureSAGEConv(hidden_dim, hidden_dim, in_edge_dim=in_dim, aggr=aggr))\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.bns = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "        \n",
    "        # Edge classifier - output num_classes logits for softmax + cross entropy (paper)\n",
    "        self.edge_classifier = nn.Sequential(\n",
    "            nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "        print(f\"   E-GraphSAGE (HÆ°á»›ng 1): edge_dim={in_dim}, {in_dim}â†’{hidden_dim}x{num_layers}â†’{num_classes}\")\n",
    "        print(f\"   Each layer projects edge_attr: {in_dim}â†’{hidden_dim}\")\n",
    "        print(f\"   Loss: Softmax + CrossEntropy (paper)\")\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        edge_index: torch.Tensor,\n",
    "        edge_attr: torch.Tensor,\n",
    "        edge_label_index: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass for edge classification.\n",
    "        \n",
    "        Args:\n",
    "            x: Node features [num_nodes, in_dim]\n",
    "            edge_index: All edges [2, num_edges]\n",
    "            edge_attr: Edge features [num_edges, in_edge_dim] - always original!\n",
    "            edge_label_index: Edges to classify [2, num_target_edges]\n",
    "            \n",
    "        Returns:\n",
    "            Edge logits [num_target_edges, num_classes]\n",
    "        \"\"\"\n",
    "        # Update node embeddings - pass original edge_attr to all layers\n",
    "        h = x\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            # Each layer projects edge_attr internally via lin_edge\n",
    "            h = conv(h, edge_index, edge_attr)  # edge_attr is always original\n",
    "            h = self.bns[i](h)\n",
    "            h = F.relu(h)\n",
    "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Get edges to classify\n",
    "        if edge_label_index is None:\n",
    "            edge_label_index = edge_index\n",
    "        \n",
    "        src_emb = h[edge_label_index[0]]\n",
    "        dst_emb = h[edge_label_index[1]]\n",
    "        \n",
    "        # Concatenate embeddings\n",
    "        edge_emb = torch.cat([src_emb, dst_emb], dim=1)\n",
    "        \n",
    "        # Classify edges - output [num_edges, num_classes] logits\n",
    "        edge_logits = self.edge_classifier(edge_emb)\n",
    "        \n",
    "        return edge_logits  # [num_edges, num_classes]\n",
    "\n",
    "print(\"âœ… Model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c855dea",
   "metadata": {},
   "source": [
    "## 6. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f13ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_edges_fullbatch(model, data, edge_indices, criterion, device, threshold=0.5):\n",
    "    \"\"\"Evaluate model on given edges using full-batch forward pass.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Single forward pass for all edges\n",
    "        logits_all = model(data.x, data.edge_index, data.edge_attr)  # [num_edges, num_classes]\n",
    "        # Get logits for evaluation edges\n",
    "        logits = logits_all[edge_indices]\n",
    "        edge_labels = data.edge_y[edge_indices]\n",
    "        loss = criterion(logits, edge_labels.long()).item()\n",
    "        \n",
    "        # Softmax -> probability of class 1 (attack)\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "        pred = (probs >= threshold).astype(int)\n",
    "        true = edge_labels.cpu().numpy()\n",
    "        \n",
    "        metrics = compute_metrics(true, pred, y_probs=probs)\n",
    "        \n",
    "    return loss, metrics\n",
    "\n",
    "def evaluate_edges_with_predictions(model, data, edge_indices, criterion, threshold, device):\n",
    "    \"\"\"Evaluate and return predictions using full-batch forward pass.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Single forward pass for all edges\n",
    "        logits_all = model(data.x, data.edge_index, data.edge_attr)  # [num_edges, num_classes]\n",
    "        # Get logits for evaluation edges\n",
    "        logits = logits_all[edge_indices]\n",
    "        edge_labels = data.edge_y[edge_indices]\n",
    "        loss = criterion(logits, edge_labels.long()).item()\n",
    "        \n",
    "        # Softmax -> probability of class 1 (attack)\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "        pred = (probs >= threshold).astype(int)\n",
    "        true = edge_labels.cpu().numpy()\n",
    "        \n",
    "        metrics = compute_metrics(true, pred, y_probs=probs)\n",
    "        \n",
    "    return loss, metrics, true, pred, probs\n",
    "\n",
    "def tune_threshold_edges(model, data, edge_indices, device):\n",
    "    \"\"\"Find optimal threshold to maximize F1 score.\n",
    "    \n",
    "    With softmax output, we tune threshold on P(attack) = softmax(logits)[:, 1]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Single forward pass for all edges\n",
    "        logits_all = model(data.x, data.edge_index, data.edge_attr)  # [num_edges, num_classes]\n",
    "        # Get logits for validation edges\n",
    "        logits = logits_all[edge_indices]\n",
    "        # Softmax -> probability of class 1 (attack)\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "        true = data.edge_y[edge_indices].cpu().numpy()\n",
    "    \n",
    "    best_t, best_f1 = 0.5, 0.0\n",
    "    \n",
    "    thresholds = np.linspace(0.01, 0.99, 99)\n",
    "    for t in tqdm(thresholds, desc=\"   Searching threshold\", ncols=100, leave=False):\n",
    "        pred = (probs >= t).astype(int)\n",
    "        f1 = f1_score(true, pred, zero_division=0)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_t = t\n",
    "    \n",
    "    return best_t\n",
    "\n",
    "print(\"âœ… Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9af4bb8",
   "metadata": {},
   "source": [
    "## 7. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a497f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "device = get_device(DEVICE)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ”· IP-BASED GNN PIPELINE (E-GraphSAGE)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "data_dir = Path(DATA_DIR)\n",
    "print(f\"\\nðŸ“‚ [1/5] Loading preprocessed data...\")\n",
    "print(f\"   Data directory: {data_dir}\")\n",
    "\n",
    "# Load data with IPs\n",
    "df = pd.read_csv(data_dir / \"data_with_ips.csv\")\n",
    "y = np.load(data_dir / \"y.npy\")\n",
    "idx_train = np.load(data_dir / \"idx_train.npy\")\n",
    "idx_val = np.load(data_dir / \"idx_val.npy\")\n",
    "idx_test = np.load(data_dir / \"idx_test.npy\")\n",
    "\n",
    "print(f\"   Total samples: {len(df):,}\")\n",
    "print(f\"   Train: {len(idx_train):,} | Val: {len(idx_val):,} | Test: {len(idx_test):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1417cf47",
   "metadata": {},
   "source": [
    "## 8. Prepare Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8a54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”§ [2/5] Preparing features...\")\n",
    "ip_cols = [SRC_IP_COL, DST_IP_COL]\n",
    "feature_cols = [c for c in df.columns if c not in ip_cols and c != LABEL_COL]\n",
    "\n",
    "# Ensure numeric\n",
    "for col in feature_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "df[feature_cols] = df[feature_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "print(f\"   Features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda72523",
   "metadata": {},
   "source": [
    "## 9. Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2845f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ”¨ [3/5] Building endpoint graph...\")\n",
    "data, num_nodes = create_endpoint_graph(df, feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c83b65",
   "metadata": {},
   "source": [
    "## 10. Use Pre-Split Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d308a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸŽ¯ [4/5] Using pre-split indices from preprocessing...\")\n",
    "\n",
    "# Use the indices that were already loaded from the preprocessing step\n",
    "# Convert to tensors\n",
    "train_idx_t = torch.tensor(idx_train, dtype=torch.long)\n",
    "val_idx_t = torch.tensor(idx_val, dtype=torch.long)\n",
    "test_idx_t = torch.tensor(idx_test, dtype=torch.long)\n",
    "\n",
    "print(f\"   Train edges: {len(idx_train):,}\")\n",
    "print(f\"   Val edges:   {len(idx_val):,}\")\n",
    "print(f\"   Test edges:  {len(idx_test):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f190098f",
   "metadata": {},
   "source": [
    "## 11. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f2c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸš€ [5/5] Training model...\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸš€ TRAINING IP-BASED GNN (E-GraphSAGE)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Move data to device\n",
    "data = data.to(device)\n",
    "train_idx_t = train_idx_t.to(device)\n",
    "val_idx_t = val_idx_t.to(device)\n",
    "test_idx_t = test_idx_t.to(device)\n",
    "\n",
    "# Calculate class weights for CrossEntropyLoss\n",
    "train_edge_y = data.edge_y[train_idx_t]\n",
    "pos = (train_edge_y == 1).sum().item()\n",
    "neg = (train_edge_y == 0).sum().item()\n",
    "total = pos + neg\n",
    "# Weight inversely proportional to class frequency\n",
    "class_weights = torch.tensor([total / (2 * neg), total / (2 * pos)], device=device)\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset Statistics:\")\n",
    "print(f\"   Training edges:   {len(idx_train):,}\")\n",
    "print(f\"   Validation edges: {len(idx_val):,}\")\n",
    "print(f\"   Test edges:       {len(idx_test):,}\")\n",
    "print(f\"   Class distribution: Benign={neg:,} ({neg/(neg+pos)*100:.1f}%), Attack={pos:,} ({pos/(neg+pos)*100:.1f}%)\")\n",
    "print(f\"   Class weights:      [{class_weights[0]:.4f}, {class_weights[1]:.4f}]\")\n",
    "\n",
    "# Model\n",
    "print(f\"\\nðŸ—ï¸  Model Configuration:\")\n",
    "print(f\"   Hidden dim:  {HIDDEN_DIM}\")\n",
    "print(f\"   Num layers:  {NUM_LAYERS}\")\n",
    "print(f\"   Dropout:     {DROPOUT}\")\n",
    "print(f\"   Aggregation: {AGGR}\")\n",
    "\n",
    "model = EGraphSAGE(\n",
    "    in_dim=data.x.shape[1],\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    aggr=AGGR\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"   Total params: {total_params:,}\")\n",
    "print(f\"   Device:       {device}\")\n",
    "\n",
    "# Optimizer & Loss (Softmax + CrossEntropy as per paper)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "print(f\"\\nâš™ï¸  Training Configuration:\")\n",
    "print(f\"   Epochs:         {EPOCHS}\")\n",
    "print(f\"   Learning rate:  {LEARNING_RATE}\")\n",
    "print(f\"   Weight decay:   {WEIGHT_DECAY}\")\n",
    "print(f\"   Early stopping: {PATIENCE} epochs\")\n",
    "print(f\"   Training mode:  Full-batch (1 forward/epoch)\")\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(patience=PATIENCE, min_delta=MIN_DELTA)\n",
    "\n",
    "# Training history\n",
    "best_f1 = 0.0\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_f1': [],\n",
    "    'val_accuracy': []\n",
    "}\n",
    "\n",
    "print(f\"\\nðŸ”¥ Starting Training (Full-batch mode)...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "epoch_pbar = tqdm(range(1, EPOCHS + 1), desc=\"Training\", unit=\"epoch\", ncols=100)\n",
    "\n",
    "for epoch in epoch_pbar:\n",
    "    # Train: 1 forward pass per epoch\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Single forward pass on entire graph for all edges\n",
    "    logits_all = model(data.x, data.edge_index, data.edge_attr)  # [num_edges, num_classes]\n",
    "    \n",
    "    # Compute loss only on training edges\n",
    "    loss = criterion(logits_all[train_idx_t], data.edge_y[train_idx_t])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_loss = loss.item()\n",
    "    \n",
    "    # Validate: use logits from full forward pass\n",
    "    val_loss, val_metrics = evaluate_edges_fullbatch(model, data, val_idx_t, criterion, device)\n",
    "    \n",
    "    # Update history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_f1'].append(val_metrics['f1'])\n",
    "    history['val_accuracy'].append(val_metrics['accuracy'])\n",
    "    \n",
    "    # Update progress bar\n",
    "    epoch_pbar.set_postfix({\n",
    "        'loss': f\"{train_loss:.4f}\",\n",
    "        'val_f1': f\"{val_metrics['f1']:.4f}\",\n",
    "        'val_acc': f\"{val_metrics['accuracy']:.4f}\"\n",
    "    })\n",
    "    \n",
    "    # Save best model\n",
    "    if val_metrics['f1'] > best_f1:\n",
    "        best_f1 = val_metrics['f1']\n",
    "        save_path = Path(OUTPUT_DIR) / 'best_model.pt'\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'metrics': val_metrics\n",
    "        }, save_path)\n",
    "    \n",
    "    # Early stopping\n",
    "    if early_stopping(val_metrics['f1']):\n",
    "        print(f\"\\nâš ï¸  Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nâœ… Training completed! Best validation F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724896f0",
   "metadata": {},
   "source": [
    "## 12. Tune Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef751748",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸŽ¯ TUNING DECISION THRESHOLD\")\n",
    "print(\"=\" * 70)\n",
    "best_threshold = tune_threshold_edges(model, data, val_idx_t, device)\n",
    "print(f\"âœ… Optimal threshold: {best_threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab79408",
   "metadata": {},
   "source": [
    "## 13. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c47322",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸ§ª FINAL EVALUATION ON TEST SET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = time.time()\n",
    "test_loss, test_metrics, y_true, y_pred, y_probs = evaluate_edges_with_predictions(\n",
    "    model, data, test_idx_t, criterion, best_threshold, device\n",
    ")\n",
    "inference_time = time.time() - start_time\n",
    "latency_per_sample = inference_time / len(y_true)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Test Results:\")\n",
    "print(f\"   Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"   Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"   Recall:    {test_metrics['recall']:.4f}\")\n",
    "print(f\"   F1 Score:  {test_metrics['f1']:.4f}\")\n",
    "print(f\"   AUC:       {test_metrics.get('auc', 0):.4f}\")\n",
    "print(f\"   FAR:       {test_metrics['far']:.4f}\")\n",
    "print(f\"\\nâ±ï¸  Inference Performance:\")\n",
    "print(f\"   Total time:   {inference_time:.2f}s\")\n",
    "print(f\"   Latency:      {latency_per_sample*1000:.4f} ms/sample\")\n",
    "print(f\"   Throughput:   {len(y_true)/inference_time:.2f} samples/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c1e951",
   "metadata": {},
   "source": [
    "## 14. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806fc9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(OUTPUT_DIR)\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save metrics as JSON\n",
    "test_metrics['latency_seconds'] = latency_per_sample\n",
    "test_metrics['latency_ms'] = latency_per_sample * 1000\n",
    "with open(output_path / 'metrics.json', 'w') as f:\n",
    "    json.dump(test_metrics, f, indent=2)\n",
    "\n",
    "# Save classification report\n",
    "report = classification_report(y_true, y_pred, target_names=['Benign', 'Attack'], digits=4)\n",
    "with open(output_path / 'classification_report.txt', 'w') as f:\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "    f.write(\"   IP-GNN CLASSIFICATION REPORT\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Results saved to {output_path}/\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ¨ ALL DONE!\")\n",
    "print(\"=\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f04b44",
   "metadata": {},
   "source": [
    "## 15. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b932e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=['Benign', 'Attack'],\n",
    "            yticklabels=['Benign', 'Attack'], ax=ax)\n",
    "ax.set_title('Confusion Matrix - IP GNN', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_path / 'confusion_matrix.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "ax.plot(fpr, tpr, color='#4CAF50', lw=2.5, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "ax.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Random')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curve - IP GNN', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_path / 'roc_curve.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Training History\n",
    "if len(history['train_loss']) > 0:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    axes[0].plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "    axes[0].plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training & Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].plot(epochs, history['val_f1'], 'g-', label='Val F1', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('F1 Score')\n",
    "    axes[1].set_title('Validation F1 Score')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[2].plot(epochs, history['val_accuracy'], 'm-', label='Val Accuracy', linewidth=2)\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Accuracy')\n",
    "    axes[2].set_title('Validation Accuracy')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path / 'training_history.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "print(\"âœ… Visualizations complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

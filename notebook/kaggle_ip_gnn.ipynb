{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77b75377",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ C√†i ƒë·∫∑t th∆∞ vi·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6017904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# C√†i ƒë·∫∑t PyTorch v√† PyTorch Geometric\n",
    "!pip install torch==2.1.0 torchvision==0.16.0 --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.1.0+cu118.html\n",
    "!pip install torch-geometric==2.4.0\n",
    "!pip install scikit-learn pandas numpy pyyaml tqdm matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d340d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ki·ªÉm tra GPU\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7a1314",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Ki·ªÉm tra Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3546cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n dataset - THAY ƒê·ªîI theo t√™n dataset b·∫°n upload\n",
    "DATASET_PATH = '/kaggle/input/cicids2018-processed'\n",
    "\n",
    "print(\"üìÅ Files trong dataset:\")\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    files = sorted(os.listdir(DATASET_PATH))\n",
    "    for f in files:\n",
    "        fpath = os.path.join(DATASET_PATH, f)\n",
    "        if os.path.isfile(fpath):\n",
    "            size_mb = os.path.getsize(fpath) / (1024**2)\n",
    "            print(f\"  ‚úì {f:<60} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(f\"  ‚ùå Kh√¥ng t√¨m th·∫•y: {DATASET_PATH}\")\n",
    "    print(\"  üëâ H√£y upload dataset v√† s·ª≠a DATASET_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5658d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load manifest\n",
    "manifest_path = os.path.join(DATASET_PATH, 'manifest.json')\n",
    "if os.path.exists(manifest_path):\n",
    "    with open(manifest_path, 'r') as f:\n",
    "        manifest = json.load(f)\n",
    "    \n",
    "    print(\"\\nüìä Th√¥ng tin dataset:\")\n",
    "    print(f\"  Mode: {manifest['mode']}\")\n",
    "    print(f\"  Total samples: {manifest['total_samples']:,}\")\n",
    "    print(f\"  Features: {manifest['num_features']}\")\n",
    "    print(f\"  Train: {manifest['splits']['train']['size']:,}\")\n",
    "    print(f\"  Val: {manifest['splits']['val']['size']:,}\")\n",
    "    print(f\"  Test: {manifest['splits']['test']['size']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d3fd69",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Load Data & Extract IP Information\n",
    "\n",
    "C·∫ßn ƒë·ªçc CSV files ƒë·ªÉ l·∫•y th√¥ng tin Src IP v√† Dst IP cho m·ªói flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0583b448",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading preprocessed data...\")\n",
    "X = np.load(os.path.join(DATASET_PATH, 'X.npy'))\n",
    "y = np.load(os.path.join(DATASET_PATH, 'y.npy'))\n",
    "idx_train = np.load(os.path.join(DATASET_PATH, 'idx_train.npy'))\n",
    "idx_val = np.load(os.path.join(DATASET_PATH, 'idx_val.npy'))\n",
    "idx_test = np.load(os.path.join(DATASET_PATH, 'idx_test.npy'))\n",
    "\n",
    "print(f\"\\n‚úì Loaded:\")\n",
    "print(f\"  X: {X.shape}\")\n",
    "print(f\"  y: {y.shape}\")\n",
    "print(f\"  Train: {len(idx_train):,}, Val: {len(idx_val):,}, Test: {len(idx_test):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8becac61",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Sampling Strategy (Optional but Recommended)\n",
    "\n",
    "**Dataset r·∫•t l·ªõn (11M+ flows)!** Build graph v√† training s·∫Ω m·∫•t nhi·ªÅu th·ªùi gian.\n",
    "\n",
    "**Khuy·∫øn ngh·ªã:**\n",
    "- ƒê·ªÉ th·ª≠ nghi·ªám nhanh: `SAMPLE_SIZE = 500_000` (500K flows, ~10-15 ph√∫t)\n",
    "- ƒê·ªÉ k·∫øt qu·∫£ t·ªët h∆°n: `SAMPLE_SIZE = 2_000_000` (2M flows, ~45-90 ph√∫t)\n",
    "- ƒê·ªÉ train full: `SAMPLE_SIZE = None` (11M+ flows, nhi·ªÅu gi·ªù!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f092c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ SAMPLING CONFIG - THAY ƒê·ªîI THEO NHU C·∫¶U\n",
    "SAMPLE_SIZE = 1_000_000  # None = full dataset, 1M = 1 tri·ªáu samples\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "if SAMPLE_SIZE is not None and SAMPLE_SIZE < len(X):\n",
    "    print(f\"\\n‚ö° Sampling {SAMPLE_SIZE:,} flows from {len(X):,} total...\")\n",
    "    \n",
    "    # Stratified sampling to keep class balance\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Sample indices while preserving class distribution\n",
    "    all_indices = np.arange(len(X))\n",
    "    sampled_indices, _ = train_test_split(\n",
    "        all_indices, \n",
    "        train_size=SAMPLE_SIZE,\n",
    "        stratify=y,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    sampled_indices.sort()  # Keep order\n",
    "    \n",
    "    # Create mapping from old to new indices\n",
    "    old_to_new = {old_idx: new_idx for new_idx, old_idx in enumerate(sampled_indices)}\n",
    "    \n",
    "    # Sample data\n",
    "    X = X[sampled_indices]\n",
    "    y = y[sampled_indices]\n",
    "    \n",
    "    # Remap train/val/test indices\n",
    "    idx_train_new = [old_to_new[i] for i in idx_train if i in old_to_new]\n",
    "    idx_val_new = [old_to_new[i] for i in idx_val if i in old_to_new]\n",
    "    idx_test_new = [old_to_new[i] for i in idx_test if i in old_to_new]\n",
    "    \n",
    "    idx_train = np.array(idx_train_new)\n",
    "    idx_val = np.array(idx_val_new)\n",
    "    idx_test = np.array(idx_test_new)\n",
    "    \n",
    "    print(f\"\\n‚úì Sampled:\")\n",
    "    print(f\"  X: {X.shape}\")\n",
    "    print(f\"  Train: {len(idx_train):,}, Val: {len(idx_val):,}, Test: {len(idx_test):,}\")\n",
    "    \n",
    "    # Check class distribution\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    print(f\"\\nüìä Sampled class distribution:\")\n",
    "    for cls, cnt in zip(unique, counts):\n",
    "        print(f\"  Class {cls}: {cnt:,} ({cnt/len(y)*100:.1f}%)\")\n",
    "else:\n",
    "    print(f\"\\n‚úì Using FULL dataset: {len(X):,} flows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b52307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files ƒë·ªÉ l·∫•y IP addresses\n",
    "print(\"\\nLoading CSV files to extract IP addresses...\")\n",
    "\n",
    "csv_files = [f for f in os.listdir(DATASET_PATH) if f.endswith('_cleaned.csv')]\n",
    "csv_files.sort()\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files\")\n",
    "\n",
    "# Load all CSV files\n",
    "dfs = []\n",
    "for csv_file in csv_files:\n",
    "    print(f\"  Reading {csv_file}...\")\n",
    "    df = pd.read_csv(os.path.join(DATASET_PATH, csv_file))\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"\\n‚úì Total flows in CSV: {len(df_all):,}\")\n",
    "\n",
    "# Verify s·ªë l∆∞·ª£ng kh·ªõp v·ªõi X, y\n",
    "if len(df_all) != len(X):\n",
    "    print(f\"‚ö†Ô∏è WARNING: CSV rows ({len(df_all)}) != X rows ({len(X)})\")\n",
    "    print(\"   ƒê·∫£m b·∫£o CSV files kh·ªõp v·ªõi X.npy, y.npy\")\n",
    "else:\n",
    "    print(f\"‚úì Number of flows matches!\")\n",
    "\n",
    "# Extract IP columns\n",
    "src_ips = df_all['Src IP'].values\n",
    "dst_ips = df_all['Dst IP'].values\n",
    "\n",
    "print(f\"\\n‚úì Extracted IP addresses:\")\n",
    "print(f\"  Unique Source IPs: {len(set(src_ips)):,}\")\n",
    "print(f\"  Unique Destination IPs: {len(set(dst_ips)):,}\")\n",
    "print(f\"  Total unique IPs: {len(set(src_ips) | set(dst_ips)):,}\")\n",
    "\n",
    "# Free memory\n",
    "del dfs, df_all\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d259d72",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Build IP Graph\n",
    "\n",
    "T·∫°o graph structure:\n",
    "- **Nodes**: M·ªói unique IP l√† m·ªôt node\n",
    "- **Edges**: M·ªói flow t·∫°o m·ªôt edge t·ª´ Src IP ‚Üí Dst IP\n",
    "- **Edge labels**: Label c·ªßa flow (benign/attack)\n",
    "- **Node features**: Aggregate features t·ª´ c√°c flows li√™n quan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4864f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "print(\"üåê Building IP-based graph...\")\n",
    "print(f\"   Dataset size: {len(X):,} flows\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Step 1: Create IP to node ID mapping\n",
    "print(f\"\\n[1/6] üó∫Ô∏è  Mapping IPs to node IDs...\")\n",
    "step_start = time.time()\n",
    "\n",
    "with tqdm(total=3, desc=\"Creating mappings\", bar_format='{l_bar}{bar}| {elapsed}') as pbar:\n",
    "    all_ips = set(src_ips) | set(dst_ips)\n",
    "    pbar.update(1)\n",
    "    \n",
    "    ip_to_id = {ip: idx for idx, ip in enumerate(sorted(all_ips))}\n",
    "    pbar.update(1)\n",
    "    \n",
    "    id_to_ip = {idx: ip for ip, idx in ip_to_id.items()}\n",
    "    pbar.update(1)\n",
    "\n",
    "num_nodes = len(ip_to_id)\n",
    "print(f\"      ‚úì Created {num_nodes:,} nodes ({time.time()-step_start:.1f}s)\")\n",
    "\n",
    "# Step 2: Create edge list from flows\n",
    "print(f\"\\n[2/6] üîó Creating edge list from flows...\")\n",
    "step_start = time.time()\n",
    "\n",
    "edge_src = []\n",
    "edge_dst = []\n",
    "edge_labels = []\n",
    "\n",
    "for i in tqdm(range(len(src_ips)), desc=\"Processing flows\", \n",
    "              bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'):\n",
    "    src_id = ip_to_id[src_ips[i]]\n",
    "    dst_id = ip_to_id[dst_ips[i]]\n",
    "    \n",
    "    edge_src.append(src_id)\n",
    "    edge_dst.append(dst_id)\n",
    "    edge_labels.append(y[i])\n",
    "\n",
    "edge_index = torch.tensor([edge_src, edge_dst], dtype=torch.long)\n",
    "edge_labels = torch.tensor(edge_labels, dtype=torch.long)\n",
    "\n",
    "print(f\"      ‚úì Created {edge_index.shape[1]:,} edges ({time.time()-step_start:.1f}s)\")\n",
    "\n",
    "# Step 3: Aggregate node features (from training flows only)\n",
    "print(f\"\\n[3/6] üìä Aggregating node features (from training data only)...\")\n",
    "print(f\"      Using {len(idx_train):,} training flows to prevent data leakage\")\n",
    "step_start = time.time()\n",
    "\n",
    "# Initialize feature aggregation\n",
    "node_feature_sum = defaultdict(lambda: np.zeros(X.shape[1]))\n",
    "node_feature_count = defaultdict(int)\n",
    "\n",
    "# Only use training flows\n",
    "for i in tqdm(idx_train, desc=\"Aggregating features\", \n",
    "              bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'):\n",
    "    src_id = ip_to_id[src_ips[i]]\n",
    "    dst_id = ip_to_id[dst_ips[i]]\n",
    "    \n",
    "    # Aggregate features for both source and destination IPs\n",
    "    node_feature_sum[src_id] += X[i]\n",
    "    node_feature_count[src_id] += 1\n",
    "    \n",
    "    node_feature_sum[dst_id] += X[i]\n",
    "    node_feature_count[dst_id] += 1\n",
    "\n",
    "print(f\"      Computing mean features...\")\n",
    "node_features = np.zeros((num_nodes, X.shape[1]), dtype=np.float32)\n",
    "for node_id in tqdm(range(num_nodes), desc=\"Computing means\",\n",
    "                    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt}'):\n",
    "    if node_feature_count[node_id] > 0:\n",
    "        node_features[node_id] = node_feature_sum[node_id] / node_feature_count[node_id]\n",
    "\n",
    "print(f\"      ‚úì Node features: {node_features.shape} ({time.time()-step_start:.1f}s)\")\n",
    "\n",
    "# Step 4: Create node labels (majority vote)\n",
    "print(f\"\\n[4/6] üè∑Ô∏è  Creating node labels (majority vote)...\")\n",
    "step_start = time.time()\n",
    "\n",
    "node_label_votes = defaultdict(lambda: [0, 0])  # [benign_count, attack_count]\n",
    "\n",
    "for i in tqdm(range(len(src_ips)), desc=\"Voting labels\",\n",
    "              bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'):\n",
    "    src_id = ip_to_id[src_ips[i]]\n",
    "    dst_id = ip_to_id[dst_ips[i]]\n",
    "    \n",
    "    label = y[i]\n",
    "    node_label_votes[src_id][label] += 1\n",
    "    node_label_votes[dst_id][label] += 1\n",
    "\n",
    "# Majority vote\n",
    "node_labels = np.zeros(num_nodes, dtype=np.int64)\n",
    "for node_id in range(num_nodes):\n",
    "    votes = node_label_votes[node_id]\n",
    "    node_labels[node_id] = 1 if votes[1] > votes[0] else 0\n",
    "\n",
    "benign_count = np.sum(node_labels==0)\n",
    "attack_count = np.sum(node_labels==1)\n",
    "print(f\"      ‚úì Node labels: benign={benign_count:,}, attack={attack_count:,} ({time.time()-step_start:.1f}s)\")\n",
    "\n",
    "# Free memory\n",
    "print(f\"\\n[5/6] üßπ Cleaning up memory...\")\n",
    "del node_feature_sum, node_feature_count, node_label_votes\n",
    "gc.collect()\n",
    "print(f\"      ‚úì Memory freed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d18055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create edge splits based on flow splits\n",
    "print(f\"\\n[6/6] üé≠ Creating edge train/val/test splits...\")\n",
    "step_start = time.time()\n",
    "\n",
    "with tqdm(total=3, desc=\"Creating masks\", bar_format='{l_bar}{bar}| {elapsed}') as pbar:\n",
    "    train_edge_mask = np.zeros(len(edge_labels), dtype=bool)\n",
    "    train_edge_mask[idx_train] = True\n",
    "    pbar.update(1)\n",
    "    \n",
    "    val_edge_mask = np.zeros(len(edge_labels), dtype=bool)\n",
    "    val_edge_mask[idx_val] = True\n",
    "    pbar.update(1)\n",
    "    \n",
    "    test_edge_mask = np.zeros(len(edge_labels), dtype=bool)\n",
    "    test_edge_mask[idx_test] = True\n",
    "    pbar.update(1)\n",
    "\n",
    "train_edges = edge_index[:, train_edge_mask]\n",
    "val_edges = edge_index[:, val_edge_mask]\n",
    "test_edges = edge_index[:, test_edge_mask]\n",
    "\n",
    "print(f\"      ‚úì Train edges: {train_edges.shape[1]:,}\")\n",
    "print(f\"      ‚úì Val edges: {val_edges.shape[1]:,}\")\n",
    "print(f\"      ‚úì Test edges: {test_edges.shape[1]:,}\")\n",
    "print(f\"      ({time.time()-step_start:.1f}s)\")\n",
    "\n",
    "# Create PyG Data\n",
    "print(f\"\\nüìä Creating PyG Data object...\")\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "data = Data(\n",
    "    x=torch.tensor(node_features, dtype=torch.float32),\n",
    "    edge_index=edge_index,\n",
    "    edge_y=edge_labels,  # Edge labels\n",
    "    y=torch.tensor(node_labels, dtype=torch.long)  # Node labels (for reference)\n",
    ")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úÖ GRAPH BUILDING COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"‚è±Ô∏è  Total time: {total_time:.1f}s ({total_time/60:.1f} min)\")\n",
    "print(f\"üìä Graph statistics:\")\n",
    "print(f\"   ‚Ä¢ Nodes (IPs): {data.num_nodes:,}\")\n",
    "print(f\"   ‚Ä¢ Edges (flows): {data.edge_index.shape[1]:,}\")\n",
    "print(f\"   ‚Ä¢ Features per node: {data.x.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Node labels: {benign_count:,} benign, {attack_count:,} attack\")\n",
    "print(f\"   ‚Ä¢ Train edges: {train_edges.shape[1]:,}\")\n",
    "print(f\"   ‚Ä¢ Val edges: {val_edges.shape[1]:,}\")\n",
    "print(f\"   ‚Ä¢ Test edges: {test_edges.shape[1]:,}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdc33e0",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Define Model\n",
    "\n",
    "Edge classification model - predict label for each edge (flow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095dbc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "class IPGraphSAGE(nn.Module):\n",
    "    \"\"\"GraphSAGE for IP-based edge classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim, hidden_dim, num_classes, num_layers=2, dropout=0.2, aggr='mean'):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Node embedding layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_dim, hidden_dim, aggr=aggr))\n",
    "        \n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(SAGEConv(hidden_dim, hidden_dim, aggr=aggr))\n",
    "        \n",
    "        # Edge classifier\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_label_index=None):\n",
    "        # Node embeddings\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < len(self.convs) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # If edge_label_index provided, classify those edges\n",
    "        if edge_label_index is not None:\n",
    "            src_emb = x[edge_label_index[0]]\n",
    "            dst_emb = x[edge_label_index[1]]\n",
    "            edge_emb = torch.cat([src_emb, dst_emb], dim=1)\n",
    "            return self.edge_mlp(edge_emb)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = IPGraphSAGE(\n",
    "    in_dim=X.shape[1],\n",
    "    hidden_dim=128,\n",
    "    num_classes=2,\n",
    "    num_layers=2,\n",
    "    dropout=0.2,\n",
    "    aggr='mean'\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\n‚úì Model created:\")\n",
    "print(model)\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df01f4b1",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Training\n",
    "\n",
    "Train model to classify edges (flows) as benign or attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71c02c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move data to device\n",
    "data = data.to(device)\n",
    "\n",
    "# Config\n",
    "EPOCHS = 50  # Gi·∫£m t·ª´ 100 -> 50 cho dataset l·ªõn\n",
    "BATCH_SIZE = 4096  # TƒÉng batch size (IP-GNN c√≥ √≠t edges h∆°n Flow-GNN)\n",
    "LR = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "PATIENCE = 10  # Early stopping\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"‚úì Setup complete:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81f0b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_edges(model, data, edge_indices):\n",
    "    \"\"\"Evaluate model on given edges.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index, edge_indices)\n",
    "        pred = out.argmax(dim=1).cpu().numpy()\n",
    "        true = data.edge_y[edge_indices[0]].cpu().numpy()  # Edge labels\n",
    "        \n",
    "        acc = accuracy_score(true, pred)\n",
    "        f1 = f1_score(true, pred, average='weighted', zero_division=0)\n",
    "        precision = precision_score(true, pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(true, pred, average='weighted', zero_division=0)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': acc,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "\n",
    "# Training loop\n",
    "best_val_f1 = 0.0\n",
    "patience_counter = 0\n",
    "history = {'train_loss': [], 'val_acc': [], 'val_f1': []}\n",
    "\n",
    "print(\"\\nüöÄ Starting training...\\n\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # Train\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    # Mini-batch training over edges\n",
    "    num_train_edges = train_edges.shape[1]\n",
    "    indices = torch.randperm(num_train_edges)\n",
    "    \n",
    "    for start_idx in range(0, num_train_edges, BATCH_SIZE):\n",
    "        end_idx = min(start_idx + BATCH_SIZE, num_train_edges)\n",
    "        batch_indices = indices[start_idx:end_idx]\n",
    "        \n",
    "        batch_edges = train_edges[:, batch_indices]\n",
    "        batch_labels = data.edge_y[train_edge_mask][batch_indices]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, batch_edges)\n",
    "        loss = criterion(out, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    train_loss = total_loss / num_batches\n",
    "    \n",
    "    # Evaluate on validation edges\n",
    "    val_metrics = evaluate_edges(model, data, val_edges)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_metrics['accuracy'])\n",
    "    history['val_f1'].append(val_metrics['f1'])\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch:3d}/{EPOCHS} | \"\n",
    "          f\"Loss: {train_loss:.4f} | \"\n",
    "          f\"Val Acc: {val_metrics['accuracy']:.4f} | \"\n",
    "          f\"Val F1: {val_metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_metrics['f1'] > best_val_f1:\n",
    "        best_val_f1 = val_metrics['f1']\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_f1': best_val_f1\n",
    "        }, 'best_ip_gnn_model.pt')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\n‚ö†Ô∏è Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n‚úì Training complete! Best Val F1: {best_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502444ad",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98854d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load('best_ip_gnn_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"‚úì Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "\n",
    "# Evaluate on test edges\n",
    "test_metrics = evaluate_edges(model, data, test_edges)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä FINAL TEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"F1 Score:  {test_metrics['f1']:.4f}\")\n",
    "print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"Recall:    {test_metrics['recall']:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40df86f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data.x, data.edge_index, test_edges)\n",
    "    y_pred = out.argmax(dim=1).cpu().numpy()\n",
    "    y_true = data.edge_y[test_edge_mask].cpu().numpy()\n",
    "\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Benign', 'Attack']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Benign', 'Attack'],\n",
    "            yticklabels=['Benign', 'Attack'])\n",
    "plt.title('Confusion Matrix - IP GNN')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('ip_gnn_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa7e9a2",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Training History Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7528e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Metrics\n",
    "axes[1].plot(history['val_acc'], label='Val Accuracy', linewidth=2)\n",
    "axes[1].plot(history['val_f1'], label='Val F1', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Validation Metrics')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ip_gnn_training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749ba1e8",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Analyze IP-level Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb49e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze some IP statistics\n",
    "print(\"üìä IP-level Statistics:\\n\")\n",
    "\n",
    "# Top IPs by degree\n",
    "from collections import Counter\n",
    "\n",
    "ip_degrees = Counter()\n",
    "for i in range(edge_index.shape[1]):\n",
    "    src = edge_index[0, i].item()\n",
    "    dst = edge_index[1, i].item()\n",
    "    ip_degrees[src] += 1\n",
    "    ip_degrees[dst] += 1\n",
    "\n",
    "print(\"Top 10 IPs by degree (number of connections):\")\n",
    "for node_id, degree in ip_degrees.most_common(10):\n",
    "    ip = id_to_ip[node_id]\n",
    "    label = \"Attack\" if node_labels[node_id] == 1 else \"Benign\"\n",
    "    print(f\"  {ip:<20} degree: {degree:>6,}  label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea03fdd",
   "metadata": {},
   "source": [
    "## üì• Download Model\n",
    "\n",
    "B·∫°n c√≥ th·ªÉ download model ƒë√£ train v·ªÅ m√°y t·ª´ Kaggle Output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6d96d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: best_ip_gnn_model.pt\n",
    "print(\"‚úì Model saved at: best_ip_gnn_model.pt\")\n",
    "print(\"‚úì Confusion matrix: ip_gnn_confusion_matrix.png\")\n",
    "print(\"‚úì Training history: ip_gnn_training_history.png\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

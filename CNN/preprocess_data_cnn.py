"""
======================================================================================
TIá»€N Xá»¬ LÃ DATASET CICIDS2018 CHO MÃ” HÃŒNH CNN - PHÃT HIá»†N LÆ¯U LÆ¯á»¢NG Máº NG IOT Báº¤T THÆ¯á»œNG
======================================================================================

Script nÃ y thá»±c hiá»‡n cÃ¡c bÆ°á»›c tiá»n xá»­ lÃ½ dá»¯ liá»‡u:
1. Äá»c tá»«ng file CSV theo chunks Ä‘á»ƒ tá»‘i Æ°u bá»™ nhá»›
2. Loáº¡i bá» cÃ¡c cá»™t khÃ´ng cáº§n thiáº¿t (IP, Port, Timestamp, Flow ID)
3. Xá»­ lÃ½ missing values, NaN, Inf
4. Loáº¡i bá» cÃ¡c hÃ ng trÃ¹ng láº·p
5. Chuyá»ƒn Ä‘á»•i nhÃ£n sang dáº¡ng binary (Benign=0, Attack=1)
6. Chuáº©n hÃ³a dá»¯ liá»‡u báº±ng StandardScaler
7. LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ sang Ä‘á»‹nh dáº¡ng nhanh (parquet/npy)

CÃ³ thá»ƒ cháº¡y trÃªn cáº£ Kaggle vÃ  Local
"""

import os
import numpy as np
import pandas as pd
import pickle
import json
import gc
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# THÆ¯ VIá»†N CHUáº¨N HÃ“A VÃ€ Xá»¬ LÃ Dá»® LIá»†U
# ============================================================================
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split

# Kiá»ƒm tra mÃ´i trÆ°á»ng cháº¡y (Kaggle hoáº·c Local)
IS_KAGGLE = os.path.exists('/kaggle/input')

# Progress bar
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    print("âš ï¸  tqdm khÃ´ng cÃ³ sáºµn. CÃ i Ä‘áº·t báº±ng: pip install tqdm")
    tqdm = lambda x, **kwargs: x

# ============================================================================
# Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN
# ============================================================================
if IS_KAGGLE:
    # ÄÆ°á»ng dáº«n trÃªn Kaggle - thay Ä‘á»•i theo dataset cá»§a báº¡n
    DATA_DIR = "/kaggle/input/cicids2018"  # Thay Ä‘á»•i náº¿u tÃªn dataset khÃ¡c
    OUTPUT_DIR = "/kaggle/working/processed_data_cnn"
    print(" Äang cháº¡y trÃªn KAGGLE")
else:
    # ÄÆ°á»ng dáº«n Local
    DATA_DIR = r"D:\PROJECT\Machine Learning\IOT\CICIDS2018-CSV"
    OUTPUT_DIR = r"D:\PROJECT\Machine Learning\IOT\CNN\processed_data_cnn"
    print(" Äang cháº¡y trÃªn LOCAL")

# ============================================================================
# Cáº¤U HÃŒNH Xá»¬ LÃ Dá»® LIá»†U
# ============================================================================

# KÃ­ch thÆ°á»›c chunk khi Ä‘á»c CSV (Ä‘iá»u chá»‰nh theo RAM cá»§a mÃ¡y)
CHUNK_SIZE = 300000  # 300k rows má»—i chunk

# Random state Ä‘á»ƒ tÃ¡i táº¡o káº¿t quáº£
RANDOM_STATE = 42

# Loáº¡i scaler: 'standard' (StandardScaler) hoáº·c 'minmax' (MinMaxScaler)
SCALER_TYPE = 'standard'

# ============================================================================
# Cáº¤U HÃŒNH SAMPLE CÃ‚N Báº°NG
# ============================================================================
# Tá»•ng sá»‘ máº«u mong muá»‘n (train + val + test)
TOTAL_SAMPLES = 4000000  # 3 triá»‡u máº«u

# Tá»· lá»‡ pháº§n trÄƒm cho má»—i class
BENIGN_RATIO = 0.50  # 70% Benign = 2,100,000 máº«u
ATTACK_RATIO = 0.50  # 30% Attack = 900,000 máº«u

# TÃ­nh sá»‘ lÆ°á»£ng máº«u cho má»—i class
TARGET_BENIGN = int(TOTAL_SAMPLES * BENIGN_RATIO)  # 2,100,000
TARGET_ATTACK = int(TOTAL_SAMPLES * ATTACK_RATIO)  # 900,000

# ============================================================================
# DANH SÃCH CÃC Cá»˜T Cáº¦N LOáº I Bá»
# ============================================================================

# CÃ¡c cá»™t khÃ´ng cáº§n thiáº¿t cho viá»‡c huáº¥n luyá»‡n CNN
COLUMNS_TO_DROP = [
    # ThÃ´ng tin Ä‘á»‹nh danh - khÃ´ng mang tÃ­nh tá»•ng quÃ¡t
    'Flow ID',          # ID duy nháº¥t cho má»—i flow
    'Src IP',           # IP nguá»“n
    'Dst IP',           # IP Ä‘Ã­ch
    'Src Port',         # Port nguá»“n
    'Timestamp',        # Thá»i gian - khÃ´ng liÃªn quan Ä‘áº¿n pattern

    # CÃ¡c cá»™t flag khÃ´ng mang nhiá»u thÃ´ng tin
    # 'Bwd PSH Flags',    # Hiáº¿m khi cÃ³ giÃ¡ trá»‹ khÃ¡c 0
    # 'Bwd URG Flags',    # Hiáº¿m khi cÃ³ giÃ¡ trá»‹ khÃ¡c 0
    # 'Fwd URG Flags',    # Hiáº¿m khi cÃ³ giÃ¡ trá»‹ khÃ¡c 0
]

# Cá»™t nhÃ£n
LABEL_COLUMN = 'Label'

# ============================================================================
# CLASS Xá»¬ LÃ Dá»® LIá»†U CHO CNN
# ============================================================================

class CICIDS2018_CNN_Preprocessor:
    """
    Class xá»­ lÃ½ dá»¯ liá»‡u CICIDS2018 cho mÃ´ hÃ¬nh CNN phÃ¡t hiá»‡n báº¥t thÆ°á»ng

    CÃ¡c bÆ°á»›c xá»­ lÃ½:
    1. Äá»c dá»¯ liá»‡u theo chunks
    2. Loáº¡i bá» cá»™t khÃ´ng cáº§n thiáº¿t
    3. Xá»­ lÃ½ giÃ¡ trá»‹ thiáº¿u, NaN, Inf
    4. Loáº¡i bá» duplicate
    5. Chuyá»ƒn Ä‘á»•i nhÃ£n sang binary
    6. Chuáº©n hÃ³a features
    7. LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
    """

    def __init__(self, data_dir, output_dir, chunk_size=CHUNK_SIZE,
                 scaler_type=SCALER_TYPE, target_benign=TARGET_BENIGN,
                 target_attack=TARGET_ATTACK):
        """
        Khá»Ÿi táº¡o preprocessor

        Args:
            data_dir: ÄÆ°á»ng dáº«n thÆ° má»¥c chá»©a file CSV
            output_dir: ÄÆ°á»ng dáº«n thÆ° má»¥c lÆ°u káº¿t quáº£
            chunk_size: Sá»‘ dÃ²ng má»—i chunk khi Ä‘á»c CSV
            scaler_type: Loáº¡i scaler ('standard' hoáº·c 'minmax')
            target_benign: Sá»‘ lÆ°á»£ng máº«u Benign mong muá»‘n
            target_attack: Sá»‘ lÆ°á»£ng máº«u Attack mong muá»‘n
        """
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.chunk_size = chunk_size
        self.scaler_type = scaler_type
        self.target_benign = target_benign
        self.target_attack = target_attack

        # Khá»Ÿi táº¡o scaler
        if scaler_type == 'minmax':
            self.scaler = MinMaxScaler()
        else:
            self.scaler = StandardScaler()

        # Táº¡o thÆ° má»¥c output náº¿u chÆ°a cÃ³
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Thá»‘ng kÃª
        self.stats = {
            'total_rows_read': 0,
            'rows_after_cleaning': 0,
            'duplicates_removed': 0,
            'nan_inf_replaced': 0,
            'benign_count': 0,
            'attack_count': 0,
            'feature_count': 0,
            'processing_time': 0
        }

        # LÆ°u tÃªn cÃ¡c features
        self.feature_names = None

    def _get_csv_files(self):
        """Láº¥y danh sÃ¡ch cÃ¡c file CSV trong thÆ° má»¥c data"""
        csv_files = list(self.data_dir.glob("*_TrafficForML_CICFlowMeter.csv"))
        if not csv_files:
            # Thá»­ pattern khÃ¡c cho Kaggle
            csv_files = list(self.data_dir.glob("*.csv"))

        if not csv_files:
            raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file CSV trong {self.data_dir}")

        print(f"\nğŸ“‚ TÃ¬m tháº¥y {len(csv_files)} file CSV:")
        for f in sorted(csv_files):
            print(f"   - {f.name}")
        return sorted(csv_files)

    def _clean_column_names(self, df):
        """Chuáº©n hÃ³a tÃªn cá»™t (loáº¡i bá» khoáº£ng tráº¯ng thá»«a)"""
        df.columns = df.columns.str.strip()
        return df

    def _drop_unnecessary_columns(self, df):
        """Loáº¡i bá» cÃ¡c cá»™t khÃ´ng cáº§n thiáº¿t cho huáº¥n luyá»‡n"""
        columns_to_drop = [col for col in COLUMNS_TO_DROP if col in df.columns]

        if columns_to_drop:
            df = df.drop(columns=columns_to_drop)

        return df

    def _convert_to_numeric(self, df):
        """Chuyá»ƒn Ä‘á»•i cÃ¡c cá»™t vá» dáº¡ng sá»‘"""
        # Láº¥y táº¥t cáº£ cá»™t trá»« Label
        feature_cols = [col for col in df.columns if col != LABEL_COLUMN]

        for col in feature_cols:
            if df[col].dtype == 'object':
                df[col] = pd.to_numeric(df[col], errors='coerce')

        return df

    def _handle_nan_inf(self, df):
        """Xá»­ lÃ½ giÃ¡ trá»‹ NaN vÃ  Infinity"""
        feature_cols = [col for col in df.columns if col != LABEL_COLUMN]

        # Äáº¿m sá»‘ lÆ°á»£ng NaN vÃ  Inf trÆ°á»›c khi xá»­ lÃ½
        nan_count = df[feature_cols].isna().sum().sum()
        inf_count = np.isinf(df[feature_cols].select_dtypes(include=[np.number])).sum().sum()

        self.stats['nan_inf_replaced'] += nan_count + inf_count

        # Thay tháº¿ Infinity báº±ng NaN trÆ°á»›c
        df[feature_cols] = df[feature_cols].replace([np.inf, -np.inf], np.nan)

        # Thay tháº¿ NaN báº±ng 0 (hoáº·c cÃ³ thá»ƒ dÃ¹ng median/mean)
        df[feature_cols] = df[feature_cols].fillna(0)

        return df

    def _remove_duplicates(self, df):
        """Loáº¡i bá» cÃ¡c hÃ ng trÃ¹ng láº·p"""
        rows_before = len(df)
        df = df.drop_duplicates()
        rows_after = len(df)

        self.stats['duplicates_removed'] += (rows_before - rows_after)

        return df

    def _convert_to_binary_label(self, df):
        """
        Chuyá»ƒn Ä‘á»•i nhÃ£n sang dáº¡ng binary:
        - Benign -> 0 (lÆ°u lÆ°á»£ng bÃ¬nh thÆ°á»ng)
        - Táº¥t cáº£ cÃ¡c loáº¡i táº¥n cÃ´ng khÃ¡c -> 1 (lÆ°u lÆ°á»£ng báº¥t thÆ°á»ng)
        """
        if LABEL_COLUMN not in df.columns:
            raise ValueError(f"KhÃ´ng tÃ¬m tháº¥y cá»™t '{LABEL_COLUMN}' trong dá»¯ liá»‡u")

        # Chuáº©n hÃ³a nhÃ£n (loáº¡i bá» khoáº£ng tráº¯ng, lowercase)
        df[LABEL_COLUMN] = df[LABEL_COLUMN].astype(str).str.strip().str.lower()

        # Loáº¡i bá» cÃ¡c hÃ ng cÃ³ nhÃ£n lÃ  'label' (header bá»‹ láº«n vÃ o data)
        df = df[df[LABEL_COLUMN] != 'label']

        # Chuyá»ƒn Ä‘á»•i sang binary: Benign=0, Attack=1
        df['binary_label'] = (df[LABEL_COLUMN] != 'benign').astype(int)

        # Äáº¿m sá»‘ lÆ°á»£ng má»—i class
        benign_count = (df['binary_label'] == 0).sum()
        attack_count = (df['binary_label'] == 1).sum()

        self.stats['benign_count'] += benign_count
        self.stats['attack_count'] += attack_count

        # XÃ³a cá»™t Label gá»‘c, giá»¯ láº¡i binary_label
        df = df.drop(columns=[LABEL_COLUMN])

        return df

    def _process_single_file(self, csv_file):
        """
        Xá»­ lÃ½ má»™t file CSV theo chunks

        Args:
            csv_file: ÄÆ°á»ng dáº«n file CSV

        Returns:
            DataFrame Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½
        """
        print(f"\nğŸ“„ Äang xá»­ lÃ½: {csv_file.name}")

        processed_chunks = []
        chunk_iterator = pd.read_csv(csv_file, chunksize=self.chunk_size,
                                     low_memory=False, encoding='utf-8')

        # Progress bar cho chunks
        if TQDM_AVAILABLE:
            # Æ¯á»›c tÃ­nh sá»‘ chunks dá»±a trÃªn file size
            file_size = csv_file.stat().st_size
            estimated_chunks = max(1, file_size // (self.chunk_size * 500))  # Æ¯á»›c tÃ­nh
            chunk_iterator = tqdm(chunk_iterator, desc="   Chunks",
                                  total=estimated_chunks, unit="chunk")

        for chunk in chunk_iterator:
            self.stats['total_rows_read'] += len(chunk)

            # BÆ°á»›c 1: Chuáº©n hÃ³a tÃªn cá»™t
            chunk = self._clean_column_names(chunk)

            # BÆ°á»›c 2: Loáº¡i bá» cá»™t khÃ´ng cáº§n thiáº¿t
            chunk = self._drop_unnecessary_columns(chunk)

            # BÆ°á»›c 3: Chuyá»ƒn Ä‘á»•i sang dáº¡ng sá»‘
            chunk = self._convert_to_numeric(chunk)

            # BÆ°á»›c 4: Xá»­ lÃ½ NaN vÃ  Inf
            chunk = self._handle_nan_inf(chunk)

            # BÆ°á»›c 5: Chuyá»ƒn Ä‘á»•i nhÃ£n sang binary
            chunk = self._convert_to_binary_label(chunk)

            processed_chunks.append(chunk)

            # Giáº£i phÃ³ng bá»™ nhá»›
            gc.collect()

        # Gá»™p cÃ¡c chunks láº¡i
        if processed_chunks:
            df = pd.concat(processed_chunks, ignore_index=True)
            del processed_chunks
            gc.collect()
            return df

        return None

    def process_all_files(self):
        """
        Xá»­ lÃ½ táº¥t cáº£ cÃ¡c file CSV vÃ  gá»™p láº¡i

        Returns:
            DataFrame Ä‘Ã£ xá»­ lÃ½ hoÃ n chá»‰nh
        """
        start_time = datetime.now()
        print("\n" + "="*80)
        print("ğŸš€ Báº®T Äáº¦U Xá»¬ LÃ Dá»® LIá»†U CICIDS2018 CHO CNN")
        print("="*80)

        csv_files = self._get_csv_files()

        all_dataframes = []

        # Xá»­ lÃ½ tá»«ng file
        for csv_file in csv_files:
            df = self._process_single_file(csv_file)
            if df is not None:
                all_dataframes.append(df)
                print(f"   âœ… ÄÃ£ xá»­ lÃ½: {len(df):,} máº«u")

        # Gá»™p táº¥t cáº£ láº¡i
        print("\n" + "-"*80)
        print("ğŸ“Š ÄANG Gá»˜P VÃ€ Xá»¬ LÃ CUá»I CÃ™NG...")

        df_combined = pd.concat(all_dataframes, ignore_index=True)
        del all_dataframes
        gc.collect()

        print(f"   Tá»•ng sá»‘ máº«u sau khi gá»™p: {len(df_combined):,}")

        # Loáº¡i bá» duplicate trÃªn toÃ n bá»™ dataset
        print("   Äang loáº¡i bá» duplicate...")
        df_combined = self._remove_duplicates(df_combined)
        print(f"   Sá»‘ máº«u sau khi loáº¡i duplicate: {len(df_combined):,}")

        # Cáº­p nháº­t thá»‘ng kÃª
        self.stats['rows_after_cleaning'] = len(df_combined)
        self.stats['feature_count'] = len(df_combined.columns) - 1  # Trá»« cá»™t label

        # LÆ°u tÃªn features
        self.feature_names = [col for col in df_combined.columns if col != 'binary_label']

        end_time = datetime.now()
        self.stats['processing_time'] = (end_time - start_time).total_seconds()

        return df_combined

    def balanced_sample(self, df):
        """
        Sample dá»¯ liá»‡u vá»›i sá»‘ lÆ°á»£ng cÃ¢n báº±ng theo target Ä‘Ã£ Ä‘á»‹nh

        Láº¥y chÃ­nh xÃ¡c:
        - TARGET_BENIGN máº«u Benign (2,100,000)
        - TARGET_ATTACK máº«u Attack (900,000)

        Args:
            df: DataFrame Ä‘Ã£ clean

        Returns:
            DataFrame Ä‘Ã£ Ä‘Æ°á»£c sample cÃ¢n báº±ng
        """
        print("\n" + "="*80)
        print("âš–ï¸ ÄANG SAMPLE CÃ‚N Báº°NG Dá»® LIá»†U")
        print("="*80)

        # TÃ¡ch theo class
        df_benign = df[df['binary_label'] == 0]
        df_attack = df[df['binary_label'] == 1]

        n_benign = len(df_benign)
        n_attack = len(df_attack)

        print(f"\n   Dá»¯ liá»‡u gá»‘c (sau khi clean):")
        print(f"   - Benign: {n_benign:,}")
        print(f"   - Attack: {n_attack:,}")
        print(f"   - Tá»•ng: {n_benign + n_attack:,}")

        print(f"\n   Target mong muá»‘n:")
        print(f"   - Benign: {self.target_benign:,} ({BENIGN_RATIO*100:.0f}%)")
        print(f"   - Attack: {self.target_attack:,} ({ATTACK_RATIO*100:.0f}%)")
        print(f"   - Tá»•ng: {self.target_benign + self.target_attack:,}")

        # Kiá»ƒm tra vÃ  Ä‘iá»u chá»‰nh náº¿u khÃ´ng Ä‘á»§ máº«u
        actual_benign = min(self.target_benign, n_benign)
        actual_attack = min(self.target_attack, n_attack)

        if actual_benign < self.target_benign:
            print(f"\n   âš ï¸ KhÃ´ng Ä‘á»§ Benign! Chá»‰ cÃ³ {n_benign:,}, cáº§n {self.target_benign:,}")
        if actual_attack < self.target_attack:
            print(f"\n   âš ï¸ KhÃ´ng Ä‘á»§ Attack! Chá»‰ cÃ³ {n_attack:,}, cáº§n {self.target_attack:,}")

        # Random sample tá»« má»—i class
        print(f"\n   Äang sample...")
        df_benign_sampled = df_benign.sample(n=actual_benign, random_state=RANDOM_STATE)
        df_attack_sampled = df_attack.sample(n=actual_attack, random_state=RANDOM_STATE)

        # Gá»™p láº¡i vÃ  shuffle
        df_balanced = pd.concat([df_benign_sampled, df_attack_sampled], ignore_index=True)
        df_balanced = df_balanced.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)

        # Thá»‘ng kÃª káº¿t quáº£
        final_benign = (df_balanced['binary_label'] == 0).sum()
        final_attack = (df_balanced['binary_label'] == 1).sum()
        total = len(df_balanced)

        print(f"\n   âœ… Káº¿t quáº£ sau khi sample:")
        print(f"   - Benign: {final_benign:,} ({final_benign/total*100:.1f}%)")
        print(f"   - Attack: {final_attack:,} ({final_attack/total*100:.1f}%)")
        print(f"   - Tá»•ng: {total:,}")
        print(f"   - Tá»· lá»‡ Benign:Attack = {final_benign/final_attack:.2f}:1")

        # Cáº­p nháº­t stats
        self.stats['benign_count'] = final_benign
        self.stats['attack_count'] = final_attack
        self.stats['rows_after_cleaning'] = total

        return df_balanced

    def normalize_features(self, df):
        """
        Chuáº©n hÃ³a cÃ¡c features báº±ng scaler

        Args:
            df: DataFrame chá»©a features vÃ  label

        Returns:
            X_normalized: Features Ä‘Ã£ chuáº©n hÃ³a
            y: Labels
        """
        print("\nğŸ”„ ÄANG CHUáº¨N HÃ“A Dá»® LIá»†U...")

        # TÃ¡ch features vÃ  label
        X = df.drop(columns=['binary_label']).values
        y = df['binary_label'].values

        # Chuáº©n hÃ³a features
        X_normalized = self.scaler.fit_transform(X)

        print(f"   Scaler type: {self.scaler_type}")
        print(f"   Shape X: {X_normalized.shape}")
        print(f"   Shape y: {y.shape}")

        return X_normalized, y

    def reshape_for_cnn(self, X):
        """
        Reshape dá»¯ liá»‡u cho CNN 1D

        CNN 1D yÃªu cáº§u input shape: (samples, features, channels)
        Trong trÆ°á»ng há»£p nÃ y: (samples, n_features, 1)

        Args:
            X: Features Ä‘Ã£ chuáº©n hÃ³a, shape (samples, features)

        Returns:
            X_reshaped: Shape (samples, features, 1)
        """
        print("\nğŸ”„ ÄANG RESHAPE Dá»® LIá»†U CHO CNN...")

        X_reshaped = X.reshape(X.shape[0], X.shape[1], 1)

        print(f"   Shape sau reshape: {X_reshaped.shape}")

        return X_reshaped

    def split_data(self, X, y, test_size=0.2, val_size=0.1):
        """
        Chia dá»¯ liá»‡u thÃ nh train/val/test sets

        Args:
            X: Features
            y: Labels
            test_size: Tá»· lá»‡ test set
            val_size: Tá»· lá»‡ validation set (tá»« train)

        Returns:
            X_train, X_val, X_test, y_train, y_val, y_test
        """
        print("\nğŸ“Š ÄANG CHIA Dá»® LIá»†U TRAIN/VAL/TEST...")

        # Chia train+val / test
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=test_size, random_state=RANDOM_STATE, stratify=y
        )

        # Chia train / val
        val_ratio = val_size / (1 - test_size)
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=val_ratio, random_state=RANDOM_STATE, stratify=y_temp
        )

        print(f"   Train set: {X_train.shape[0]:,} máº«u")
        print(f"   Val set:   {X_val.shape[0]:,} máº«u")
        print(f"   Test set:  {X_test.shape[0]:,} máº«u")

        # Thá»‘ng kÃª phÃ¢n bá»‘ class
        print(f"\n   PhÃ¢n bá»‘ Train - Benign: {(y_train==0).sum():,}, Attack: {(y_train==1).sum():,}")
        print(f"   PhÃ¢n bá»‘ Val   - Benign: {(y_val==0).sum():,}, Attack: {(y_val==1).sum():,}")
        print(f"   PhÃ¢n bá»‘ Test  - Benign: {(y_test==0).sum():,}, Attack: {(y_test==1).sum():,}")

        return X_train, X_val, X_test, y_train, y_val, y_test


    def save_processed_data(self, X_train, X_val, X_test, y_train, y_val, y_test):
        """
        LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ sang Ä‘á»‹nh dáº¡ng nhanh

        LÆ°u thÃ nh cÃ¡c file:
        - X_train.npy, X_val.npy, X_test.npy
        - y_train.npy, y_val.npy, y_test.npy
        - scaler.pkl
        - metadata.json
        """
        print("\n ÄANG LÆ¯U Dá»® LIá»†U ÄÃƒ Xá»¬ LÃ...")

        # LÆ°u numpy arrays
        np.save(self.output_dir / 'X_train.npy', X_train)
        np.save(self.output_dir / 'X_val.npy', X_val)
        np.save(self.output_dir / 'X_test.npy', X_test)
        np.save(self.output_dir / 'y_train.npy', y_train)
        np.save(self.output_dir / 'y_val.npy', y_val)
        np.save(self.output_dir / 'y_test.npy', y_test)

        print(f"   âœ… ÄÃ£ lÆ°u X_train.npy: {X_train.shape}")
        print(f"   âœ… ÄÃ£ lÆ°u X_val.npy: {X_val.shape}")
        print(f"   âœ… ÄÃ£ lÆ°u X_test.npy: {X_test.shape}")
        print(f"   âœ… ÄÃ£ lÆ°u y_train.npy: {y_train.shape}")
        print(f"   âœ… ÄÃ£ lÆ°u y_val.npy: {y_val.shape}")
        print(f"   âœ… ÄÃ£ lÆ°u y_test.npy: {y_test.shape}")

        # LÆ°u scaler
        with open(self.output_dir / 'scaler.pkl', 'wb') as f:
            pickle.dump(self.scaler, f)
        print(f"   âœ… ÄÃ£ lÆ°u scaler.pkl")

        # LÆ°u feature names
        with open(self.output_dir / 'feature_names.txt', 'w') as f:
            for name in self.feature_names:
                f.write(name + '\n')
        print(f"   âœ… ÄÃ£ lÆ°u feature_names.txt")

        # Chuyá»ƒn Ä‘á»•i stats sang kiá»ƒu Python native (Ä‘á»ƒ trÃ¡nh lá»—i JSON vá»›i numpy.int64)
        stats_native = {}
        for key, value in self.stats.items():
            if hasattr(value, 'item'):  # Kiá»ƒm tra náº¿u lÃ  numpy type
                stats_native[key] = value.item()
            elif isinstance(value, (np.integer, np.floating)):
                stats_native[key] = int(value) if isinstance(value, np.integer) else float(value)
            else:
                stats_native[key] = value

        # LÆ°u metadata
        metadata = {
            'n_features': len(self.feature_names),
            'feature_names': self.feature_names,
            'train_samples': int(X_train.shape[0]),
            'val_samples': int(X_val.shape[0]),
            'test_samples': int(X_test.shape[0]),
            'input_shape': [int(x) for x in X_train.shape[1:]],
            'scaler_type': self.scaler_type,
            'stats': stats_native,
            'created_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

        with open(self.output_dir / 'metadata.json', 'w') as f:
            json.dump(metadata, f, indent=4)
        print(f"   âœ… ÄÃ£ lÆ°u metadata.json")

        print(f"\nğŸ“ Táº¥t cáº£ file Ä‘Æ°á»£c lÆ°u táº¡i: {self.output_dir}")

    def print_summary(self):
        """In tÃ³m táº¯t quÃ¡ trÃ¬nh xá»­ lÃ½"""
        print("\n" + "="*80)
        print("ğŸ“Š TÃ“M Táº®T Xá»¬ LÃ Dá»® LIá»†U")
        print("="*80)
        print(f"   Tá»•ng sá»‘ dÃ²ng Ä‘á»c Ä‘Æ°á»£c:     {self.stats['total_rows_read']:,}")
        print(f"   Sá»‘ dÃ²ng sau khi xá»­ lÃ½:     {self.stats['rows_after_cleaning']:,}")
        print(f"   Sá»‘ duplicate Ä‘Ã£ loáº¡i:      {self.stats['duplicates_removed']:,}")
        print(f"   Sá»‘ NaN/Inf Ä‘Ã£ thay tháº¿:    {self.stats['nan_inf_replaced']:,}")
        print(f"   Sá»‘ features:               {self.stats['feature_count']}")
        print(f"   Sá»‘ máº«u Benign:             {self.stats['benign_count']:,}")
        print(f"   Sá»‘ máº«u Attack:             {self.stats['attack_count']:,}")
        print(f"   Thá»i gian xá»­ lÃ½:           {self.stats['processing_time']:.2f} giÃ¢y")
        print("="*80)


def main():
    """HÃ m chÃ­nh Ä‘á»ƒ cháº¡y preprocessing"""

    print("\n" + "="*80)
    print("ğŸ”§ TIá»€N Xá»¬ LÃ Dá»® LIá»†U CICIDS2018 CHO MÃ” HÃŒNH CNN")
    print("   PhÃ¡t hiá»‡n lÆ°u lÆ°á»£ng máº¡ng IoT báº¥t thÆ°á»ng")
    print("="*80)

    print(f"\nğŸ“‹ Cáº¤U HÃŒNH:")
    print(f"   - Tá»•ng máº«u mong muá»‘n: {TOTAL_SAMPLES:,}")
    print(f"   - Benign: {TARGET_BENIGN:,} ({BENIGN_RATIO*100:.0f}%)")
    print(f"   - Attack: {TARGET_ATTACK:,} ({ATTACK_RATIO*100:.0f}%)")

    # Khá»Ÿi táº¡o preprocessor
    preprocessor = CICIDS2018_CNN_Preprocessor(
        data_dir=DATA_DIR,
        output_dir=OUTPUT_DIR,
        chunk_size=CHUNK_SIZE,
        scaler_type=SCALER_TYPE,
        target_benign=TARGET_BENIGN,
        target_attack=TARGET_ATTACK
    )

    # BÆ°á»›c 1: Xá»­ lÃ½ táº¥t cáº£ cÃ¡c file CSV (clean data)
    df = preprocessor.process_all_files()

    # BÆ°á»›c 2: SAMPLE CÃ‚N Báº°NG TRÆ¯á»šC KHI CHIA
    # Äiá»u nÃ y Ä‘áº£m báº£o train/val/test Ä‘á»u cÃ³ tá»· lá»‡ 70-30
    df = preprocessor.balanced_sample(df)

    # BÆ°á»›c 3: Chuáº©n hÃ³a features
    X, y = preprocessor.normalize_features(df)

    # Giáº£i phÃ³ng bá»™ nhá»› cá»§a DataFrame
    del df
    gc.collect()

    # BÆ°á»›c 4: Reshape cho CNN
    X = preprocessor.reshape_for_cnn(X)

    # BÆ°á»›c 5: Chia dá»¯ liá»‡u (stratify Ä‘á»ƒ giá»¯ tá»· lá»‡ 70-30 trong táº¥t cáº£ cÃ¡c táº­p)
    X_train, X_val, X_test, y_train, y_val, y_test = preprocessor.split_data(X, y)

    # Giáº£i phÃ³ng bá»™ nhá»›
    del X, y
    gc.collect()

    # BÆ°á»›c 6: LÆ°u dá»¯ liá»‡u
    preprocessor.save_processed_data(X_train, X_val, X_test, y_train, y_val, y_test)

    # In tÃ³m táº¯t
    preprocessor.print_summary()

    print("\nâœ… HOÃ€N THÃ€NH! Dá»¯ liá»‡u Ä‘Ã£ sáºµn sÃ ng cho viá»‡c huáº¥n luyá»‡n CNN.")

    return preprocessor


if __name__ == "__main__":
    preprocessor = main()


{
 "cells": [
}
 "nbformat_minor": 4
 "nbformat": 4,
 },
  }
   "version": "3.10.0"
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "name": "python",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   },
    "version": 3
    "name": "ipython",
   "codemirror_mode": {
  "language_info": {
  },
   "name": "python3"
   "language": "python",
   "display_name": "Python 3",
  "kernelspec": {
 "metadata": {
 ],
  }
   ]
    "print(f\"   - results_summary.json\")"
    "print(f\"   - roc_curve.png\")\n",
    "print(f\"   - confusion_matrix.png\")\n",
    "print(f\"   - training_history.png\")\n",
    "print(f\"   - final_model.keras\")\n",
    "print(f\"   - best_model.keras\")\n",
    "print(f\"\\nüìÅ K·∫øt qu·∫£ ƒë∆∞·ª£c l∆∞u t·∫°i: {results_dir}\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ HO√ÄN TH√ÄNH!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "print(f\"‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£: {results_dir / 'results_summary.json'}\")"
    "    json.dump(results_summary, f, indent=4)\n",
    "with open(results_dir / 'results_summary.json', 'w') as f:\n",
    "\n",
    "}\n",
    "    'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    },\n",
    "        'false_alarm_rate': float(false_alarm_rate)\n",
    "        'detection_rate': float(detection_rate),\n",
    "        'auc_roc': float(auc),\n",
    "        'f1_score': float(f1),\n",
    "        'recall': float(recall),\n",
    "        'precision': float(precision),\n",
    "        'accuracy': float(accuracy),\n",
    "    'results': {\n",
    "    'config': CNN_CONFIG,\n",
    "    'training_time_minutes': training_time / 60,\n",
    "    'dataset': 'CICIDS2018',\n",
    "    'model_name': 'CNN_1D_IDS',\n",
    "results_summary = {\n",
    "# L∆∞u k·∫øt qu·∫£\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u model: {results_dir / 'final_model.keras'}\")\n",
    "model.save(results_dir / 'final_model.keras')\n",
    "# L∆∞u model cu·ªëi c√πng\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "## 9. L∆∞u m√¥ h√¨nh v√† k·∫øt qu·∫£"
   "source": [
   "metadata": {},
   "cell_type": "markdown",
  {
  },
   ]
    "plt.show()"
    "plt.savefig(results_dir / 'roc_curve.png', dpi=150)\n",
    "plt.tight_layout()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=1)\n",
    "ax.plot(fpr, tpr, color='blue', linewidth=2, label=f'ROC (AUC = {auc:.4f})')\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "# ROC Curve\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "plt.show()"
    "plt.savefig(results_dir / 'confusion_matrix.png', dpi=150)\n",
    "plt.tight_layout()\n",
    "ax.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Actual', fontsize=12)\n",
    "ax.set_xlabel('Predicted', fontsize=12)\n",
    "            yticklabels=['Benign', 'Attack'])\n",
    "            xticklabels=['Benign', 'Attack'],\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "# Confusion Matrix heatmap\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "plt.show()"
    "plt.savefig(results_dir / 'training_history.png', dpi=150)\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Training History', fontsize=16, fontweight='bold')\n",
    "\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_title('AUC-ROC', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].plot(history.history['val_auc'], label='Val', linewidth=2)\n",
    "axes[1, 1].plot(history.history['auc'], label='Train', linewidth=2)\n",
    "# AUC\n",
    "\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_title('Precision & Recall', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].plot(history.history['val_recall'], label='Val Recall', linewidth=2)\n",
    "axes[1, 0].plot(history.history['recall'], label='Train Recall', linewidth=2)\n",
    "axes[1, 0].plot(history.history['val_precision'], label='Val Precision', linewidth=2)\n",
    "axes[1, 0].plot(history.history['precision'], label='Train Precision', linewidth=2)\n",
    "# Precision & Recall\n",
    "\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_title('Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].plot(history.history['val_accuracy'], label='Val', linewidth=2)\n",
    "axes[0, 1].plot(history.history['accuracy'], label='Train', linewidth=2)\n",
    "# Accuracy\n",
    "\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_title('Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Val', linewidth=2)\n",
    "axes[0, 0].plot(history.history['loss'], label='Train', linewidth=2)\n",
    "# Loss\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "# Training history\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "## 8. V·∫Ω bi·ªÉu ƒë·ªì"
   "source": [
   "metadata": {},
   "cell_type": "markdown",
  {
  },
   ]
    "print(f\"   False Alarm Rate: {false_alarm_rate:.4f}\")"
    "print(f\"\\n   Detection Rate: {detection_rate:.4f}\")\n",
    "false_alarm_rate = fp / (fp + tn)\n",
    "detection_rate = tp / (tp + fn)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "# Detection rate v√† False alarm rate\n",
    "\n",
    "print(f\"       Attack {cm[1,0]:6d}  {cm[1,1]:6d}\")\n",
    "print(f\"Actual Benign {cm[0,0]:6d}  {cm[0,1]:6d}\")\n",
    "print(f\"            Benign  Attack\")\n",
    "print(f\"              Predicted\")\n",
    "print(\"-\"*60)\n",
    "print(\"CONFUSION MATRIX:\")\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "# Confusion matrix\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "print(classification_report(y_test, y_pred, target_names=['Benign', 'Attack']))"
    "print(\"-\"*60)\n",
    "print(\"CLASSIFICATION REPORT:\")\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "# Classification report\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "print(f\"   AUC-ROC:   {auc:.4f}\")"
    "print(f\"   F1-Score:  {f1:.4f}\")\n",
    "print(f\"   Recall:    {recall:.4f}\")\n",
    "print(f\"   Precision: {precision:.4f}\")\n",
    "print(f\"   Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"\\nüìà K·∫æT QU·∫¢:\")\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred_prob)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# C√°c metrics\n",
    "\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "y_pred_prob = model.predict(X_test, verbose=0)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä ƒê√ÅNH GI√Å TR√äN TEST SET\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "# D·ª± ƒëo√°n tr√™n test set\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "## 7. ƒê√°nh gi√° m√¥ h√¨nh"
   "source": [
   "metadata": {},
   "cell_type": "markdown",
  {
  },
   ]
    "print(f\"\\n‚è±Ô∏è Th·ªùi gian hu·∫•n luy·ªán: {training_time/60:.2f} ph√∫t\")"
    "training_time = (end_time - start_time).total_seconds()\n",
    "end_time = datetime.now()\n",
    "\n",
    ")\n",
    "    verbose=1\n",
    "    callbacks=callback_list,\n",
    "    class_weight=class_weight,\n",
    "    batch_size=CNN_CONFIG['batch_size'],\n",
    "    epochs=CNN_CONFIG['epochs'],\n",
    "    validation_data=(X_val, y_val),\n",
    "    X_train, y_train,\n",
    "history = model.fit(\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üöÄ B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "# Hu·∫•n luy·ªán\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "]"
    "    )\n",
    "        append=False\n",
    "        separator=',',\n",
    "        filename=str(results_dir / 'training_history.csv'),\n",
    "    callbacks.CSVLogger(\n",
    "    ),\n",
    "        verbose=1\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        monitor='val_auc',\n",
    "        filepath=str(results_dir / 'best_model.keras'),\n",
    "    callbacks.ModelCheckpoint(\n",
    "    ),\n",
    "        verbose=1\n",
    "        min_lr=1e-7,\n",
    "        patience=CNN_CONFIG['reduce_lr_patience'],\n",
    "        factor=0.5,\n",
    "        monitor='val_loss',\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "    ),\n",
    "        verbose=1\n",
    "        restore_best_weights=True,\n",
    "        patience=CNN_CONFIG['early_stopping_patience'],\n",
    "        monitor='val_loss',\n",
    "    callbacks.EarlyStopping(\n",
    "callback_list = [\n",
    "# Callbacks\n",
    "\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_dir = Path(RESULTS_DIR)\n",
    "# T·∫°o th∆∞ m·ª•c results\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "    print(f\"Class weights: {class_weight}\")"
    "    class_weight = compute_class_weights(y_train)\n",
    "if CNN_CONFIG['use_class_weight']:\n",
    "class_weight = None\n",
    "\n",
    "    }\n",
    "        1: total / (2 * n_attack)\n",
    "        0: total / (2 * n_benign),\n",
    "    return {\n",
    "    total = len(y)\n",
    "    n_attack = (y == 1).sum()\n",
    "    n_benign = (y == 0).sum()\n",
    "def compute_class_weights(y):\n",
    "# T√≠nh class weights\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "## 6. Hu·∫•n luy·ªán m√¥ h√¨nh"
   "source": [
   "metadata": {},
   "cell_type": "markdown",
  {
  },
   ]
    "model.summary()"
    "model = build_cnn_model(input_shape, CNN_CONFIG)\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "# X√¢y d·ª±ng model\n",
    "\n",
    "    return model\n",
    "    \n",
    "    )\n",
    "                 keras.metrics.AUC(name='auc')]\n",
    "                 keras.metrics.Recall(name='recall'),\n",
    "                 keras.metrics.Precision(name='precision'),\n",
    "        metrics=['accuracy',\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "    model.compile(\n",
    "    optimizer = Adam(learning_rate=config['learning_rate'])\n",
    "    # Compile\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='sigmoid', name='output'))\n",
    "    # Output\n",
    "    \n",
    "        model.add(layers.Dropout(config['dropout_rate'], name=f'dropout_dense_{i+1}'))\n",
    "        model.add(layers.Activation('relu', name=f'relu_dense_{i+1}'))\n",
    "        model.add(layers.BatchNormalization(name=f'bn_dense_{i+1}'))\n",
    "        model.add(layers.Dense(units, name=f'dense_{i+1}'))\n",
    "    for i, units in enumerate(config['dense_units']):\n",
    "    # Dense layers\n",
    "    \n",
    "    model.add(layers.Flatten(name='flatten'))\n",
    "    # Flatten\n",
    "    \n",
    "        model.add(layers.Dropout(config['dropout_rate'], name=f'dropout_conv_{i+1}'))\n",
    "                                       padding='same', name=f'maxpool_{i+1}'))\n",
    "        model.add(layers.MaxPooling1D(pool_size=config['pool_size'], \n",
    "        model.add(layers.Activation('relu', name=f'relu_{i+1}'))\n",
    "        model.add(layers.BatchNormalization(name=f'bn_{i+1}'))\n",
    "                                padding='same', name=f'conv1d_{i+1}'))\n",
    "        model.add(layers.Conv1D(filters=filters, kernel_size=config['kernel_size'], \n",
    "    for i, filters in enumerate(config['conv_filters']):\n",
    "    # Convolutional blocks\n",
    "    \n",
    "    model.add(layers.InputLayer(input_shape=input_shape))\n",
    "    model = models.Sequential(name=\"CNN_IDS\")\n",
    "    \"\"\"X√¢y d·ª±ng m√¥ h√¨nh CNN 1D\"\"\"\n",
    "def build_cnn_model(input_shape, config):\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "}"
    "    'use_class_weight': True,\n",
    "    'reduce_lr_patience': 5,\n",
    "    'early_stopping_patience': 10,\n",
    "    'learning_rate': 0.001,\n",
    "    'epochs': 50,\n",
    "    'batch_size': 256,\n",
    "    'dropout_rate': 0.3,\n",
    "    'dense_units': [128, 64],\n",
    "    'pool_size': 2,\n",
    "    'kernel_size': 3,\n",
    "    'conv_filters': [64, 128, 256],\n",
    "CNN_CONFIG = {\n",
    "# C·∫•u h√¨nh m√¥ h√¨nh\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "## 5. X√¢y d·ª±ng m√¥ h√¨nh CNN"
   "source": [
   "metadata": {},
   "cell_type": "markdown",
  {
  },
   ]
    "print(f\"‚úÖ ƒê√£ l∆∞u t·∫°i: {output_dir}\")"
    "\n",
    "        f.write(name + '\\n')\n",
    "    for name in feature_names:\n",
    "with open(output_dir / 'feature_names.txt', 'w') as f:\n",
    "\n",
    "    pickle.dump(scaler, f)\n",
    "with open(output_dir / 'scaler.pkl', 'wb') as f:\n",
    "\n",
    "np.save(output_dir / 'y_test.npy', y_test)\n",
    "np.save(output_dir / 'y_val.npy', y_val)\n",
    "np.save(output_dir / 'y_train.npy', y_train)\n",
    "np.save(output_dir / 'X_test.npy', X_test)\n",
    "np.save(output_dir / 'X_val.npy', X_val)\n",
    "np.save(output_dir / 'X_train.npy', X_train)\n",
    "print(\"\\nüíæ ƒêang l∆∞u d·ªØ li·ªáu...\")\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_dir = Path(OUTPUT_DIR)\n",
    "# L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "print(f\"   Test  - Benign: {(y_test==0).sum():,}, Attack: {(y_test==1).sum():,}\")"
    "print(f\"   Val   - Benign: {(y_val==0).sum():,}, Attack: {(y_val==1).sum():,}\")\n",
    "print(f\"\\n   Train - Benign: {(y_train==0).sum():,}, Attack: {(y_train==1).sum():,}\")\n",
    "\n",
    "print(f\"   Test:  {X_test.shape[0]:,} m·∫´u\")\n",
    "print(f\"   Val:   {X_val.shape[0]:,} m·∫´u\")\n",
    "print(f\"   Train: {X_train.shape[0]:,} m·∫´u\")\n",
    "\n",
    "gc.collect()\n",
    "del X_temp, y_temp, X, y\n",
    "\n",
    ")\n",
    "    X_temp, y_temp, test_size=0.125, random_state=RANDOM_STATE, stratify=y_temp\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "# Chia train / val (87.5/12.5 = 70/10 overall)\n",
    "\n",
    ")\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "# Chia train+val / test (80/20)\n",
    "\n",
    "print(\"\\nüìä ƒêang chia d·ªØ li·ªáu...\")\n",
    "# Chia d·ªØ li·ªáu train/val/test\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "print(f\"X shape sau reshape: {X.shape}\")"
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "print(\"\\nüîÑ ƒêang reshape cho CNN...\")\n",
    "# Reshape cho CNN 1D: (samples, features, 1)\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "print(\"‚úÖ Chu·∫©n h√≥a xong!\")"
    "X = scaler.fit_transform(X)\n",
    "scaler = StandardScaler()\n",
    "print(\"\\nüîÑ ƒêang chu·∫©n h√≥a d·ªØ li·ªáu...\")\n",
    "# Chu·∫©n h√≥a features\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "gc.collect()"
    "del df_combined\n",
    "# Gi·∫£i ph√≥ng b·ªô nh·ªõ\n",
    "\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "\n",
    "y = df_combined['binary_label'].values\n",
    "X = df_combined.drop(columns=['binary_label']).values\n",
    "# T√°ch features v√† labels\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "## 4. Chu·∫©n h√≥a v√† chia d·ªØ li·ªáu"
   "source": [
   "metadata": {},
   "cell_type": "markdown",
  {
  },
   ]
    "    print(f\"   {i:2d}. {name}\")"
    "for i, name in enumerate(feature_names, 1):\n",
    "print(\"C√°c features:\")\n",
    "print(f\"\\nüìã S·ªë features: {len(feature_names)}\")\n",
    "feature_names = [col for col in df_combined.columns if col != 'binary_label']\n",
    "# L∆∞u t√™n features\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "print(f\"   T·ª∑ l·ªá Attack: {label_counts.get(1, 0) / len(df_combined) * 100:.2f}%\")"
    "print(f\"   Attack (1): {label_counts.get(1, 0):,}\")\n",
    "print(f\"   Benign (0): {label_counts.get(0, 0):,}\")\n",
    "label_counts = df_combined['binary_label'].value_counts()\n",
    "print(\"\\nüìä PH√ÇN B·ªê D·ªÆ LI·ªÜU:\")\n",
    "# Xem ph√¢n b·ªë d·ªØ li·ªáu\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "    df_combined = df_combined.reset_index(drop=True)"
    "    df_combined = df_combined.sample(n=SAMPLE_SIZE, random_state=RANDOM_STATE)\n",
    "    print(f\"\\nüìâ ƒêang l·∫•y m·∫´u {SAMPLE_SIZE:,}...\")\n",
    "if SAMPLE_SIZE is not None and len(df_combined) > SAMPLE_SIZE:\n",
    "# L·∫•y sample n·∫øu c·∫ßn\n",
    "\n",
    "print(f\"   C√≤n l·∫°i: {rows_after:,} m·∫´u\")\n",
    "print(f\"   ƒê√£ lo·∫°i b·ªè {rows_before - rows_after:,} duplicate\")\n",
    "rows_after = len(df_combined)\n",
    "df_combined = df_combined.drop_duplicates()\n",
    "rows_before = len(df_combined)\n",
    "print(\"\\nüßπ ƒêang lo·∫°i b·ªè duplicate...\")\n",
    "# Lo·∫°i b·ªè duplicate\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "print(f\"T·ªïng s·ªë m·∫´u: {len(df_combined):,}\")"
    "\n",
    "gc.collect()\n",
    "del all_dataframes\n",
    "df_combined = pd.concat(all_dataframes, ignore_index=True)\n",
    "print(\"\\nüìä ƒêang g·ªôp d·ªØ li·ªáu...\")\n",
    "# G·ªôp t·∫•t c·∫£\n",
    "\n",
    "        print(f\"   ‚úÖ ƒê√£ x·ª≠ l√Ω: {len(df):,} m·∫´u\")\n",
    "        total_rows += len(df)\n",
    "        all_dataframes.append(df)\n",
    "    if df is not None:\n",
    "    df = process_single_file(csv_file)\n",
    "for csv_file in sorted(csv_files):\n",
    "\n",
    "total_rows = 0\n",
    "all_dataframes = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üöÄ B·∫ÆT ƒê·∫¶U X·ª¨ L√ù D·ªÆ LI·ªÜU\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "# ============================================================================\n",
    "# X·ª¨ L√ù T·∫§T C·∫¢ C√ÅC FILE\n",
    "# ============================================================================\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "    return None"
    "        return df\n",
    "        gc.collect()\n",
    "        del processed_chunks\n",
    "        df = pd.concat(processed_chunks, ignore_index=True)\n",
    "    if processed_chunks:\n",
    "    \n",
    "        gc.collect()\n",
    "        processed_chunks.append(chunk)\n",
    "        chunk = convert_to_binary_label(chunk)\n",
    "        chunk = handle_nan_inf(chunk)\n",
    "        chunk = convert_to_numeric(chunk)\n",
    "        chunk = drop_unnecessary_columns(chunk)\n",
    "        chunk = clean_column_names(chunk)\n",
    "    for chunk in tqdm(chunk_iterator, desc=\"   Chunks\"):\n",
    "    \n",
    "    chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size, low_memory=False)\n",
    "    processed_chunks = []\n",
    "    \n",
    "    print(f\"\\nüìÑ ƒêang x·ª≠ l√Ω: {csv_file.name}\")\n",
    "    \"\"\"X·ª≠ l√Ω m·ªôt file CSV theo chunks\"\"\"\n",
    "def process_single_file(csv_file, chunk_size=CHUNK_SIZE):\n",
    "\n",
    "    return df\n",
    "    df = df.drop(columns=[LABEL_COLUMN])\n",
    "    df['binary_label'] = (df[LABEL_COLUMN] != 'benign').astype(int)\n",
    "    # Binary: Benign=0, Attack=1\n",
    "    df = df[df[LABEL_COLUMN] != 'label']\n",
    "    # Lo·∫°i b·ªè c√°c h√†ng c√≥ nh√£n l√† 'label' (header b·ªã l·∫´n)\n",
    "    df[LABEL_COLUMN] = df[LABEL_COLUMN].astype(str).str.strip().str.lower()\n",
    "    \"\"\"Chuy·ªÉn ƒë·ªïi nh√£n sang binary\"\"\"\n",
    "def convert_to_binary_label(df):\n",
    "\n",
    "    return df\n",
    "    df[feature_cols] = df[feature_cols].fillna(0)\n",
    "    df[feature_cols] = df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    feature_cols = [col for col in df.columns if col != LABEL_COLUMN]\n",
    "    \"\"\"X·ª≠ l√Ω NaN v√† Infinity\"\"\"\n",
    "def handle_nan_inf(df):\n",
    "\n",
    "    return df\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if df[col].dtype == 'object':\n",
    "    for col in feature_cols:\n",
    "    feature_cols = [col for col in df.columns if col != LABEL_COLUMN]\n",
    "    \"\"\"Chuy·ªÉn ƒë·ªïi c√°c c·ªôt v·ªÅ d·∫°ng s·ªë\"\"\"\n",
    "def convert_to_numeric(df):\n",
    "\n",
    "    return df\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "    if cols_to_drop:\n",
    "    cols_to_drop = [col for col in COLUMNS_TO_DROP if col in df.columns]\n",
    "    \"\"\"Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn thi·∫øt\"\"\"\n",
    "def drop_unnecessary_columns(df):\n",
    "\n",
    "    return df\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \"\"\"Chu·∫©n h√≥a t√™n c·ªôt (lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a)\"\"\"\n",
    "def clean_column_names(df):\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "## 3. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu"
   "source": [
   "metadata": {},
   "cell_type": "markdown",
  {
  },
   ]
    "display(df_sample)"
    "print(f\"S·ªë c·ªôt: {len(df_sample.columns)}\")\n",
    "print(f\"\\nüìã Sample t·ª´ {sample_file.name}:\")\n",
    "df_sample = pd.read_csv(sample_file, nrows=5)\n",
    "sample_file = sorted(csv_files)[0]\n",
    "# Xem sample c·ªßa m·ªôt file\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "    print(f\"   - {f.name} ({size_mb:.1f} MB)\")"
    "    size_mb = f.stat().st_size / (1024 * 1024)\n",
    "for f in sorted(csv_files):\n",
    "print(f\"üìÑ T√¨m th·∫•y {len(csv_files)} file CSV:\")\n",
    "print(f\"üìÇ Th∆∞ m·ª•c: {DATA_DIR}\")\n",
    "\n",
    "csv_files = list(data_path.glob(\"*.csv\"))\n",
    "data_path = Path(DATA_DIR)\n",
    "# Li·ªát k√™ c√°c file CSV\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "## 2. Xem c·∫•u tr√∫c d·ªØ li·ªáu"
   "source": [
   "metadata": {},
   "cell_type": "markdown",
  {
  },
   ]
    "LABEL_COLUMN = 'Label'"
    "\n",
    "]\n",
    "    'Bwd PSH Flags', 'Bwd URG Flags', 'Fwd URG Flags'\n",
    "    'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg',\n",
    "    'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg',\n",
    "    'Flow ID', 'Src IP', 'Dst IP', 'Src Port', 'Timestamp',\n",
    "COLUMNS_TO_DROP = [\n",
    "# C√°c c·ªôt c·∫ßn lo·∫°i b·ªè\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "SAMPLE_SIZE = None   # None = to√†n b·ªô, ho·∫∑c s·ªë ƒë·ªÉ l·∫•y m·∫´u (v√≠ d·ª•: 500000)\n",
    "CHUNK_SIZE = 100000  # S·ªë d√≤ng m·ªói chunk\n",
    "# C·∫•u h√¨nh x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "\n",
    "    print(\"üíª ƒêang ch·∫°y tr√™n LOCAL\")\n",
    "    RESULTS_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CNN\\results\"\n",
    "    OUTPUT_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CNN\\processed_data_cnn\"\n",
    "    DATA_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CICIDS2018-CSV\"\n",
    "    # ƒê∆∞·ªùng d·∫´n Local\n",
    "else:\n",
    "    print(\"üåê ƒêang ch·∫°y tr√™n KAGGLE\")\n",
    "    RESULTS_DIR = \"/kaggle/working/cnn_results\"\n",
    "    OUTPUT_DIR = \"/kaggle/working/processed_data_cnn\"\n",
    "    DATA_DIR = \"/kaggle/input/cicids2018\"  # Thay ƒë·ªïi n·∫øu c·∫ßn\n",
    "    # ƒê∆∞·ªùng d·∫´n tr√™n Kaggle - thay ƒë·ªïi theo t√™n dataset c·ªßa b·∫°n\n",
    "if IS_KAGGLE:\n",
    "\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
    "# Ki·ªÉm tra m√¥i tr∆∞·ªùng (Kaggle ho·∫∑c Local)\n",
    "\n",
    "# ============================================================================\n",
    "# C·∫§U H√åNH\n",
    "# ============================================================================\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "# Progress bar\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualization\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "# Deep Learning\n",
    "\n",
    ")\n",
    "    f1_score, roc_auc_score, roc_curve\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix,\n",
    "from sklearn.metrics import (\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# Th∆∞ vi·ªán chu·∫©n h√≥a v√† x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "## 1. Import th∆∞ vi·ªán v√† c·∫•u h√¨nh"
   "source": [
   "metadata": {},
   "cell_type": "markdown",
  {
  },
   ]
    "**Dataset**: CICIDS2018"
    "\n",
    "- **Attack (1)**: L∆∞u l∆∞·ª£ng m·∫°ng b·∫•t th∆∞·ªùng/t·∫•n c√¥ng\n",
    "- **Benign (0)**: L∆∞u l∆∞·ª£ng m·∫°ng b√¨nh th∆∞·ªùng\n",
    "**B√†i to√°n**: Binary Classification\n",
    "\n",
    "Notebook n√†y hu·∫•n luy·ªán m√¥ h√¨nh CNN 1D ƒë·ªÉ ph√°t hi·ªán l∆∞u l∆∞·ª£ng m·∫°ng IoT b·∫•t th∆∞·ªùng.\n",
    "\n",
    "# üõ°Ô∏è CNN-based IoT Anomaly Detection - CICIDS2018\n",
   "source": [
   "metadata": {},
   "cell_type": "markdown",
  {
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

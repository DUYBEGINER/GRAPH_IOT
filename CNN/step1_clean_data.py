"""
======================================================================================
B∆Ø·ªöC 1: CLEAN V√Ä TI·ªÄN X·ª¨ L√ù DATASET CICIDS2018 CHO M√î H√åNH CNN
======================================================================================

Script n√†y th·ª±c hi·ªán c√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu:
1. ƒê·ªçc t·ª´ng file CSV theo chunks ƒë·ªÉ t·ªëi ∆∞u b·ªô nh·ªõ
2. Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn thi·∫øt (IP, Port, Timestamp, Flow ID)
3. Lo·∫°i b·ªè c√°c c·ªôt c√≥ variance = 0 (zero-variance columns)
4. X·ª≠ l√Ω Infinity v√† NaN b·∫±ng Mode c·ªßa c·ªôt
5. Lo·∫°i b·ªè c√°c h√†ng tr√πng l·∫∑p
6. Chuy·ªÉn ƒë·ªïi nh√£n sang d·∫°ng binary (Benign=0, Attack=1)
7. L∆∞u d·ªØ li·ªáu ƒë√£ clean v√†o folder ƒë·ªÉ s·ª≠ d·ª•ng sau

C√≥ th·ªÉ ch·∫°y tr√™n c·∫£ Kaggle v√† Local
"""

import os
import numpy as np
import pandas as pd
import pickle
import json
import gc
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Ki·ªÉm tra m√¥i tr∆∞·ªùng ch·∫°y (Kaggle ho·∫∑c Local)
IS_KAGGLE = os.path.exists('/kaggle/input')

# Progress bar
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    print("‚ö†Ô∏è  tqdm kh√¥ng c√≥ s·∫µn. C√†i ƒë·∫∑t b·∫±ng: pip install tqdm")
    tqdm = lambda x, **kwargs: x

# ============================================================================
# C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N
# ============================================================================
if IS_KAGGLE:
    # ƒê∆∞·ªùng d·∫´n tr√™n Kaggle - thay ƒë·ªïi theo dataset c·ªßa b·∫°n
    DATA_DIR = "/kaggle/input/cicids2018"  # Thay ƒë·ªïi n·∫øu t√™n dataset kh√°c
    OUTPUT_DIR = "/kaggle/working/cleaned_data"
    print("üåê ƒêang ch·∫°y tr√™n KAGGLE")
else:
    # ƒê∆∞·ªùng d·∫´n Local
    DATA_DIR = r"D:\PROJECT\Machine Learning\IOT\CICIDS2018-CSV"
    OUTPUT_DIR = r"D:\PROJECT\Machine Learning\IOT\CNN\cleaned_data"
    print("üíª ƒêang ch·∫°y tr√™n LOCAL")

# ============================================================================
# C·∫§U H√åNH X·ª¨ L√ù D·ªÆ LI·ªÜU
# ============================================================================

# K√≠ch th∆∞·ªõc chunk khi ƒë·ªçc CSV (ƒëi·ªÅu ch·ªânh theo RAM c·ªßa m√°y)
CHUNK_SIZE = 300000  # 300k rows m·ªói chunk

# Random state ƒë·ªÉ t√°i t·∫°o k·∫øt qu·∫£
RANDOM_STATE = 42

# ============================================================================
# DANH S√ÅCH C√ÅC C·ªòT C·∫¶N LO·∫†I B·ªé (Identification columns)
# ============================================================================

COLUMNS_TO_DROP = [
    'Flow ID',          # ID duy nh·∫•t cho m·ªói flow - kh√¥ng c√≥ √Ω nghƒ©a ph√¢n lo·∫°i
    'Src IP',           # IP ngu·ªìn - kh√¥ng t·ªïng qu√°t
    'Dst IP',           # IP ƒë√≠ch - kh√¥ng t·ªïng qu√°t
    'Src Port',         # Port ngu·ªìn - c√≥ th·ªÉ b·ªã overfitting
    'Dst Port',         # Port ƒë√≠ch - c√≥ th·ªÉ b·ªã overfitting
    'Timestamp',        # Th·ªùi gian - kh√¥ng li√™n quan ƒë·∫øn pattern t·∫•n c√¥ng
]

# C·ªôt nh√£n
LABEL_COLUMN = 'Label'

# ============================================================================
# CLASS X·ª¨ L√ù D·ªÆ LI·ªÜU
# ============================================================================

class CICIDS2018_DataCleaner:
    """
    Class clean d·ªØ li·ªáu CICIDS2018 cho m√¥ h√¨nh CNN

    C√°c b∆∞·ªõc x·ª≠ l√Ω:
    1. ƒê·ªçc d·ªØ li·ªáu theo chunks
    2. Lo·∫°i b·ªè c·ªôt identification
    3. Lo·∫°i b·ªè zero-variance columns
    4. X·ª≠ l√Ω Infinity v√† NaN b·∫±ng Mode
    5. Lo·∫°i b·ªè duplicate
    6. Chuy·ªÉn ƒë·ªïi nh√£n sang binary
    7. L∆∞u d·ªØ li·ªáu ƒë√£ clean
    """

    def __init__(self, data_dir, output_dir, chunk_size=CHUNK_SIZE):
        """
        Kh·ªüi t·∫°o data cleaner

        Args:
            data_dir: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a file CSV
            output_dir: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£
            chunk_size: S·ªë d√≤ng m·ªói chunk khi ƒë·ªçc CSV
        """
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.chunk_size = chunk_size

        # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # L∆∞u t√™n c√°c features v√† th√¥ng tin x·ª≠ l√Ω
        self.feature_names = None
        self.zero_variance_cols = []
        self.column_modes = {}  # L∆∞u mode c·ªßa t·ª´ng c·ªôt ƒë·ªÉ x·ª≠ l√Ω NaN/Inf

        # Th·ªëng k√™
        self.stats = {
            'total_rows_read': 0,
            'rows_after_cleaning': 0,
            'duplicates_removed': 0,
            'nan_replaced': 0,
            'inf_replaced': 0,
            'zero_variance_cols_removed': 0,
            'benign_count': 0,
            'attack_count': 0,
            'feature_count': 0,
            'processing_time': 0.0  # Float ƒë·ªÉ l∆∞u th·ªùi gian x·ª≠ l√Ω (gi√¢y)
        }

    def _get_csv_files(self):
        """L·∫•y danh s√°ch c√°c file CSV trong th∆∞ m·ª•c data"""
        csv_files = list(self.data_dir.glob("*_TrafficForML_CICFlowMeter.csv"))
        if not csv_files:
            # Th·ª≠ pattern kh√°c cho Kaggle
            csv_files = list(self.data_dir.glob("*.csv"))
            # Lo·∫°i b·ªè file zip n·∫øu c√≥
            csv_files = [f for f in csv_files if not f.name.endswith('.zip')]

        if not csv_files:
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file CSV trong {self.data_dir}")

        print(f"\nüìÇ T√¨m th·∫•y {len(csv_files)} file CSV:")
        for f in sorted(csv_files):
            print(f"   - {f.name}")
        return sorted(csv_files)

    def _clean_column_names(self, df):
        """Chu·∫©n h√≥a t√™n c·ªôt (lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a)"""
        df.columns = df.columns.str.strip()
        return df

    def _drop_identification_columns(self, df):
        """Lo·∫°i b·ªè c√°c c·ªôt identification kh√¥ng c·∫ßn thi·∫øt cho hu·∫•n luy·ªán"""
        columns_to_drop = [col for col in COLUMNS_TO_DROP if col in df.columns]
        if columns_to_drop:
            df = df.drop(columns=columns_to_drop)
        return df

    def _convert_to_numeric(self, df):
        """Chuy·ªÉn ƒë·ªïi c√°c c·ªôt v·ªÅ d·∫°ng s·ªë"""
        feature_cols = [col for col in df.columns if col != LABEL_COLUMN]
        for col in feature_cols:
            if df[col].dtype == 'object':
                df[col] = pd.to_numeric(df[col], errors='coerce')
        return df

    def _convert_to_binary_label(self, df):
        """
        Chuy·ªÉn ƒë·ªïi nh√£n sang d·∫°ng binary:
        - Benign -> 0 (l∆∞u l∆∞·ª£ng b√¨nh th∆∞·ªùng)
        - T·∫•t c·∫£ c√°c lo·∫°i t·∫•n c√¥ng kh√°c -> 1 (l∆∞u l∆∞·ª£ng b·∫•t th∆∞·ªùng)
        """
        if LABEL_COLUMN not in df.columns:
            raise ValueError(f"Kh√¥ng t√¨m th·∫•y c·ªôt '{LABEL_COLUMN}' trong d·ªØ li·ªáu")

        # Chu·∫©n h√≥a nh√£n (lo·∫°i b·ªè kho·∫£ng tr·∫Øng, lowercase)
        df[LABEL_COLUMN] = df[LABEL_COLUMN].astype(str).str.strip().str.lower()

        # Lo·∫°i b·ªè c√°c h√†ng c√≥ nh√£n l√† 'label' (header b·ªã l·∫´n v√†o data)
        df = df[df[LABEL_COLUMN] != 'label']

        # Chuy·ªÉn ƒë·ªïi sang binary: Benign=0, Attack=1
        df['binary_label'] = (df[LABEL_COLUMN] != 'benign').astype(int)

        # X√≥a c·ªôt Label g·ªëc, gi·ªØ l·∫°i binary_label
        df = df.drop(columns=[LABEL_COLUMN])

        return df

    def _first_pass_collect_info(self, csv_files):
        """
        L·∫ßn ƒë·ªçc ƒë·∫ßu ti√™n: Thu th·∫≠p th√¥ng tin v·ªÅ columns v√† t√≠nh mode

        M·ª•c ƒë√≠ch:
        - X√°c ƒë·ªãnh c√°c c·ªôt c√≥ variance = 0
        - T√≠nh mode c·ªßa t·ª´ng c·ªôt ƒë·ªÉ thay th·∫ø NaN/Inf
        """
        print("\n" + "="*80)
        print("üìä B∆Ø·ªöC 1: THU TH·∫¨P TH√îNG TIN T·ª™ D·ªÆ LI·ªÜU")
        print("="*80)

        all_columns = None
        column_value_counts = {}  # ƒê·ªÉ t√≠nh mode
        column_min_max = {}  # ƒê·ªÉ ki·ªÉm tra variance

        for csv_file in csv_files:
            print(f"\n   ƒêang scan: {csv_file.name}")
            chunk_iterator = pd.read_csv(csv_file, chunksize=self.chunk_size,
                                        low_memory=False, encoding='utf-8')

            for chunk in chunk_iterator:
                chunk = self._clean_column_names(chunk)
                chunk = self._drop_identification_columns(chunk)
                chunk = self._convert_to_numeric(chunk)

                if all_columns is None:
                    all_columns = [col for col in chunk.columns if col != LABEL_COLUMN]
                    for col in all_columns:
                        column_value_counts[col] = {}
                        column_min_max[col] = {'min': np.inf, 'max': -np.inf}

                # Thu th·∫≠p th√¥ng tin cho m·ªói c·ªôt
                for col in all_columns:
                    if col in chunk.columns:
                        # Thay th·∫ø inf tr∆∞·ªõc khi t√≠nh
                        col_data = chunk[col].replace([np.inf, -np.inf], np.nan)
                        valid_data = col_data.dropna()

                        if len(valid_data) > 0:
                            # C·∫≠p nh·∫≠t min/max
                            col_min = valid_data.min()
                            col_max = valid_data.max()
                            column_min_max[col]['min'] = min(column_min_max[col]['min'], col_min)
                            column_min_max[col]['max'] = max(column_min_max[col]['max'], col_max)

                            # Thu th·∫≠p value counts cho mode (l·∫•y top 10 ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ)
                            vc = valid_data.value_counts().head(10).to_dict()
                            for val, count in vc.items():
                                if val not in column_value_counts[col]:
                                    column_value_counts[col][val] = 0
                                column_value_counts[col][val] += count

                gc.collect()

        # X√°c ƒë·ªãnh zero-variance columns
        print("\n   ƒêang ph√¢n t√≠ch variance c·ªßa c√°c c·ªôt...")
        for col in all_columns:
            if column_min_max[col]['min'] == column_min_max[col]['max']:
                self.zero_variance_cols.append(col)

        # T√≠nh mode cho m·ªói c·ªôt
        print("   ƒêang t√≠nh mode cho m·ªói c·ªôt...")
        for col in all_columns:
            if col not in self.zero_variance_cols:
                if column_value_counts[col]:
                    # Mode l√† gi√° tr·ªã xu·∫•t hi·ªán nhi·ªÅu nh·∫•t
                    mode_val = max(column_value_counts[col], key=column_value_counts[col].get)
                    self.column_modes[col] = mode_val
                else:
                    self.column_modes[col] = 0  # Fallback n·∫øu kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá

        self.stats['zero_variance_cols_removed'] = len(self.zero_variance_cols)

        print(f"\n   ‚úÖ S·ªë c·ªôt zero-variance s·∫Ω lo·∫°i b·ªè: {len(self.zero_variance_cols)}")
        if self.zero_variance_cols:
            print(f"      C√°c c·ªôt: {self.zero_variance_cols}")
        print(f"   ‚úÖ S·ªë c·ªôt s·∫Ω gi·ªØ l·∫°i: {len(all_columns) - len(self.zero_variance_cols)}")

        return all_columns

    def _handle_nan_inf_with_mode(self, df):
        """
        X·ª≠ l√Ω NaN v√† Infinity b·∫±ng Mode c·ªßa c·ªôt

        Replace Infinity and NaN with the Mode of the column
        """
        feature_cols = [col for col in df.columns if col != 'binary_label']

        for col in feature_cols:
            if col in self.column_modes:
                mode_val = self.column_modes[col]

                # ƒê·∫øm s·ªë l∆∞·ª£ng inf v√† nan
                inf_mask = np.isinf(df[col])
                nan_mask = df[col].isna()

                self.stats['inf_replaced'] += inf_mask.sum()
                self.stats['nan_replaced'] += nan_mask.sum()

                # Thay th·∫ø inf b·∫±ng nan tr∆∞·ªõc
                df[col] = df[col].replace([np.inf, -np.inf], np.nan)

                # Thay th·∫ø t·∫•t c·∫£ nan b·∫±ng mode
                df[col] = df[col].fillna(mode_val)

        return df

    def _drop_zero_variance_columns(self, df):
        """Lo·∫°i b·ªè c√°c c·ªôt c√≥ variance = 0"""
        cols_to_drop = [col for col in self.zero_variance_cols if col in df.columns]
        if cols_to_drop:
            df = df.drop(columns=cols_to_drop)
        return df

    def _process_single_file(self, csv_file):
        """
        X·ª≠ l√Ω m·ªôt file CSV theo chunks

        Args:
            csv_file: ƒê∆∞·ªùng d·∫´n file CSV

        Returns:
            DataFrame ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
        """
        print(f"\nüìÑ ƒêang x·ª≠ l√Ω: {csv_file.name}")

        processed_chunks = []
        chunk_iterator = pd.read_csv(csv_file, chunksize=self.chunk_size,
                                     low_memory=False, encoding='utf-8')

        # Progress bar cho chunks
        if TQDM_AVAILABLE:
            file_size = csv_file.stat().st_size
            estimated_chunks = max(1, file_size // (self.chunk_size * 500))
            chunk_iterator = tqdm(chunk_iterator, desc="   Chunks",
                                  total=estimated_chunks, unit="chunk")

        for chunk in chunk_iterator:
            self.stats['total_rows_read'] += len(chunk)

            # B∆∞·ªõc 1: Chu·∫©n h√≥a t√™n c·ªôt
            chunk = self._clean_column_names(chunk)

            # B∆∞·ªõc 2: Lo·∫°i b·ªè c·ªôt identification
            chunk = self._drop_identification_columns(chunk)

            # B∆∞·ªõc 3: Chuy·ªÉn ƒë·ªïi sang d·∫°ng s·ªë
            chunk = self._convert_to_numeric(chunk)

            # B∆∞·ªõc 4: Chuy·ªÉn ƒë·ªïi nh√£n sang binary
            chunk = self._convert_to_binary_label(chunk)

            # B∆∞·ªõc 5: Lo·∫°i b·ªè zero-variance columns
            chunk = self._drop_zero_variance_columns(chunk)

            # B∆∞·ªõc 6: X·ª≠ l√Ω NaN v√† Inf b·∫±ng Mode
            chunk = self._handle_nan_inf_with_mode(chunk)

            processed_chunks.append(chunk)
            gc.collect()

        # G·ªôp c√°c chunks l·∫°i
        if processed_chunks:
            df = pd.concat(processed_chunks, ignore_index=True)
            del processed_chunks
            gc.collect()
            return df

        return None

    def clean_all_files(self):
        """
        Clean t·∫•t c·∫£ c√°c file CSV

        Returns:
            DataFrame ƒë√£ clean ho√†n ch·ªânh
        """
        start_time = datetime.now()
        print("\n" + "="*80)
        print(" B·∫ÆT ƒê·∫¶U CLEAN D·ªÆ LI·ªÜU CICIDS2018")
        print("="*80)

        csv_files = self._get_csv_files()

        # B∆∞·ªõc 1: Thu th·∫≠p th√¥ng tin (mode, zero-variance)
        all_columns = self._first_pass_collect_info(csv_files)

        # B∆∞·ªõc 2: X·ª≠ l√Ω t·ª´ng file
        print("\n" + "="*80)
        print(" B∆Ø·ªöC 2: CLEAN D·ªÆ LI·ªÜU")
        print("="*80)

        all_dataframes = []
        for csv_file in csv_files:
            df = self._process_single_file(csv_file)
            if df is not None:
                all_dataframes.append(df)
                print(f"   ‚úÖ ƒê√£ x·ª≠ l√Ω: {len(df):,} m·∫´u")

        # G·ªôp t·∫•t c·∫£ l·∫°i
        print("\n" + "-"*80)
        print(" ƒêANG G·ªòP D·ªÆ LI·ªÜU...")

        df_combined = pd.concat(all_dataframes, ignore_index=True)
        del all_dataframes
        gc.collect()

        print(f"   T·ªïng s·ªë m·∫´u sau khi g·ªôp: {len(df_combined):,}")

        # Lo·∫°i b·ªè duplicate tr√™n to√†n b·ªô dataset
        print("   ƒêang lo·∫°i b·ªè duplicate...")
        rows_before = len(df_combined)
        df_combined = df_combined.drop_duplicates()
        rows_after = len(df_combined)
        self.stats['duplicates_removed'] = rows_before - rows_after
        print(f"   S·ªë m·∫´u sau khi lo·∫°i duplicate: {len(df_combined):,}")
        print(f"   S·ªë duplicate ƒë√£ lo·∫°i: {self.stats['duplicates_removed']:,}")

        # ƒê·∫øm s·ªë l∆∞·ª£ng m·ªói class
        self.stats['benign_count'] = int((df_combined['binary_label'] == 0).sum())
        self.stats['attack_count'] = int((df_combined['binary_label'] == 1).sum())

        # C·∫≠p nh·∫≠t th·ªëng k√™
        self.stats['rows_after_cleaning'] = len(df_combined)
        self.stats['feature_count'] = len(df_combined.columns) - 1  # Tr·ª´ c·ªôt label

        # L∆∞u t√™n features
        self.feature_names = [col for col in df_combined.columns if col != 'binary_label']

        end_time = datetime.now()
        self.stats['processing_time'] = (end_time - start_time).total_seconds()

        return df_combined

    def save_cleaned_data(self, df):
        """
        L∆∞u d·ªØ li·ªáu ƒë√£ clean

        L∆∞u th√†nh c√°c file:
        - cleaned_data.parquet (d·ªØ li·ªáu ƒë√£ clean, ch∆∞a normalize)
        - feature_names.txt
        - cleaning_metadata.json
        """
        print("\n" + "="*80)
        print(" ƒêANG L∆ØU D·ªÆ LI·ªÜU ƒê√É CLEAN...")
        print("="*80)

        # L∆∞u d·ªØ li·ªáu d·∫°ng parquet (nhanh v√† nh·ªè g·ªçn)
        parquet_path = self.output_dir / 'cleaned_data.parquet'
        df.to_parquet(parquet_path, index=False)
        print(f"   ‚úÖ ƒê√£ l∆∞u: {parquet_path}")
        print(f"      K√≠ch th∆∞·ªõc file: {parquet_path.stat().st_size / (1024*1024):.2f} MB")

        # C≈©ng l∆∞u d·∫°ng CSV ƒë·ªÉ d·ªÖ ki·ªÉm tra (optional, c√≥ th·ªÉ comment n·∫øu file qu√° l·ªõn)
        # csv_path = self.output_dir / 'cleaned_data.csv'
        # df.to_csv(csv_path, index=False)
        # print(f"   ‚úÖ ƒê√£ l∆∞u: {csv_path}")

        # L∆∞u feature names
        with open(self.output_dir / 'feature_names.txt', 'w') as f:
            for name in self.feature_names:
                f.write(name + '\n')
        print(f"   ‚úÖ ƒê√£ l∆∞u: feature_names.txt")

        # L∆∞u column modes (ƒë·ªÉ c√≥ th·ªÉ s·ª≠ d·ª•ng cho d·ªØ li·ªáu m·ªõi)
        with open(self.output_dir / 'column_modes.pkl', 'wb') as f:
            pickle.dump(self.column_modes, f)
        print(f"   ‚úÖ ƒê√£ l∆∞u: column_modes.pkl")

        # L∆∞u zero-variance columns
        with open(self.output_dir / 'zero_variance_cols.pkl', 'wb') as f:
            pickle.dump(self.zero_variance_cols, f)
        print(f"   ‚úÖ ƒê√£ l∆∞u: zero_variance_cols.pkl")

        # Chuy·ªÉn ƒë·ªïi stats sang ki·ªÉu Python native
        stats_native = {}
        for key, value in self.stats.items():
            if hasattr(value, 'item'):
                stats_native[key] = value.item()
            elif isinstance(value, (np.integer, np.floating)):
                stats_native[key] = int(value) if isinstance(value, np.integer) else float(value)
            else:
                stats_native[key] = value

        # L∆∞u metadata
        metadata = {
            'n_features': len(self.feature_names),
            'feature_names': self.feature_names,
            'total_samples': int(len(df)),
            'benign_count': self.stats['benign_count'],
            'attack_count': self.stats['attack_count'],
            'benign_ratio': self.stats['benign_count'] / len(df),
            'attack_ratio': self.stats['attack_count'] / len(df),
            'zero_variance_cols': self.zero_variance_cols,
            'stats': stats_native,
            'created_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

        with open(self.output_dir / 'cleaning_metadata.json', 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=4, ensure_ascii=False)
        print(f"   ‚úÖ ƒê√£ l∆∞u: cleaning_metadata.json")

        print(f"\nüìÅ T·∫•t c·∫£ file ƒë∆∞·ª£c l∆∞u t·∫°i: {self.output_dir}")

    def print_summary(self):
        """In t√≥m t·∫Øt qu√° tr√¨nh x·ª≠ l√Ω"""
        print("\n" + "="*80)
        print(" T√ìM T·∫ÆT CLEAN D·ªÆ LI·ªÜU")
        print("="*80)
        print(f"   T·ªïng s·ªë d√≤ng ƒë·ªçc ƒë∆∞·ª£c:        {self.stats['total_rows_read']:,}")
        print(f"   S·ªë d√≤ng sau khi clean:        {self.stats['rows_after_cleaning']:,}")
        print(f"   S·ªë duplicate ƒë√£ lo·∫°i:         {self.stats['duplicates_removed']:,}")
        print(f"   S·ªë NaN ƒë√£ thay b·∫±ng mode:     {self.stats['nan_replaced']:,}")
        print(f"   S·ªë Inf ƒë√£ thay b·∫±ng mode:     {self.stats['inf_replaced']:,}")
        print(f"   S·ªë c·ªôt zero-variance ƒë√£ lo·∫°i: {self.stats['zero_variance_cols_removed']}")
        print(f"   S·ªë features c√≤n l·∫°i:          {self.stats['feature_count']}")
        print(f"\n   üìà PH√ÇN B·ªê NH√ÉN:")
        print(f"   S·ªë m·∫´u Benign (0):  {self.stats['benign_count']:,} ({self.stats['benign_count']/self.stats['rows_after_cleaning']*100:.1f}%)")
        print(f"   S·ªë m·∫´u Attack (1):  {self.stats['attack_count']:,} ({self.stats['attack_count']/self.stats['rows_after_cleaning']*100:.1f}%)")
        print(f"\n   Th·ªùi gian x·ª≠ l√Ω: {self.stats['processing_time']:.2f} gi√¢y")
        print("="*80)


def main():
    """H√†m ch√≠nh ƒë·ªÉ ch·∫°y cleaning"""

    print("\n" + "="*80)
    print("üßπ B∆Ø·ªöC 1: CLEAN D·ªÆ LI·ªÜU CICIDS2018 CHO CNN")
    print("   Ph√°t hi·ªán l∆∞u l∆∞·ª£ng m·∫°ng IoT b·∫•t th∆∞·ªùng")
    print("="*80)

    # Kh·ªüi t·∫°o cleaner
    cleaner = CICIDS2018_DataCleaner(
        data_dir=DATA_DIR,
        output_dir=OUTPUT_DIR,
        chunk_size=CHUNK_SIZE
    )

    # Clean t·∫•t c·∫£ c√°c file
    df = cleaner.clean_all_files()

    # L∆∞u d·ªØ li·ªáu ƒë√£ clean
    cleaner.save_cleaned_data(df)

    # In t√≥m t·∫Øt
    cleaner.print_summary()

    print("\n‚úÖ HO√ÄN TH√ÄNH B∆Ø·ªöC 1!")
    print("   D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c clean v√† l∆∞u v√†o folder.")
    print("   Ch·∫°y step2_prepare_training_data.py ƒë·ªÉ chia train/val/test v√† c√¢n b·∫±ng d·ªØ li·ªáu.")

    return cleaner


if __name__ == "__main__":
    cleaner = main()


{
 "cells": [
}
 "nbformat_minor": 4
 "nbformat": 4,
 },
  }
   "version": "3.10.0"
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "name": "python",
   "mimetype": "text/x-python",
   "file_extension": ".py",
   },
    "version": 3
    "name": "ipython",
   "codemirror_mode": {
  "language_info": {
  },
   "name": "python3"
   "language": "python",
   "display_name": "Python 3",
  "kernelspec": {
 "metadata": {
 ],
  }
   ]
    "print(\"\\n   M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u v√† s·∫µn s√†ng s·ª≠ d·ª•ng!\")"
    "print(f\"   {'='*40}\")\n",
    "print(f\"   Training Time:  {training_time/60:.2f} ph√∫t\")\n",
    "print(f\"   Test F1-Score:  {f1_score*100:.2f}%\")\n",
    "print(f\"   Test Recall:    {recall*100:.2f}%\")\n",
    "print(f\"   Test Precision: {precision*100:.2f}%\")\n",
    "print(f\"   Test Accuracy:  {accuracy*100:.2f}%\")\n",
    "print(f\"   {'='*40}\")\n",
    "print(f\"\\n   üìä K·∫æT QU·∫¢ CU·ªêI C√ôNG:\")\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ HO√ÄN TH√ÄNH HU·∫§N LUY·ªÜN M√î H√åNH CNN!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "# T√≥m t·∫Øt k·∫øt qu·∫£\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "print(f\"\\nüìÅ T·∫•t c·∫£ file ƒë∆∞·ª£c l∆∞u t·∫°i: {MODEL_DIR}\")"
    "\n",
    "print(\"   ‚úÖ evaluation_results.json\")\n",
    "    json.dump(results, f, indent=4)\n",
    "with open(os.path.join(MODEL_DIR, 'evaluation_results.json'), 'w') as f:\n",
    "}\n",
    "    'confusion_matrix': cm.tolist()\n",
    "    'epochs_trained': len(history.history['loss']),\n",
    "    'training_time_minutes': training_time / 60,\n",
    "    'test_f1_score': float(f1_score),\n",
    "    'test_recall': float(recall),\n",
    "    'test_precision': float(precision),\n",
    "    'test_accuracy': float(accuracy),\n",
    "    'test_loss': float(loss),\n",
    "results = {\n",
    "# L∆∞u k·∫øt qu·∫£\n",
    "\n",
    "print(\"   ‚úÖ training_history.json\")\n",
    "    json.dump(history_dict, f, indent=4)\n",
    "with open(os.path.join(MODEL_DIR, 'training_history.json'), 'w') as f:\n",
    "history_dict = {key: [float(v) for v in values] for key, values in history.history.items()}\n",
    "# L∆∞u history\n",
    "\n",
    "print(\"   ‚úÖ model_weights.h5\")\n",
    "model.save_weights(os.path.join(MODEL_DIR, 'model_weights.h5'))\n",
    "# L∆∞u weights\n",
    "\n",
    "print(\"   ‚úÖ final_model.keras\")\n",
    "model.save(os.path.join(MODEL_DIR, 'final_model.keras'))\n",
    "# L∆∞u model\n",
    "\n",
    "print(\"\\nüíæ ƒêANG L∆ØU MODEL V√Ä K·∫æT QU·∫¢...\")\n",
    "# L∆∞u model v√† k·∫øt qu·∫£\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "print(\"   ‚úÖ ƒê√£ l∆∞u training_history.png\")"
    "\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(MODEL_DIR, 'training_history.png'), dpi=150)\n",
    "plt.tight_layout()\n",
    "\n",
    "axes[1, 1].grid(True)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].set_ylabel('Recall')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_title('Model Recall')\n",
    "axes[1, 1].plot(history.history['val_recall'], label='Val Recall')\n",
    "axes[1, 1].plot(history.history['recall'], label='Train Recall')\n",
    "# Recall\n",
    "\n",
    "axes[1, 0].grid(True)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].set_ylabel('Precision')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_title('Model Precision')\n",
    "axes[1, 0].plot(history.history['val_precision'], label='Val Precision')\n",
    "axes[1, 0].plot(history.history['precision'], label='Train Precision')\n",
    "# Precision\n",
    "\n",
    "axes[0, 1].grid(True)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_title('Model Accuracy')\n",
    "axes[0, 1].plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "axes[0, 1].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "# Accuracy\n",
    "\n",
    "axes[0, 0].grid(True)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_title('Model Loss')\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Val Loss')\n",
    "axes[0, 0].plot(history.history['loss'], label='Train Loss')\n",
    "# Loss\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "# V·∫Ω bi·ªÉu ƒë·ªì training history\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "print(classification_report(y_test, y_pred, target_names=['Benign', 'Attack']))"
    "print(\"\\nüìã CLASSIFICATION REPORT:\")\n",
    "\n",
    "print(f\"   Actual Attack  {cm[1,0]:>6}  {cm[1,1]:>6}\")\n",
    "print(f\"   Actual Benign  {cm[0,0]:>6}  {cm[0,1]:>6}\")\n",
    "print(f\"                 Benign  Attack\")\n",
    "print(f\"                 Predicted\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nüìã CONFUSION MATRIX:\")\n",
    "\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "y_pred_prob = model.predict(X_test, verbose=0)\n",
    "# Confusion Matrix v√† Classification Report\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "print(f\"   {'='*40}\")"
    "print(f\"   F1-Score:  {f1_score:.4f}\")\n",
    "print(f\"   Recall:    {recall:.4f}\")\n",
    "print(f\"   Precision: {precision:.4f}\")\n",
    "print(f\"   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"   Loss:      {loss:.4f}\")\n",
    "print(f\"   {'='*40}\")\n",
    "print(f\"\\n   üìä K·∫æT QU·∫¢:\")\n",
    "\n",
    "f1_score = 2 * (precision * recall) / (precision + recall + 1e-7)\n",
    "loss, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä ƒê√ÅNH GI√Å M√î H√åNH TR√äN TEST SET\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "# ƒê√°nh gi√° tr√™n test set\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "print(f\"   üìà Best val_accuracy: {max(history.history['val_accuracy']):.4f}\")"
    "print(f\"   üìà Best val_loss: {min(history.history['val_loss']):.4f}\")\n",
    "print(f\"\\n   ‚è±Ô∏è Th·ªùi gian training: {training_time/60:.2f} ph√∫t\")\n",
    "\n",
    "training_time = (end_time - start_time).total_seconds()\n",
    "end_time = datetime.now()\n",
    "\n",
    ")\n",
    "    verbose=1\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    X_train, y_train,\n",
    "history = model.fit(\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Epochs: {EPOCHS}\")\n",
    "print(\"=\"*80)\n",
    "print(\"üöÄ B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN M√î H√åNH\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "# Train model\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "print(\"   ‚úÖ ReduceLROnPlateau: factor=0.5, patience=5\")"
    "print(\"   ‚úÖ ModelCheckpoint: save best model\")\n",
    "print(\"   ‚úÖ EarlyStopping: patience=10\")\n",
    "\n",
    "]\n",
    "    )\n",
    "        verbose=1\n",
    "        min_lr=1e-7,\n",
    "        patience=5,\n",
    "        factor=0.5,\n",
    "        monitor='val_loss',\n",
    "    ReduceLROnPlateau(\n",
    "    # Reduce LR\n",
    "    \n",
    "    ),\n",
    "        mode='min'\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "        monitor='val_loss',\n",
    "        filepath=os.path.join(MODEL_DIR, 'best_model.keras'),\n",
    "    ModelCheckpoint(\n",
    "    # Model Checkpoint\n",
    "    \n",
    "    ),\n",
    "        restore_best_weights=True\n",
    "        mode='min',\n",
    "        verbose=1,\n",
    "        patience=PATIENCE,\n",
    "        monitor='val_loss',\n",
    "    EarlyStopping(\n",
    "    # Early Stopping\n",
    "callbacks = [\n",
    "\n",
    "print(\"\\nüìå ƒêANG C·∫§U H√åNH CALLBACKS...\")\n",
    "# T·∫°o callbacks\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "model = build_cnn_model(input_shape)"
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "# X√¢y d·ª±ng model\n",
    "\n",
    "    return model\n",
    "    \n",
    "    model.summary()\n",
    "    print(\"\\n   üìã KI·∫æN TR√öC M√î H√åNH:\")\n",
    "    \n",
    "    )\n",
    "        metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    model.compile(\n",
    "    # Compile\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid', name='output'))\n",
    "    model.add(Flatten(name='flatten'))\n",
    "    # Flatten v√† Output\n",
    "    \n",
    "    model.add(Dropout(DROPOUT_RATE, name='dropout'))\n",
    "    model.add(BatchNormalization(name='batch_norm'))\n",
    "    # Regularization\n",
    "    \n",
    "    model.add(MaxPooling1D(pool_size=2, name='maxpool_5'))\n",
    "    model.add(Conv1D(64, kernel_size=2, activation='relu', padding='same', name='conv1d_5'))\n",
    "    # Conv Block 5: 64 filters\n",
    "    \n",
    "    model.add(MaxPooling1D(pool_size=2, name='maxpool_4'))\n",
    "    model.add(Conv1D(64, kernel_size=2, activation='relu', padding='same', name='conv1d_4'))\n",
    "    # Conv Block 4: 64 filters\n",
    "    \n",
    "    model.add(MaxPooling1D(pool_size=2, name='maxpool_3'))\n",
    "    model.add(Conv1D(64, kernel_size=2, activation='relu', padding='same', name='conv1d_3'))\n",
    "    # Conv Block 3: 64 filters\n",
    "    \n",
    "    model.add(MaxPooling1D(pool_size=2, name='maxpool_2'))\n",
    "    model.add(Conv1D(32, kernel_size=2, activation='relu', padding='same', name='conv1d_2'))\n",
    "    # Conv Block 2: 32 filters\n",
    "    \n",
    "    model.add(MaxPooling1D(pool_size=2, name='maxpool_1'))\n",
    "    model.add(Conv1D(32, kernel_size=2, activation='relu', padding='same', name='conv1d_1'))\n",
    "    # Conv Block 1: 32 filters\n",
    "    \n",
    "    model.add(Input(shape=input_shape))\n",
    "    # Input\n",
    "    \n",
    "    model = Sequential(name='CNN_Binary_Classification')\n",
    "    \n",
    "    print(f\"   Input shape: {input_shape}\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"üèóÔ∏è ƒêANG X√ÇY D·ª∞NG M√î H√åNH CNN\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \"\"\"\n",
    "    - Output: Dense(1, sigmoid)\n",
    "    - BatchNormalization + Dropout tr∆∞·ªõc Flatten\n",
    "    - 5 l·ªõp Conv1D v·ªõi MaxPooling\n",
    "    Ki·∫øn tr√∫c:\n",
    "    \n",
    "    X√¢y d·ª±ng m√¥ h√¨nh CNN cho ph√¢n lo·∫°i binary\n",
    "    \"\"\"\n",
    "def build_cnn_model(input_shape):\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "---"
    "# üß† B∆Ø·ªöC 3: X√ÇY D·ª∞NG V√Ä TRAIN M√î H√åNH CNN\n",
    "---\n",
   "source": [
   "metadata": {},
   "cell_type": "markdown",
  {
  },
   ]
    "print(f\"\\n‚úÖ HO√ÄN TH√ÄNH B∆Ø·ªöC 2 - CHU·∫®N B·ªä D·ªÆ LI·ªÜU\")"
    "print(f\"   ‚úÖ ƒê√£ l∆∞u t·∫•t c·∫£ d·ªØ li·ªáu training\")\n",
    "\n",
    "    pickle.dump(class_weights, f)\n",
    "with open(os.path.join(TRAINING_DATA_DIR, 'class_weights.pkl'), 'wb') as f:\n",
    "\n",
    "    pickle.dump(scaler, f)\n",
    "with open(os.path.join(TRAINING_DATA_DIR, 'scaler.pkl'), 'wb') as f:\n",
    "\n",
    "np.save(os.path.join(TRAINING_DATA_DIR, 'y_test.npy'), y_test)\n",
    "np.save(os.path.join(TRAINING_DATA_DIR, 'y_val.npy'), y_val)\n",
    "np.save(os.path.join(TRAINING_DATA_DIR, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(TRAINING_DATA_DIR, 'X_test.npy'), X_test)\n",
    "np.save(os.path.join(TRAINING_DATA_DIR, 'X_val.npy'), X_val)\n",
    "np.save(os.path.join(TRAINING_DATA_DIR, 'X_train.npy'), X_train)\n",
    "\n",
    "print(\"\\nüíæ ƒêANG L∆ØU D·ªÆ LI·ªÜU TRAINING...\")\n",
    "# L∆∞u d·ªØ li·ªáu training\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "print(f\"   Class 1 (Attack): {class_weights[1]:.4f}\")"
    "print(f\"   Class 0 (Benign): {class_weights[0]:.4f}\")\n",
    "\n",
    "class_weights = dict(zip(classes, weights))\n",
    "weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "classes = np.unique(y_train)\n",
    "\n",
    "print(\"\\n‚öñÔ∏è ƒêANG T√çNH CLASS WEIGHTS...\")\n",
    "# T√≠nh class weights\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "print(f\"   {'Total':<10} {len(X_train)+len(X_val)+len(X_test):>12,}\")"
    "print(f\"   {'-'*60}\")\n",
    "print(f\"   {'Test':<10} {len(X_test):>12,} {(y_test==0).sum():>12,} {(y_test==1).sum():>12,} {(y_test==1).sum()/len(y_test)*100:>9.1f}%\")\n",
    "print(f\"   {'Val':<10} {len(X_val):>12,} {(y_val==0).sum():>12,} {(y_val==1).sum():>12,} {(y_val==1).sum()/len(y_val)*100:>9.1f}%\")\n",
    "print(f\"   {'Train':<10} {len(X_train):>12,} {(y_train==0).sum():>12,} {(y_train==1).sum():>12,} {(y_train==1).sum()/len(y_train)*100:>9.1f}%\")\n",
    "print(f\"   {'-'*60}\")\n",
    "print(f\"   {'Set':<10} {'Samples':>12} {'Benign':>12} {'Attack':>12} {'Attack%':>10}\")\n",
    "print(f\"   {'='*60}\")\n",
    "print(f\"\\n   üìà K·∫æT QU·∫¢ CHIA D·ªÆ LI·ªÜU:\")\n",
    "\n",
    "gc.collect()\n",
    "del X, y, X_temp, y_temp\n",
    "\n",
    ")\n",
    "    X_temp, y_temp, test_size=val_ratio, random_state=RANDOM_STATE, stratify=y_temp\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "val_ratio = VAL_SIZE / (1 - TEST_SIZE)\n",
    "# Chia train / val\n",
    "\n",
    ")\n",
    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "# Chia train+val / test\n",
    "\n",
    "print(\"\\nüìä ƒêANG CHIA D·ªÆ LI·ªÜU TRAIN/VAL/TEST...\")\n",
    "# Chia train/val/test\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "print(f\"      (samples, features, channels)\")"
    "print(f\"   ‚úÖ Shape: {X.shape}\")\n",
    "\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "print(\"\\nüîÑ ƒêANG RESHAPE CHO CNN 1D...\")\n",
    "# Reshape cho CNN 1D\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "print(f\"      Std:  {X.std():.6f}\")"
    "print(f\"      Mean: {X.mean():.6f}\")\n",
    "print(f\"   ‚úÖ StandardScaler ho√†n t·∫•t\")\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print(\"\\nüìê ƒêANG CHU·∫®N H√ìA B·∫∞NG STANDARDSCALER...\")\n",
    "# Chu·∫©n h√≥a b·∫±ng StandardScaler\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "print(f\"      Range: [{X.min():.4f}, {X.max():.4f}]\")"
    "print(f\"   ‚úÖ Log transform ho√†n t·∫•t\")\n",
    "\n",
    "X = np.log1p(X)\n",
    "# √Åp d·ª•ng log(1+x)\n",
    "\n",
    "    X = X - min_val\n",
    "    print(f\"   ‚ö†Ô∏è Ph√°t hi·ªán gi√° tr·ªã √¢m (min={min_val:.4f}), ƒëang shift...\")\n",
    "if min_val < 0:\n",
    "min_val = X.min()\n",
    "# ƒê·∫£m b·∫£o kh√¥ng c√≥ gi√° tr·ªã √¢m\n",
    "\n",
    "print(\"\\nüî¢ ƒêANG √ÅP D·ª§NG LOG TRANSFORM: log_e(1+x)...\")\n",
    "# √Åp d·ª•ng Log Transform: log_e(1+x)\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "print(f\"   y: {y.shape}\")"
    "print(f\"   X: {X.shape}\")\n",
    "print(f\"\\nüìä Shape:\")\n",
    "\n",
    "gc.collect()\n",
    "del df_balanced\n",
    "\n",
    "y = df_balanced['binary_label'].values\n",
    "X = df_balanced.drop(columns=['binary_label']).values\n",
    "# T√°ch features v√† labels\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "gc.collect()"
    "del df_cleaned\n",
    "# Gi·∫£i ph√≥ng b·ªô nh·ªõ\n",
    "\n",
    "df_balanced = balanced_sample(df_cleaned, TARGET_BENIGN, TARGET_ATTACK)\n",
    "\n",
    "    return df_balanced\n",
    "    \n",
    "    gc.collect()\n",
    "    del df_benign, df_attack, df_benign_sampled, df_attack_sampled\n",
    "    \n",
    "    print(f\"   - T·ªïng: {total:,}\")\n",
    "    print(f\"   - Attack: {actual_attack:,} ({actual_attack/total*100:.1f}%)\")\n",
    "    print(f\"   - Benign: {actual_benign:,} ({actual_benign/total*100:.1f}%)\")\n",
    "    print(f\"\\n   ‚úÖ K·∫øt qu·∫£:\")\n",
    "    total = len(df_balanced)\n",
    "    \n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "    df_balanced = pd.concat([df_benign_sampled, df_attack_sampled], ignore_index=True)\n",
    "    # G·ªôp v√† shuffle\n",
    "    \n",
    "    df_attack_sampled = df_attack.sample(n=actual_attack, random_state=RANDOM_STATE)\n",
    "    df_benign_sampled = df_benign.sample(n=actual_benign, random_state=RANDOM_STATE)\n",
    "    # Random sample\n",
    "    \n",
    "    print(f\"   - Attack: {actual_attack:,}\")\n",
    "    print(f\"   - Benign: {actual_benign:,}\")\n",
    "    print(f\"\\n   S·ªë l∆∞·ª£ng s·∫Ω l·∫•y:\")\n",
    "    \n",
    "        actual_attack = int(actual_benign * (ATTACK_RATIO / BENIGN_RATIO))\n",
    "    if actual_benign < int(actual_attack * (BENIGN_RATIO / ATTACK_RATIO)):\n",
    "    # ƒêi·ªÅu ch·ªânh n·∫øu c·∫ßn\n",
    "    \n",
    "    actual_benign = min(actual_benign, n_benign)\n",
    "    actual_benign = int(actual_attack * (BENIGN_RATIO / ATTACK_RATIO))\n",
    "    actual_attack = min(target_attack, n_attack)\n",
    "    # X√°c ƒë·ªãnh s·ªë l∆∞·ª£ng th·ª±c t·∫ø\n",
    "    \n",
    "    print(f\"   - Attack: {target_attack:,} ({ATTACK_RATIO*100:.0f}%)\")\n",
    "    print(f\"   - Benign: {target_benign:,} ({BENIGN_RATIO*100:.0f}%)\")\n",
    "    print(f\"\\n   Target mong mu·ªën:\")\n",
    "    \n",
    "    print(f\"   - Attack: {n_attack:,}\")\n",
    "    print(f\"   - Benign: {n_benign:,}\")\n",
    "    print(f\"\\n   D·ªØ li·ªáu g·ªëc:\")\n",
    "    \n",
    "    n_attack = len(df_attack)\n",
    "    n_benign = len(df_benign)\n",
    "    \n",
    "    df_attack = df[df['binary_label'] == 1]\n",
    "    df_benign = df[df['binary_label'] == 0]\n",
    "    # T√°ch theo class\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"‚öñÔ∏è ƒêANG C√ÇN B·∫∞NG D·ªÆ LI·ªÜU\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \"\"\"\n",
    "    Sample d·ªØ li·ªáu v·ªõi t·ª∑ l·ªá c√¢n b·∫±ng mong mu·ªën\n",
    "    \"\"\"\n",
    "def balanced_sample(df, target_benign, target_attack):\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "---"
    "# ‚öñÔ∏è B∆Ø·ªöC 2: C√ÇN B·∫∞NG V√Ä CHU·∫®N B·ªä D·ªÆ LI·ªÜU TRAINING\n",
    "---\n",
   "source": [
   "metadata": {},
   "cell_type": "markdown",
  {
  },
   ]
    "print(f\"\\n‚úÖ HO√ÄN TH√ÄNH B∆Ø·ªöC 1 - CLEAN D·ªÆ LI·ªÜU\")"
    "\n",
    "print(f\"   ‚úÖ ƒê√£ l∆∞u: column_modes.pkl\")\n",
    "    pickle.dump(column_modes, f)\n",
    "with open(os.path.join(CLEANED_DATA_DIR, 'column_modes.pkl'), 'wb') as f:\n",
    "# L∆∞u column modes\n",
    "\n",
    "print(f\"   ‚úÖ ƒê√£ l∆∞u: feature_names.txt ({len(feature_names)} features)\")\n",
    "        f.write(name + '\\n')\n",
    "    for name in feature_names:\n",
    "with open(os.path.join(CLEANED_DATA_DIR, 'feature_names.txt'), 'w') as f:\n",
    "feature_names = [c for c in df_cleaned.columns if c != 'binary_label']\n",
    "# L∆∞u feature names\n",
    "\n",
    "print(f\"   ‚úÖ ƒê√£ l∆∞u: {parquet_path}\")\n",
    "df_cleaned.to_parquet(parquet_path, index=False)\n",
    "parquet_path = os.path.join(CLEANED_DATA_DIR, 'cleaned_data.parquet')\n",
    "# L∆∞u parquet\n",
    "\n",
    "print(\"\\nüíæ ƒêANG L∆ØU D·ªÆ LI·ªÜU ƒê√É CLEAN...\")\n",
    "# L∆∞u d·ªØ li·ªáu ƒë√£ clean\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "df_cleaned, clean_stats = clean_all_files(csv_files, zero_variance_cols, column_modes)"
    "\n",
    "    return df_combined, stats\n",
    "    \n",
    "    print(f\"   Attack: {attack_count:,} ({attack_count/len(df_combined)*100:.1f}%)\")\n",
    "    print(f\"   Benign: {benign_count:,} ({benign_count/len(df_combined)*100:.1f}%)\")\n",
    "    print(f\"\\n   üìä PH√ÇN B·ªê NH√ÉN SAU CLEAN:\")\n",
    "    \n",
    "    attack_count = (df_combined['binary_label'] == 1).sum()\n",
    "    benign_count = (df_combined['binary_label'] == 0).sum()\n",
    "    # Th·ªëng k√™\n",
    "    \n",
    "    print(f\"   S·ªë duplicate ƒë√£ lo·∫°i: {duplicates_removed:,}\")\n",
    "    print(f\"   S·ªë m·∫´u sau khi lo·∫°i duplicate: {len(df_combined):,}\")\n",
    "    \n",
    "    duplicates_removed = rows_before - rows_after\n",
    "    rows_after = len(df_combined)\n",
    "    df_combined = df_combined.drop_duplicates()\n",
    "    rows_before = len(df_combined)\n",
    "    print(\"   ƒêang lo·∫°i b·ªè duplicate...\")\n",
    "    # Lo·∫°i b·ªè duplicate\n",
    "    \n",
    "    print(f\"   T·ªïng s·ªë m·∫´u sau khi g·ªôp: {len(df_combined):,}\")\n",
    "    \n",
    "    gc.collect()\n",
    "    del all_dataframes\n",
    "    df_combined = pd.concat(all_dataframes, ignore_index=True)\n",
    "    print(\"\\n   ƒêang g·ªôp d·ªØ li·ªáu...\")\n",
    "    # G·ªôp t·∫•t c·∫£\n",
    "    \n",
    "            gc.collect()\n",
    "            del processed_chunks, df_file\n",
    "            print(f\"   ‚úÖ ƒê√£ x·ª≠ l√Ω: {len(df_file):,} m·∫´u\")\n",
    "            all_dataframes.append(df_file)\n",
    "            df_file = pd.concat(processed_chunks, ignore_index=True)\n",
    "        if processed_chunks:\n",
    "        \n",
    "            gc.collect()\n",
    "            processed_chunks.append(chunk)\n",
    "            \n",
    "                    chunk[col] = chunk[col].fillna(mode_val)\n",
    "                    chunk[col] = chunk[col].replace([np.inf, -np.inf], np.nan)\n",
    "                    # Thay th·∫ø\n",
    "                    \n",
    "                    stats['nan_replaced'] += nan_mask.sum()\n",
    "                    stats['inf_replaced'] += inf_mask.sum()\n",
    "                    nan_mask = chunk[col].isna()\n",
    "                    inf_mask = np.isinf(chunk[col])\n",
    "                    # ƒê·∫øm inf v√† nan\n",
    "                    \n",
    "                    mode_val = column_modes[col]\n",
    "                if col in column_modes:\n",
    "            for col in feature_cols:\n",
    "            feature_cols = [c for c in chunk.columns if c != 'binary_label']\n",
    "            # X·ª≠ l√Ω NaN/Inf b·∫±ng mode\n",
    "            \n",
    "            chunk = chunk.drop(columns=cols_zv)\n",
    "            cols_zv = [c for c in zero_variance_cols if c in chunk.columns]\n",
    "            # Lo·∫°i b·ªè zero-variance columns\n",
    "            \n",
    "            chunk = chunk.drop(columns=[LABEL_COLUMN])\n",
    "            chunk['binary_label'] = (chunk[LABEL_COLUMN] != 'benign').astype(int)\n",
    "            chunk = chunk[chunk[LABEL_COLUMN] != 'label']  # Lo·∫°i header l·∫´n v√†o\n",
    "            chunk[LABEL_COLUMN] = chunk[LABEL_COLUMN].astype(str).str.strip().str.lower()\n",
    "            # Chuy·ªÉn ƒë·ªïi nh√£n sang binary\n",
    "            \n",
    "                    chunk[col] = pd.to_numeric(chunk[col], errors='coerce')\n",
    "                if chunk[col].dtype == 'object':\n",
    "            for col in feature_cols:\n",
    "            feature_cols = [c for c in chunk.columns if c != LABEL_COLUMN]\n",
    "            # Chuy·ªÉn sang numeric\n",
    "            \n",
    "            chunk = chunk.drop(columns=cols_to_drop)\n",
    "            cols_to_drop = [c for c in COLUMNS_TO_DROP if c in chunk.columns]\n",
    "            # Lo·∫°i b·ªè c·ªôt identification\n",
    "            \n",
    "            chunk.columns = chunk.columns.str.strip()\n",
    "            # Chu·∫©n h√≥a t√™n c·ªôt\n",
    "            \n",
    "            stats['total_rows_read'] += len(chunk)\n",
    "        for chunk in tqdm(chunk_iterator, desc=\"   Chunks\"):\n",
    "        \n",
    "                                     low_memory=False, encoding='utf-8')\n",
    "        chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size,\n",
    "        processed_chunks = []\n",
    "        print(f\"\\nüìÑ ƒêang x·ª≠ l√Ω: {csv_file.name}\")\n",
    "    for csv_file in csv_files:\n",
    "    \n",
    "    }\n",
    "        'inf_replaced': 0\n",
    "        'nan_replaced': 0,\n",
    "        'total_rows_read': 0,\n",
    "    stats = {\n",
    "    all_dataframes = []\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"üìä B∆Ø·ªöC 1.2: CLEAN D·ªÆ LI·ªÜU\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \"\"\"\n",
    "    - Chuy·ªÉn nh√£n sang binary\n",
    "    - X·ª≠ l√Ω NaN/Inf b·∫±ng mode\n",
    "    - Lo·∫°i b·ªè zero-variance columns\n",
    "    - Lo·∫°i b·ªè c·ªôt identification\n",
    "    Clean t·∫•t c·∫£ c√°c file CSV\n",
    "    \"\"\"\n",
    "def clean_all_files(csv_files, zero_variance_cols, column_modes, chunk_size=CHUNK_SIZE):\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "all_columns, zero_variance_cols, column_modes = first_pass_collect_info(csv_files)"
    "\n",
    "    return all_columns, zero_variance_cols, column_modes\n",
    "    \n",
    "    print(f\"   ‚úÖ S·ªë c·ªôt s·∫Ω gi·ªØ l·∫°i: {len(all_columns) - len(zero_variance_cols)}\")\n",
    "        print(f\"      C√°c c·ªôt: {zero_variance_cols}\")\n",
    "    if zero_variance_cols:\n",
    "    print(f\"\\n   ‚úÖ S·ªë c·ªôt zero-variance s·∫Ω lo·∫°i b·ªè: {len(zero_variance_cols)}\")\n",
    "    \n",
    "                column_modes[col] = 0\n",
    "            else:\n",
    "                column_modes[col] = mode_val\n",
    "                mode_val = max(column_value_counts[col], key=column_value_counts[col].get)\n",
    "            if column_value_counts[col]:\n",
    "        if col not in zero_variance_cols:\n",
    "    for col in all_columns:\n",
    "    column_modes = {}\n",
    "    # T√≠nh mode cho m·ªói c·ªôt\n",
    "    \n",
    "            zero_variance_cols.append(col)\n",
    "        if column_min_max[col]['min'] == column_min_max[col]['max']:\n",
    "    for col in all_columns:\n",
    "    zero_variance_cols = []\n",
    "    # X√°c ƒë·ªãnh zero-variance columns\n",
    "    \n",
    "            gc.collect()\n",
    "            \n",
    "                            column_value_counts[col][val] += count\n",
    "                                column_value_counts[col][val] = 0\n",
    "                            if val not in column_value_counts[col]:\n",
    "                        for val, count in vc.items():\n",
    "                        vc = valid_data.value_counts().head(10).to_dict()\n",
    "                        # Thu th·∫≠p value counts cho mode\n",
    "                        \n",
    "                        column_min_max[col]['max'] = max(column_min_max[col]['max'], valid_data.max())\n",
    "                        column_min_max[col]['min'] = min(column_min_max[col]['min'], valid_data.min())\n",
    "                        # C·∫≠p nh·∫≠t min/max\n",
    "                    if len(valid_data) > 0:\n",
    "                    \n",
    "                    valid_data = col_data.dropna()\n",
    "                    col_data = chunk[col].replace([np.inf, -np.inf], np.nan)\n",
    "                if col in chunk.columns:\n",
    "            for col in all_columns:\n",
    "            # Thu th·∫≠p th√¥ng tin cho m·ªói c·ªôt\n",
    "            \n",
    "                    column_min_max[col] = {'min': np.inf, 'max': -np.inf}\n",
    "                    column_value_counts[col] = {}\n",
    "                for col in all_columns:\n",
    "                all_columns = feature_cols\n",
    "            if all_columns is None:\n",
    "            \n",
    "                    chunk[col] = pd.to_numeric(chunk[col], errors='coerce')\n",
    "                if chunk[col].dtype == 'object':\n",
    "            for col in feature_cols:\n",
    "            feature_cols = [c for c in chunk.columns if c != LABEL_COLUMN]\n",
    "            # Chuy·ªÉn sang numeric\n",
    "            \n",
    "            chunk = chunk.drop(columns=cols_to_drop)\n",
    "            cols_to_drop = [c for c in COLUMNS_TO_DROP if c in chunk.columns]\n",
    "            # Lo·∫°i b·ªè c·ªôt identification\n",
    "            \n",
    "            chunk.columns = chunk.columns.str.strip()\n",
    "            # Chu·∫©n h√≥a t√™n c·ªôt\n",
    "        for chunk in tqdm(chunk_iterator, desc=\"   Chunks\"):\n",
    "        \n",
    "                                    low_memory=False, encoding='utf-8')\n",
    "        chunk_iterator = pd.read_csv(csv_file, chunksize=chunk_size,\n",
    "        print(f\"\\n   ƒêang scan: {csv_file.name}\")\n",
    "    for csv_file in csv_files:\n",
    "    \n",
    "    column_min_max = {}  # ƒê·ªÉ ki·ªÉm tra variance\n",
    "    column_value_counts = {}  # ƒê·ªÉ t√≠nh mode\n",
    "    all_columns = None\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"üìä B∆Ø·ªöC 1.1: THU TH·∫¨P TH√îNG TIN T·ª™ D·ªÆ LI·ªÜU\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \"\"\"\n",
    "    - T√≠nh mode c·ªßa t·ª´ng c·ªôt ƒë·ªÉ thay th·∫ø NaN/Inf\n",
    "    - X√°c ƒë·ªãnh c√°c c·ªôt c√≥ variance = 0\n",
    "    M·ª•c ƒë√≠ch:\n",
    "    L·∫ßn ƒë·ªçc ƒë·∫ßu ti√™n: Thu th·∫≠p th√¥ng tin v·ªÅ columns v√† t√≠nh mode\n",
    "    \"\"\"\n",
    "def first_pass_collect_info(csv_files, chunk_size=CHUNK_SIZE):\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "csv_files = get_csv_files(DATA_DIR)"
    "\n",
    "    return sorted(csv_files)\n",
    "        print(f\"   - {f.name}\")\n",
    "    for f in sorted(csv_files):\n",
    "    print(f\"\\nüìÇ T√¨m th·∫•y {len(csv_files)} file CSV:\")\n",
    "    \n",
    "        raise FileNotFoundError(f\"Kh√¥ng t√¨m th·∫•y file CSV trong {data_dir}\")\n",
    "    if not csv_files:\n",
    "    \n",
    "        csv_files = [f for f in data_dir.glob(\"*.csv\") if not f.name.endswith('.zip')]\n",
    "    if not csv_files:\n",
    "    csv_files = list(data_dir.glob(\"*_TrafficForML_CICFlowMeter.csv\"))\n",
    "    data_dir = Path(data_dir)\n",
    "    \"\"\"L·∫•y danh s√°ch c√°c file CSV\"\"\"\n",
    "def get_csv_files(data_dir):\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "---"
    "# üßπ B∆Ø·ªöC 1: CLEAN D·ªÆ LI·ªÜU\n",
    "---\n",
   "source": [
   "metadata": {},
   "cell_type": "markdown",
  {
  },
   ]
    "print(f\"   Attack: {TARGET_ATTACK:,} ({ATTACK_RATIO*100:.0f}%)\")"
    "print(f\"   Benign: {TARGET_BENIGN:,} ({BENIGN_RATIO*100:.0f}%)\")\n",
    "print(f\"   T·ªïng m·∫´u: {TOTAL_SAMPLES:,}\")\n",
    "print(f\"\\nüìã C·∫§U H√åNH:\")\n",
    "\n",
    "LABEL_COLUMN = 'Label'\n",
    "]\n",
    "    'Flow ID', 'Src IP', 'Dst IP', 'Src Port', 'Dst Port', 'Timestamp'\n",
    "COLUMNS_TO_DROP = [\n",
    "# ============================================================================\n",
    "# C√ÅC C·ªòT C·∫¶N LO·∫†I B·ªé\n",
    "# ============================================================================\n",
    "\n",
    "PATIENCE = 10\n",
    "DROPOUT_RATE = 0.5\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "# ============================================================================\n",
    "# C·∫§U H√åNH TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "VAL_SIZE = 0.10    # 10% validation\n",
    "TEST_SIZE = 0.20   # 20% test\n",
    "# T·ª∑ l·ªá chia train/val/test\n",
    "\n",
    "TARGET_ATTACK = int(TOTAL_SAMPLES * ATTACK_RATIO)  # 900,000\n",
    "TARGET_BENIGN = int(TOTAL_SAMPLES * BENIGN_RATIO)  # 2,100,000\n",
    "\n",
    "ATTACK_RATIO = 0.30        # 30% Attack\n",
    "BENIGN_RATIO = 0.70        # 70% Benign\n",
    "TOTAL_SAMPLES = 3000000    # T·ªïng s·ªë m·∫´u mong mu·ªën (3 tri·ªáu)\n",
    "# ============================================================================\n",
    "# C·∫§U H√åNH C√ÇN B·∫∞NG D·ªÆ LI·ªÜU\n",
    "# ============================================================================\n",
    "\n",
    "RANDOM_STATE = 42          # Random seed\n",
    "CHUNK_SIZE = 300000        # S·ªë d√≤ng m·ªói chunk khi ƒë·ªçc CSV\n",
    "# ============================================================================\n",
    "# C·∫§U H√åNH X·ª¨ L√ù D·ªÆ LI·ªÜU\n",
    "# ============================================================================\n",
    "\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "for d in [CLEANED_DATA_DIR, TRAINING_DATA_DIR, MODEL_DIR]:\n",
    "\n",
    "MODEL_DIR = os.path.join(OUTPUT_DIR, \"models\")\n",
    "TRAINING_DATA_DIR = os.path.join(OUTPUT_DIR, \"training_data\")\n",
    "CLEANED_DATA_DIR = os.path.join(OUTPUT_DIR, \"cleaned_data\")\n",
    "# T·∫°o c√°c th∆∞ m·ª•c con\n",
    "\n",
    "    print(\"üíª ƒêang ch·∫°y tr√™n LOCAL\")\n",
    "    OUTPUT_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CNN\"\n",
    "    DATA_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CICIDS2018-CSV\"\n",
    "    # ƒê∆∞·ªùng d·∫´n Local\n",
    "else:\n",
    "    print(\"üåê ƒêang ch·∫°y tr√™n KAGGLE\")\n",
    "    OUTPUT_DIR = \"/kaggle/working\"\n",
    "    DATA_DIR = \"/kaggle/input/cicids2018\"  # Thay ƒë·ªïi theo t√™n dataset c·ªßa b·∫°n\n",
    "    # ƒê∆∞·ªùng d·∫´n Kaggle - THAY ƒê·ªîI T√äN DATASET N·∫æU C·∫¶N\n",
    "if IS_KAGGLE:\n",
    "\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
    "# ============================================================================\n",
    "# C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N\n",
    "# ============================================================================\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "## ‚öôÔ∏è C·∫§U H√åNH"
   "source": [
   "metadata": {},
   "cell_type": "markdown",
  {
  },
   ]
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "# Progress bar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualization\n",
    "\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    ")\n",
    "    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import (\n",
    ")\n",
    "    Dropout, BatchNormalization, Input\n",
    "    Conv1D, MaxPooling1D, Flatten, Dense,\n",
    "from tensorflow.keras.layers import (\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "# TensorFlow/Keras\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Sklearn\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
   "source": [
   "outputs": [],
   "metadata": {},
   "execution_count": null,
   "cell_type": "code",
  {
  },
   ]
    "## üì¶ IMPORT TH∆Ø VI·ªÜN"
   "source": [
   "metadata": {},
   "cell_type": "markdown",
  {
  },
   ]
    "- Output: Dense(1, sigmoid) cho binary classification"
    "- BatchNormalization + Dropout\n",
    "- 5 l·ªõp Conv1D v·ªõi MaxPooling\n",
    "**Ki·∫øn tr√∫c CNN:**\n",
    "\n",
    "3. **B∆∞·ªõc 3:** Train m√¥ h√¨nh CNN\n",
    "2. **B∆∞·ªõc 2:** C√¢n b·∫±ng d·ªØ li·ªáu v√† chu·∫©n b·ªã training (70% Benign, 30% Attack)\n",
    "1. **B∆∞·ªõc 1:** Clean d·ªØ li·ªáu (lo·∫°i b·ªè c·ªôt ID, x·ª≠ l√Ω NaN/Inf, lo·∫°i duplicate)\n",
    "**C√°c b∆∞·ªõc th·ª±c hi·ªán:**\n",
    "\n",
    "---\n",
    "\n",
    "### Dataset: CSE-CIC-IDS2018\n",
    "## Binary Classification: Benign vs Attack\n",
    "# üß† PH√ÅT HI·ªÜN L∆ØU L∆Ø·ª¢NG M·∫†NG IOT B·∫§T TH∆Ø·ªúNG B·∫∞NG CNN\n",
   "source": [
   "metadata": {},
   "cell_type": "markdown",
  {
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

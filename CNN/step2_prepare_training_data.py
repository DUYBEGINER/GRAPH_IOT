"""
======================================================================================
BÆ¯á»šC 2: CHUáº¨N Bá»Š Dá»® LIá»†U TRAINING CHO CNN - CÃ‚N Báº°NG VÃ€ CHIA TRAIN/VAL/TEST
======================================================================================

Script nÃ y thá»±c hiá»‡n:
1. Äá»c dá»¯ liá»‡u Ä‘Ã£ clean tá»« step1
2. CÃ¢n báº±ng sá»‘ lÆ°á»£ng nhÃ£n (70% Benign, 30% Attack hoáº·c tá»· lá»‡ tÃ¹y chá»‰nh)
3. Ãp dá»¥ng Log Transform: log_e(1+x)
4. Chuáº©n hÃ³a báº±ng StandardScaler
5. Reshape cho CNN 1D
6. Chia train/val/test vá»›i stratify Ä‘á»ƒ giá»¯ tá»· lá»‡
7. LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ Ä‘á»ƒ train

CÃ³ thá»ƒ cháº¡y trÃªn cáº£ Kaggle vÃ  Local
"""

import os
import numpy as np
import pandas as pd
import pickle
import json
import gc
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# THÆ¯ VIá»†N CHUáº¨N HÃ“A VÃ€ Xá»¬ LÃ Dá»® LIá»†U
# ============================================================================
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Kiá»ƒm tra mÃ´i trÆ°á»ng cháº¡y (Kaggle hoáº·c Local)
IS_KAGGLE = os.path.exists('/kaggle/input')

# ============================================================================
# Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN
# ============================================================================
if IS_KAGGLE:
    CLEANED_DATA_DIR = "/kaggle/working/cleaned_data"
    OUTPUT_DIR = "/kaggle/working/training_data"
    print("ğŸŒ Äang cháº¡y trÃªn KAGGLE")
else:
    CLEANED_DATA_DIR = r"D:\PROJECT\Machine Learning\IOT\CNN\cleaned_data"
    OUTPUT_DIR = r"D:\PROJECT\Machine Learning\IOT\CNN\training_data"
    print("ğŸ’» Äang cháº¡y trÃªn LOCAL")

# ============================================================================
# Cáº¤U HÃŒNH CÃ‚N Báº°NG Dá»® LIá»†U
# ============================================================================

# Random state Ä‘á»ƒ tÃ¡i táº¡o káº¿t quáº£
RANDOM_STATE = 42

# Tá»•ng sá»‘ máº«u mong muá»‘n (train + val + test)
TOTAL_SAMPLES = 3000000  # 3 triá»‡u máº«u

# Tá»· lá»‡ pháº§n trÄƒm cho má»—i class
BENIGN_RATIO = 0.70  # 70% Benign
ATTACK_RATIO = 0.30  # 30% Attack

# TÃ­nh sá»‘ lÆ°á»£ng máº«u cho má»—i class
TARGET_BENIGN = int(TOTAL_SAMPLES * BENIGN_RATIO)  # 2,100,000
TARGET_ATTACK = int(TOTAL_SAMPLES * ATTACK_RATIO)  # 900,000

# Tá»· lá»‡ chia train/val/test
TEST_SIZE = 0.20   # 20% cho test
VAL_SIZE = 0.10    # 10% cho validation (tá»« tá»•ng)
# Train sáº½ lÃ  70%

# ============================================================================
# CLASS CHUáº¨N Bá»Š Dá»® LIá»†U TRAINING
# ============================================================================

class TrainingDataPreparer:
    """
    Class chuáº©n bá»‹ dá»¯ liá»‡u training cho CNN

    CÃ¡c bÆ°á»›c:
    1. Äá»c dá»¯ liá»‡u Ä‘Ã£ clean
    2. CÃ¢n báº±ng dá»¯ liá»‡u theo tá»· lá»‡ mong muá»‘n
    3. Ãp dá»¥ng log transform: log_e(1+x)
    4. Chuáº©n hÃ³a báº±ng StandardScaler
    5. Reshape cho CNN
    6. Chia train/val/test
    7. LÆ°u dá»¯ liá»‡u
    """

    def __init__(self, cleaned_data_dir, output_dir,
                 total_samples=TOTAL_SAMPLES,
                 benign_ratio=BENIGN_RATIO,
                 attack_ratio=ATTACK_RATIO,
                 test_size=TEST_SIZE,
                 val_size=VAL_SIZE):
        """
        Khá»Ÿi táº¡o preparer

        Args:
            cleaned_data_dir: ÄÆ°á»ng dáº«n thÆ° má»¥c chá»©a dá»¯ liá»‡u Ä‘Ã£ clean
            output_dir: ÄÆ°á»ng dáº«n thÆ° má»¥c lÆ°u káº¿t quáº£
            total_samples: Tá»•ng sá»‘ máº«u mong muá»‘n
            benign_ratio: Tá»· lá»‡ Benign (0-1)
            attack_ratio: Tá»· lá»‡ Attack (0-1)
            test_size: Tá»· lá»‡ test set
            val_size: Tá»· lá»‡ validation set
        """
        self.cleaned_data_dir = Path(cleaned_data_dir)
        self.output_dir = Path(output_dir)
        self.total_samples = total_samples
        self.benign_ratio = benign_ratio
        self.attack_ratio = attack_ratio
        self.test_size = test_size
        self.val_size = val_size

        # TÃ­nh target cho má»—i class
        self.target_benign = int(total_samples * benign_ratio)
        self.target_attack = int(total_samples * attack_ratio)

        # Khá»Ÿi táº¡o scaler
        self.scaler = StandardScaler()

        # Táº¡o thÆ° má»¥c output
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # LÆ°u tÃªn features
        self.feature_names = None

        # Thá»‘ng kÃª
        self.stats = {
            'original_benign': 0,
            'original_attack': 0,
            'sampled_benign': 0,
            'sampled_attack': 0,
            'train_samples': 0,
            'val_samples': 0,
            'test_samples': 0,
            'n_features': 0
        }

    def load_cleaned_data(self):
        """Äá»c dá»¯ liá»‡u Ä‘Ã£ clean tá»« step1"""
        print("\n" + "="*80)
        print("ğŸ“‚ ÄANG Äá»ŒC Dá»® LIá»†U ÄÃƒ CLEAN...")
        print("="*80)

        parquet_path = self.cleaned_data_dir / 'cleaned_data.parquet'

        if not parquet_path.exists():
            raise FileNotFoundError(
                f"KhÃ´ng tÃ¬m tháº¥y file {parquet_path}\n"
                f"HÃ£y cháº¡y step1_clean_data.py trÆ°á»›c!"
            )

        df = pd.read_parquet(parquet_path)

        # Äá»c feature names
        feature_names_path = self.cleaned_data_dir / 'feature_names.txt'
        if feature_names_path.exists():
            with open(feature_names_path, 'r') as f:
                self.feature_names = [line.strip() for line in f.readlines()]
        else:
            self.feature_names = [col for col in df.columns if col != 'binary_label']

        # Thá»‘ng kÃª
        self.stats['original_benign'] = int((df['binary_label'] == 0).sum())
        self.stats['original_attack'] = int((df['binary_label'] == 1).sum())
        self.stats['n_features'] = len(self.feature_names)

        print(f"   âœ… ÄÃ£ Ä‘á»c: {len(df):,} máº«u")
        print(f"   ğŸ“Š PhÃ¢n bá»‘ gá»‘c:")
        print(f"      - Benign: {self.stats['original_benign']:,} ({self.stats['original_benign']/len(df)*100:.1f}%)")
        print(f"      - Attack: {self.stats['original_attack']:,} ({self.stats['original_attack']/len(df)*100:.1f}%)")
        print(f"   ğŸ“‹ Sá»‘ features: {self.stats['n_features']}")

        return df

    def balanced_sample(self, df):
        """
        Sample dá»¯ liá»‡u vá»›i tá»· lá»‡ cÃ¢n báº±ng mong muá»‘n

        Chiáº¿n lÆ°á»£c:
        - Náº¿u cÃ³ Ä‘á»§ máº«u: láº¥y Ä‘Ãºng sá»‘ lÆ°á»£ng target
        - Náº¿u khÃ´ng Ä‘á»§ Attack: giáº£m Benign tÆ°Æ¡ng á»©ng Ä‘á»ƒ giá»¯ tá»· lá»‡
        - Náº¿u khÃ´ng Ä‘á»§ cáº£ hai: láº¥y tá»‘i Ä‘a cÃ³ thá»ƒ vá»›i tá»· lá»‡ Ä‘Ãºng
        """
        print("\n" + "="*80)
        print("âš–ï¸ ÄANG CÃ‚N Báº°NG Dá»® LIá»†U...")
        print("="*80)

        # TÃ¡ch theo class
        df_benign = df[df['binary_label'] == 0]
        df_attack = df[df['binary_label'] == 1]

        n_benign = len(df_benign)
        n_attack = len(df_attack)

        print(f"\n   ğŸ¯ Target mong muá»‘n:")
        print(f"      - Tá»•ng: {self.total_samples:,}")
        print(f"      - Benign: {self.target_benign:,} ({self.benign_ratio*100:.0f}%)")
        print(f"      - Attack: {self.target_attack:,} ({self.attack_ratio*100:.0f}%)")

        # XÃ¡c Ä‘á»‹nh sá»‘ lÆ°á»£ng thá»±c táº¿ cÃ³ thá»ƒ láº¥y
        # Æ¯u tiÃªn giá»¯ Ä‘Ãºng tá»· lá»‡
        actual_attack = min(self.target_attack, n_attack)
        # TÃ­nh Benign dá»±a trÃªn Attack thá»±c táº¿ Ä‘á»ƒ giá»¯ tá»· lá»‡
        actual_benign = int(actual_attack * (self.benign_ratio / self.attack_ratio))
        actual_benign = min(actual_benign, n_benign)

        # Náº¿u Benign bá»‹ giá»›i háº¡n, Ä‘iá»u chá»‰nh Attack
        if actual_benign < int(actual_attack * (self.benign_ratio / self.attack_ratio)):
            actual_attack = int(actual_benign * (self.attack_ratio / self.benign_ratio))

        print(f"\n   ğŸ“Š Sá»‘ lÆ°á»£ng thá»±c táº¿ sáº½ láº¥y:")
        print(f"      - Benign: {actual_benign:,}")
        print(f"      - Attack: {actual_attack:,}")
        print(f"      - Tá»•ng: {actual_benign + actual_attack:,}")
        print(f"      - Tá»· lá»‡ thá»±c táº¿: {actual_benign/(actual_benign+actual_attack)*100:.1f}% - {actual_attack/(actual_benign+actual_attack)*100:.1f}%")

        if actual_benign < self.target_benign or actual_attack < self.target_attack:
            print(f"\n   âš ï¸ KhÃ´ng Ä‘á»§ máº«u Ä‘á»ƒ Ä‘áº¡t target!")
            print(f"      CÃ³ sáºµn: Benign={n_benign:,}, Attack={n_attack:,}")

        # Random sample tá»« má»—i class
        print(f"\n   ğŸ”„ Äang sample...")

        # Sá»­ dá»¥ng random sampling
        df_benign_sampled = df_benign.sample(n=actual_benign, random_state=RANDOM_STATE)
        df_attack_sampled = df_attack.sample(n=actual_attack, random_state=RANDOM_STATE)

        # Gá»™p láº¡i vÃ  shuffle
        df_balanced = pd.concat([df_benign_sampled, df_attack_sampled], ignore_index=True)
        df_balanced = df_balanced.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)

        # Cáº­p nháº­t stats
        self.stats['sampled_benign'] = actual_benign
        self.stats['sampled_attack'] = actual_attack

        print(f"\n   âœ… Káº¿t quáº£ sau khi cÃ¢n báº±ng:")
        print(f"      - Benign: {actual_benign:,} ({actual_benign/(actual_benign+actual_attack)*100:.1f}%)")
        print(f"      - Attack: {actual_attack:,} ({actual_attack/(actual_benign+actual_attack)*100:.1f}%)")
        print(f"      - Tá»•ng: {len(df_balanced):,}")

        # Giáº£i phÃ³ng bá»™ nhá»›
        del df_benign, df_attack, df_benign_sampled, df_attack_sampled
        gc.collect()

        return df_balanced

    def apply_log_transform(self, X):
        """
        Ãp dá»¥ng Log Transform: log_e(1+x)

        LÆ°u Ã½: log(1+x) giÃºp:
        - Giáº£m skewness cá»§a dá»¯ liá»‡u
        - Xá»­ lÃ½ cÃ¡c giÃ¡ trá»‹ lá»›n
        - Báº£o toÃ n giÃ¡ trá»‹ 0 (log(1+0) = 0)
        """
        print("\nğŸ”¢ ÄANG ÃP Dá»¤NG LOG TRANSFORM: log_e(1+x)...")

        # Äáº£m báº£o khÃ´ng cÃ³ giÃ¡ trá»‹ Ã¢m (log khÃ´ng xÃ¡c Ä‘á»‹nh cho sá»‘ Ã¢m)
        # Vá»›i dá»¯ liá»‡u network flow, cÃ¡c giÃ¡ trá»‹ thÆ°á»ng >= 0
        # Náº¿u cÃ³ giÃ¡ trá»‹ Ã¢m, ta shift Ä‘á»ƒ min = 0
        min_val = X.min()
        if min_val < 0:
            print(f"   âš ï¸ PhÃ¡t hiá»‡n giÃ¡ trá»‹ Ã¢m (min={min_val:.4f}), Ä‘ang shift...")
            X = X - min_val  # Shift Ä‘á»ƒ min = 0

        # Ãp dá»¥ng log(1+x)
        X_log = np.log1p(X)  # log1p(x) = log(1+x), á»•n Ä‘á»‹nh hÆ¡n vá»›i x nhá»

        print(f"   âœ… Log transform hoÃ n táº¥t")
        print(f"      Range trÆ°á»›c: [{X.min():.4f}, {X.max():.4f}]")
        print(f"      Range sau:   [{X_log.min():.4f}, {X_log.max():.4f}]")

        return X_log

    def normalize_features(self, X):
        """
        Chuáº©n hÃ³a features báº±ng StandardScaler
        """
        print("\nğŸ“ ÄANG CHUáº¨N HÃ“A Báº°NG STANDARDSCALER...")

        X_normalized = self.scaler.fit_transform(X)

        print(f"   âœ… StandardScaler hoÃ n táº¥t")
        print(f"      Mean: {X_normalized.mean():.6f}")
        print(f"      Std:  {X_normalized.std():.6f}")

        return X_normalized

    def reshape_for_cnn(self, X):
        """
        Reshape dá»¯ liá»‡u cho CNN 1D
        CNN 1D yÃªu cáº§u input shape: (samples, features, channels)
        """
        print("\nğŸ”„ ÄANG RESHAPE CHO CNN 1D...")

        X_reshaped = X.reshape(X.shape[0], X.shape[1], 1)

        print(f"   âœ… Shape: {X.shape} -> {X_reshaped.shape}")
        print(f"      (samples, features, channels)")

        return X_reshaped

    def split_data(self, X, y):
        """
        Chia dá»¯ liá»‡u thÃ nh train/val/test

        ThÃªm validation: Train 70%, Val 10%, Test 20%

        Sá»­ dá»¥ng stratify Ä‘á»ƒ giá»¯ tá»· lá»‡ class trong táº¥t cáº£ cÃ¡c táº­p
        """
        print("\nğŸ“Š ÄANG CHIA Dá»® LIá»†U TRAIN/VAL/TEST...")

        # BÆ°á»›c 1: Chia train+val / test (80/20)
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y,
            test_size=self.test_size,
            random_state=RANDOM_STATE,
            stratify=y  # Giá»¯ tá»· lá»‡ class
        )

        # BÆ°á»›c 2: Chia train / val
        val_ratio_from_temp = self.val_size / (1 - self.test_size)
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp,
            test_size=val_ratio_from_temp,
            random_state=RANDOM_STATE,
            stratify=y_temp
        )

        # Cáº­p nháº­t stats
        self.stats['train_samples'] = len(X_train)
        self.stats['val_samples'] = len(X_val)
        self.stats['test_samples'] = len(X_test)

        print(f"\n   ğŸ“ˆ Káº¾T QUáº¢ CHIA Dá»® LIá»†U:")
        print(f"   {'='*50}")
        print(f"   {'Set':<10} {'Samples':>12} {'Benign':>12} {'Attack':>12}")
        print(f"   {'-'*50}")
        print(f"   {'Train':<10} {len(X_train):>12,} {(y_train==0).sum():>12,} {(y_train==1).sum():>12,}")
        print(f"   {'Val':<10} {len(X_val):>12,} {(y_val==0).sum():>12,} {(y_val==1).sum():>12,}")
        print(f"   {'Test':<10} {len(X_test):>12,} {(y_test==0).sum():>12,} {(y_test==1).sum():>12,}")
        print(f"   {'-'*50}")
        print(f"   {'Total':<10} {len(X_train)+len(X_val)+len(X_test):>12,}")

        # Kiá»ƒm tra tá»· lá»‡
        print(f"\n   ğŸ“Š Tá»¶ Lá»† ATTACK TRONG Má»–I Táº¬P:")
        print(f"      Train: {(y_train==1).sum()/len(y_train)*100:.1f}%")
        print(f"      Val:   {(y_val==1).sum()/len(y_val)*100:.1f}%")
        print(f"      Test:  {(y_test==1).sum()/len(y_test)*100:.1f}%")

        return X_train, X_val, X_test, y_train, y_val, y_test

    def save_training_data(self, X_train, X_val, X_test, y_train, y_val, y_test):
        """
        LÆ°u dá»¯ liá»‡u training

        LÆ°u cÃ¡c file:
        - X_train.npy, X_val.npy, X_test.npy
        - y_train.npy, y_val.npy, y_test.npy
        - scaler.pkl
        - training_metadata.json
        - feature_names.txt
        """
        print("\n" + "="*80)
        print("ğŸ’¾ ÄANG LÆ¯U Dá»® LIá»†U TRAINING...")
        print("="*80)

        # LÆ°u numpy arrays
        np.save(self.output_dir / 'X_train.npy', X_train)
        np.save(self.output_dir / 'X_val.npy', X_val)
        np.save(self.output_dir / 'X_test.npy', X_test)
        np.save(self.output_dir / 'y_train.npy', y_train)
        np.save(self.output_dir / 'y_val.npy', y_val)
        np.save(self.output_dir / 'y_test.npy', y_test)

        print(f"   âœ… X_train.npy: {X_train.shape}")
        print(f"   âœ… X_val.npy:   {X_val.shape}")
        print(f"   âœ… X_test.npy:  {X_test.shape}")
        print(f"   âœ… y_train.npy: {y_train.shape}")
        print(f"   âœ… y_val.npy:   {y_val.shape}")
        print(f"   âœ… y_test.npy:  {y_test.shape}")

        # LÆ°u scaler
        with open(self.output_dir / 'scaler.pkl', 'wb') as f:
            pickle.dump(self.scaler, f)
        print(f"   âœ… scaler.pkl")

        # LÆ°u feature names
        with open(self.output_dir / 'feature_names.txt', 'w') as f:
            for name in self.feature_names:
                f.write(name + '\n')
        print(f"   âœ… feature_names.txt")

        # Chuáº©n bá»‹ metadata
        metadata = {
            'n_features': len(self.feature_names),
            'input_shape': [int(X_train.shape[1]), int(X_train.shape[2])],
            'train_samples': int(X_train.shape[0]),
            'val_samples': int(X_val.shape[0]),
            'test_samples': int(X_test.shape[0]),
            'total_samples': int(X_train.shape[0] + X_val.shape[0] + X_test.shape[0]),
            'class_distribution': {
                'train': {
                    'benign': int((y_train == 0).sum()),
                    'attack': int((y_train == 1).sum())
                },
                'val': {
                    'benign': int((y_val == 0).sum()),
                    'attack': int((y_val == 1).sum())
                },
                'test': {
                    'benign': int((y_test == 0).sum()),
                    'attack': int((y_test == 1).sum())
                }
            },
            'benign_ratio': float(self.benign_ratio),
            'attack_ratio': float(self.attack_ratio),
            'preprocessing': {
                'log_transform': 'log_e(1+x)',
                'normalization': 'StandardScaler'
            },
            'created_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

        with open(self.output_dir / 'training_metadata.json', 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=4, ensure_ascii=False)
        print(f"   âœ… training_metadata.json")

        print(f"\nğŸ“ Táº¥t cáº£ file Ä‘Æ°á»£c lÆ°u táº¡i: {self.output_dir}")

    def calculate_class_weights(self, y_train):
        """
        TÃ­nh class weights cho training

        Sá»­ dá»¥ng khi dá»¯ liá»‡u váº«n cÃ²n imbalanced
        """
        from sklearn.utils.class_weight import compute_class_weight

        classes = np.unique(y_train)
        weights = compute_class_weight('balanced', classes=classes, y=y_train)
        class_weights = dict(zip(classes, weights))

        print(f"\nâš–ï¸ CLASS WEIGHTS (cho training):")
        print(f"   Class 0 (Benign): {class_weights[0]:.4f}")
        print(f"   Class 1 (Attack): {class_weights[1]:.4f}")

        # LÆ°u class weights
        with open(self.output_dir / 'class_weights.pkl', 'wb') as f:
            pickle.dump(class_weights, f)
        print(f"   âœ… ÄÃ£ lÆ°u class_weights.pkl")

        return class_weights


def main():
    """HÃ m chÃ­nh"""

    print("\n" + "="*80)
    print("ğŸ“Š BÆ¯á»šC 2: CHUáº¨N Bá»Š Dá»® LIá»†U TRAINING CHO CNN")
    print("   CÃ¢n báº±ng vÃ  chia train/val/test")
    print("="*80)

    print(f"\nğŸ“‹ Cáº¤U HÃŒNH:")
    print(f"   - Tá»•ng máº«u mong muá»‘n: {TOTAL_SAMPLES:,}")
    print(f"   - Tá»· lá»‡ Benign: {BENIGN_RATIO*100:.0f}%")
    print(f"   - Tá»· lá»‡ Attack: {ATTACK_RATIO*100:.0f}%")
    print(f"   - Train/Val/Test: {(1-TEST_SIZE-VAL_SIZE)*100:.0f}%/{VAL_SIZE*100:.0f}%/{TEST_SIZE*100:.0f}%")

    # Khá»Ÿi táº¡o preparer
    preparer = TrainingDataPreparer(
        cleaned_data_dir=CLEANED_DATA_DIR,
        output_dir=OUTPUT_DIR,
        total_samples=TOTAL_SAMPLES,
        benign_ratio=BENIGN_RATIO,
        attack_ratio=ATTACK_RATIO,
        test_size=TEST_SIZE,
        val_size=VAL_SIZE
    )

    # BÆ°á»›c 1: Äá»c dá»¯ liá»‡u Ä‘Ã£ clean
    df = preparer.load_cleaned_data()

    # BÆ°á»›c 2: CÃ¢n báº±ng dá»¯ liá»‡u
    df = preparer.balanced_sample(df)

    # TÃ¡ch features vÃ  labels
    X = df.drop(columns=['binary_label']).values
    y = df['binary_label'].values

    # Giáº£i phÃ³ng bá»™ nhá»› DataFrame
    del df
    gc.collect()

    # BÆ°á»›c 3: Ãp dá»¥ng Log Transform
    X = preparer.apply_log_transform(X)

    # BÆ°á»›c 4: Chuáº©n hÃ³a
    X = preparer.normalize_features(X)

    # BÆ°á»›c 5: Reshape cho CNN
    X = preparer.reshape_for_cnn(X)

    # BÆ°á»›c 6: Chia train/val/test
    X_train, X_val, X_test, y_train, y_val, y_test = preparer.split_data(X, y)

    # Giáº£i phÃ³ng bá»™ nhá»›
    del X, y
    gc.collect()

    # BÆ°á»›c 7: TÃ­nh class weights
    class_weights = preparer.calculate_class_weights(y_train)

    # BÆ°á»›c 8: LÆ°u dá»¯ liá»‡u
    preparer.save_training_data(X_train, X_val, X_test, y_train, y_val, y_test)

    print("\n" + "="*80)
    print("âœ… HOÃ€N THÃ€NH BÆ¯á»šC 2!")
    print("   Dá»¯ liá»‡u Ä‘Ã£ sáºµn sÃ ng cho viá»‡c huáº¥n luyá»‡n CNN.")
    print("   Cháº¡y step3_train_cnn.py Ä‘á»ƒ train mÃ´ hÃ¬nh.")
    print("="*80)

    return preparer


if __name__ == "__main__":
    preparer = main()


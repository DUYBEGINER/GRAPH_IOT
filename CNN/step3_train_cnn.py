"""
======================================================================================
B∆Ø·ªöC 3: TRAIN M√î H√åNH CNN CHO PH√ÅT HI·ªÜN L∆ØU L∆Ø·ª¢NG M·∫†NG IOT B·∫§T TH∆Ø·ªúNG
======================================================================================

Ki·∫øn tr√∫c CNN theo y√™u c·∫ßu:
- Input Layer: Shape (num_features, 1)
- Conv1D (32 filters, kernel 2) -> MaxPooling1D (2)
- Conv1D (32 filters, kernel 2) -> MaxPooling1D (2)
- Conv1D (64 filters, kernel 2) -> MaxPooling1D (2)
- Conv1D (64 filters, kernel 2) -> MaxPooling1D (2)
- Conv1D (64 filters, kernel 2) -> MaxPooling1D (2)
- BatchNormalization + Dropout (0.5)
- Flatten
- Dense(1, activation='sigmoid')

Loss: binary_crossentropy
Optimizer: Adam
Metrics: Accuracy, Precision, Recall

C√≥ th·ªÉ ch·∫°y tr√™n c·∫£ Kaggle v√† Local
"""

import os
import numpy as np
import pickle
import json
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# TENSORFLOW/KERAS
# ============================================================================
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Conv1D, MaxPooling1D, Flatten, Dense,
    Dropout, BatchNormalization, Input
)
from tensorflow.keras.callbacks import (
    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard
)
from tensorflow.keras.metrics import Precision, Recall

# Ki·ªÉm tra GPU
print("="*80)
print("üñ•Ô∏è TH√îNG TIN H·ªÜ TH·ªêNG")
print("="*80)
print(f"TensorFlow version: {tf.__version__}")
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    print(f"‚úÖ GPU available: {len(gpus)} GPU(s)")
    for gpu in gpus:
        print(f"   - {gpu}")
    # C·∫•u h√¨nh GPU memory growth ƒë·ªÉ tr√°nh chi·∫øm h·∫øt b·ªô nh·ªõ
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
else:
    print("‚ö†Ô∏è Kh√¥ng c√≥ GPU, s·∫Ω s·ª≠ d·ª•ng CPU")

# Ki·ªÉm tra m√¥i tr∆∞·ªùng ch·∫°y
IS_KAGGLE = os.path.exists('/kaggle/input')

# ============================================================================
# C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N
# ============================================================================
if IS_KAGGLE:
    TRAINING_DATA_DIR = "/kaggle/working/training_data"
    MODEL_DIR = "/kaggle/working/models"
    LOG_DIR = "/kaggle/working/logs"
    print("üåê ƒêang ch·∫°y tr√™n KAGGLE")
else:
    TRAINING_DATA_DIR = r"D:\PROJECT\Machine Learning\IOT\CNN\training_data"
    MODEL_DIR = r"D:\PROJECT\Machine Learning\IOT\CNN\models"
    LOG_DIR = r"D:\PROJECT\Machine Learning\IOT\CNN\logs"
    print("üíª ƒêang ch·∫°y tr√™n LOCAL")

# ============================================================================
# C·∫§U H√åNH HU·∫§N LUY·ªÜN
# ============================================================================

# Hyperparameters
BATCH_SIZE = 256        # Batch size cho training
EPOCHS = 50             # S·ªë epochs t·ªëi ƒëa
LEARNING_RATE = 0.001   # Learning rate ban ƒë·∫ßu

# Regularization
DROPOUT_RATE = 0.5      # Dropout rate tr∆∞·ªõc Flatten

# Early stopping
PATIENCE = 10           # S·ªë epochs ch·ªù tr∆∞·ªõc khi d·ª´ng

# Random seed
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)


# ============================================================================
# H√ÄM X√ÇY D·ª∞NG M√î H√åNH CNN
# ============================================================================

def build_cnn_model(input_shape):
    """
    X√¢y d·ª±ng m√¥ h√¨nh CNN cho ph√¢n lo·∫°i binary

    Ki·∫øn tr√∫c theo y√™u c·∫ßu:
    - 5 l·ªõp Conv1D v·ªõi MaxPooling
    - BatchNormalization v√† Dropout tr∆∞·ªõc Flatten
    - Output layer v·ªõi sigmoid activation

    Args:
        input_shape: Shape c·ªßa input (n_features, 1)

    Returns:
        model: Keras Sequential model
    """
    print("\n" + "="*80)
    print("üèóÔ∏è ƒêANG X√ÇY D·ª∞NG M√î H√åNH CNN")
    print("="*80)
    print(f"   Input shape: {input_shape}")

    model = Sequential(name='CNN_Binary_Classification')

    # Input layer
    model.add(Input(shape=input_shape))

    # ========== KH·ªêI CONV 1 ==========
    # Conv1D (32 filters, kernel 2x1) -> MaxPooling1D (2)
    model.add(Conv1D(
        filters=32,
        kernel_size=2,
        activation='relu',
        padding='same',  # Gi·ªØ nguy√™n k√≠ch th∆∞·ªõc
        name='conv1d_1'
    ))
    model.add(MaxPooling1D(pool_size=2, name='maxpool_1'))

    # ========== KH·ªêI CONV 2 ==========
    # Conv1D (32 filters, kernel 2x1) -> MaxPooling1D (2)
    model.add(Conv1D(
        filters=32,
        kernel_size=2,
        activation='relu',
        padding='same',
        name='conv1d_2'
    ))
    model.add(MaxPooling1D(pool_size=2, name='maxpool_2'))

    # ========== KH·ªêI CONV 3 ==========
    # Conv1D (64 filters, kernel 2x1) -> MaxPooling1D (2)
    model.add(Conv1D(
        filters=64,
        kernel_size=2,
        activation='relu',
        padding='same',
        name='conv1d_3'
    ))
    model.add(MaxPooling1D(pool_size=2, name='maxpool_3'))

    # ========== KH·ªêI CONV 4 ==========
    # Conv1D (64 filters, kernel 2x1) -> MaxPooling1D (2)
    model.add(Conv1D(
        filters=64,
        kernel_size=2,
        activation='relu',
        padding='same',
        name='conv1d_4'
    ))
    model.add(MaxPooling1D(pool_size=2, name='maxpool_4'))

    # ========== KH·ªêI CONV 5 ==========
    # Conv1D (64 filters, kernel 2x1) -> MaxPooling1D (2)
    model.add(Conv1D(
        filters=64,
        kernel_size=2,
        activation='relu',
        padding='same',
        name='conv1d_5'
    ))
    model.add(MaxPooling1D(pool_size=2, name='maxpool_5'))

    # ========== REGULARIZATION ==========
    # BatchNormalization v√† Dropout tr∆∞·ªõc Flatten
    model.add(BatchNormalization(name='batch_norm'))
    model.add(Dropout(DROPOUT_RATE, name='dropout'))

    # ========== FLATTEN ==========
    model.add(Flatten(name='flatten'))

    # ========== OUTPUT LAYER ==========
    # Dense(1, activation='sigmoid') cho binary classification
    model.add(Dense(1, activation='sigmoid', name='output'))

    # ========== COMPILE ==========
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),
        loss='binary_crossentropy',
        metrics=[
            'accuracy',
            Precision(name='precision'),
            Recall(name='recall')
        ]
    )

    # In t√≥m t·∫Øt m√¥ h√¨nh
    print("\n   üìã KI·∫æN TR√öC M√î H√åNH:")
    model.summary()

    return model


def load_training_data(data_dir):
    """
    Load d·ªØ li·ªáu training ƒë√£ ƒë∆∞·ª£c chu·∫©n b·ªã t·ª´ step 2

    Args:
        data_dir: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu

    Returns:
        X_train, X_val, X_test, y_train, y_val, y_test, class_weights
    """
    print("\n" + "="*80)
    print("üìÇ ƒêANG LOAD D·ªÆ LI·ªÜU TRAINING...")
    print("="*80)

    data_dir = Path(data_dir)

    # Load numpy arrays
    X_train = np.load(data_dir / 'X_train.npy')
    X_val = np.load(data_dir / 'X_val.npy')
    X_test = np.load(data_dir / 'X_test.npy')
    y_train = np.load(data_dir / 'y_train.npy')
    y_val = np.load(data_dir / 'y_val.npy')
    y_test = np.load(data_dir / 'y_test.npy')

    print(f"   ‚úÖ X_train: {X_train.shape}")
    print(f"   ‚úÖ X_val:   {X_val.shape}")
    print(f"   ‚úÖ X_test:  {X_test.shape}")
    print(f"   ‚úÖ y_train: {y_train.shape}")
    print(f"   ‚úÖ y_val:   {y_val.shape}")
    print(f"   ‚úÖ y_test:  {y_test.shape}")

    # Load class weights n·∫øu c√≥
    class_weights = None
    class_weights_path = data_dir / 'class_weights.pkl'
    if class_weights_path.exists():
        with open(class_weights_path, 'rb') as f:
            class_weights = pickle.load(f)
        print(f"\n   ‚öñÔ∏è Class weights loaded:")
        print(f"      Class 0 (Benign): {class_weights[0]:.4f}")
        print(f"      Class 1 (Attack): {class_weights[1]:.4f}")

    # Th·ªëng k√™ ph√¢n b·ªë
    print(f"\n   üìä PH√ÇN B·ªê D·ªÆ LI·ªÜU:")
    for name, y in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:
        benign = (y == 0).sum()
        attack = (y == 1).sum()
        total = len(y)
        print(f"      {name}: Benign={benign:,} ({benign/total*100:.1f}%), Attack={attack:,} ({attack/total*100:.1f}%)")

    return X_train, X_val, X_test, y_train, y_val, y_test, class_weights


def create_callbacks(model_dir, log_dir):
    """
    T·∫°o c√°c callbacks cho training

    Callbacks:
    - EarlyStopping: D·ª´ng s·ªõm khi val_loss kh√¥ng gi·∫£m
    - ModelCheckpoint: L∆∞u model t·ªët nh·∫•t
    - ReduceLROnPlateau: Gi·∫£m learning rate khi plateau
    - TensorBoard: Logging cho visualization
    """
    print("\nüìå ƒêANG C·∫§U H√åNH CALLBACKS...")

    model_dir = Path(model_dir)
    log_dir = Path(log_dir)
    model_dir.mkdir(parents=True, exist_ok=True)
    log_dir.mkdir(parents=True, exist_ok=True)

    callbacks = []

    # 1. Early Stopping
    # D·ª´ng training khi val_loss kh√¥ng c·∫£i thi·ªán sau PATIENCE epochs
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=PATIENCE,
        verbose=1,
        mode='min',
        restore_best_weights=True  # Kh√¥i ph·ª•c weights t·ªët nh·∫•t
    )
    callbacks.append(early_stopping)
    print(f"   ‚úÖ EarlyStopping: patience={PATIENCE}")

    # 2. Model Checkpoint
    # L∆∞u model c√≥ val_loss th·∫•p nh·∫•t
    checkpoint_path = model_dir / 'best_model.keras'
    model_checkpoint = ModelCheckpoint(
        filepath=str(checkpoint_path),
        monitor='val_loss',
        verbose=1,
        save_best_only=True,
        mode='min'
    )
    callbacks.append(model_checkpoint)
    print(f"   ‚úÖ ModelCheckpoint: {checkpoint_path}")

    # 3. Reduce Learning Rate on Plateau
    # Gi·∫£m LR khi val_loss kh√¥ng gi·∫£m
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,        # Gi·∫£m LR c√≤n 1/2
        patience=5,        # Ch·ªù 5 epochs
        min_lr=1e-7,       # LR t·ªëi thi·ªÉu
        verbose=1
    )
    callbacks.append(reduce_lr)
    print(f"   ‚úÖ ReduceLROnPlateau: factor=0.5, patience=5")

    # 4. TensorBoard (optional)
    tensorboard_log = log_dir / datetime.now().strftime("%Y%m%d-%H%M%S")
    tensorboard = TensorBoard(
        log_dir=str(tensorboard_log),
        histogram_freq=1
    )
    callbacks.append(tensorboard)
    print(f"   ‚úÖ TensorBoard: {tensorboard_log}")

    return callbacks


def train_model(model, X_train, y_train, X_val, y_val, class_weights, callbacks):
    """
    Hu·∫•n luy·ªán m√¥ h√¨nh

    Args:
        model: Keras model
        X_train, y_train: D·ªØ li·ªáu training
        X_val, y_val: D·ªØ li·ªáu validation
        class_weights: Dictionary class weights
        callbacks: List c√°c callbacks

    Returns:
        history: Training history
    """
    print("\n" + "="*80)
    print("üöÄ B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN M√î H√åNH")
    print("="*80)
    print(f"   Epochs: {EPOCHS}")
    print(f"   Batch size: {BATCH_SIZE}")
    print(f"   Learning rate: {LEARNING_RATE}")
    print(f"   Class weights: {'C√≥' if class_weights else 'Kh√¥ng'}")

    start_time = datetime.now()

    history = model.fit(
        X_train, y_train,
        batch_size=BATCH_SIZE,
        epochs=EPOCHS,
        validation_data=(X_val, y_val),
        class_weight=class_weights,  # S·ª≠ d·ª•ng class weights ƒë·ªÉ x·ª≠ l√Ω imbalance
        callbacks=callbacks,
        verbose=1
    )

    end_time = datetime.now()
    training_time = (end_time - start_time).total_seconds()

    print(f"\n   ‚è±Ô∏è Th·ªùi gian training: {training_time/60:.2f} ph√∫t")
    print(f"   üìà Best val_loss: {min(history.history['val_loss']):.4f}")
    print(f"   üìà Best val_accuracy: {max(history.history['val_accuracy']):.4f}")

    return history, training_time


def evaluate_model(model, X_test, y_test):
    """
    ƒê√°nh gi√° m√¥ h√¨nh tr√™n test set

    Args:
        model: Trained model
        X_test, y_test: D·ªØ li·ªáu test

    Returns:
        results: Dictionary k·∫øt qu·∫£ ƒë√°nh gi√°
    """
    print("\n" + "="*80)
    print("üìä ƒê√ÅNH GI√Å M√î H√åNH TR√äN TEST SET")
    print("="*80)

    # Evaluate
    loss, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=1)

    # T√≠nh F1-score
    f1_score = 2 * (precision * recall) / (precision + recall + 1e-7)

    results = {
        'test_loss': float(loss),
        'test_accuracy': float(accuracy),
        'test_precision': float(precision),
        'test_recall': float(recall),
        'test_f1_score': float(f1_score)
    }

    print(f"\n   üìä K·∫æT QU·∫¢:")
    print(f"   {'='*40}")
    print(f"   Loss:      {loss:.4f}")
    print(f"   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"   Precision: {precision:.4f}")
    print(f"   Recall:    {recall:.4f}")
    print(f"   F1-Score:  {f1_score:.4f}")
    print(f"   {'='*40}")

    # Predictions cho confusion matrix
    y_pred_prob = model.predict(X_test, verbose=0)
    y_pred = (y_pred_prob > 0.5).astype(int).flatten()

    # Confusion matrix
    from sklearn.metrics import confusion_matrix, classification_report

    cm = confusion_matrix(y_test, y_pred)
    print(f"\n   üìã CONFUSION MATRIX:")
    print(f"                 Predicted")
    print(f"                 Benign  Attack")
    print(f"   Actual Benign  {cm[0,0]:>6}  {cm[0,1]:>6}")
    print(f"   Actual Attack  {cm[1,0]:>6}  {cm[1,1]:>6}")

    print(f"\n   üìã CLASSIFICATION REPORT:")
    report = classification_report(y_test, y_pred, target_names=['Benign', 'Attack'])
    print(report)

    # L∆∞u classification report d·∫°ng dictionary
    report_dict = classification_report(y_test, y_pred, target_names=['Benign', 'Attack'], output_dict=True)

    # Th√™m confusion matrix v√† c√°c metrics kh√°c v√†o results
    results['confusion_matrix'] = cm.tolist()
    results['classification_report'] = report_dict

    # Th√™m c√°c metrics chi ti·∫øt cho t·ª´ng class
    results['benign_precision'] = float(report_dict['Benign']['precision'])
    results['benign_recall'] = float(report_dict['Benign']['recall'])
    results['benign_f1'] = float(report_dict['Benign']['f1-score'])
    results['attack_precision'] = float(report_dict['Attack']['precision'])
    results['attack_recall'] = float(report_dict['Attack']['recall'])
    results['attack_f1'] = float(report_dict['Attack']['f1-score'])

    # T√≠nh th√™m m·ªôt s·ªë metrics b·ªï sung
    tn, fp, fn, tp = cm.ravel()
    results['true_negative'] = int(tn)
    results['false_positive'] = int(fp)
    results['false_negative'] = int(fn)
    results['true_positive'] = int(tp)
    results['specificity'] = float(tn / (tn + fp + 1e-7))  # True Negative Rate
    results['false_positive_rate'] = float(fp / (fp + tn + 1e-7))
    results['false_negative_rate'] = float(fn / (fn + tp + 1e-7))

    return results, y_pred, y_pred_prob


def save_model_and_results(model, history, results, training_time, model_dir, y_pred=None, y_pred_prob=None):
    """
    L∆∞u model v√† k·∫øt qu·∫£ training

    Args:
        model: Trained model
        history: Training history
        results: Evaluation results
        training_time: Th·ªùi gian training (seconds)
        model_dir: ƒê∆∞·ªùng d·∫´n l∆∞u
        y_pred: Predictions (optional)
        y_pred_prob: Prediction probabilities (optional)
    """
    print("\n" + "="*80)
    print("üíæ ƒêANG L∆ØU MODEL V√Ä K·∫æT QU·∫¢...")
    print("="*80)

    model_dir = Path(model_dir)
    model_dir.mkdir(parents=True, exist_ok=True)

    # L∆∞u model cu·ªëi c√πng
    final_model_path = model_dir / 'final_model.keras'
    model.save(final_model_path)
    print(f"   ‚úÖ Final model: {final_model_path}")

    # L∆∞u model weights
    weights_path = model_dir / 'model_weights.weights.h5'
    model.save_weights(weights_path)
    print(f"   ‚úÖ Model weights: {weights_path}")

    # L∆∞u training history
    history_dict = {key: [float(v) for v in values] for key, values in history.history.items()}
    with open(model_dir / 'training_history.json', 'w') as f:
        json.dump(history_dict, f, indent=4)
    print(f"   ‚úÖ Training history: training_history.json")

    # L∆∞u k·∫øt qu·∫£ ƒë√°nh gi√° v·ªõi th√¥ng tin b·ªï sung
    results['training_time_seconds'] = float(training_time)
    results['training_time_minutes'] = float(training_time / 60)
    results['epochs_trained'] = int(len(history.history['loss']))

    # Th√™m best validation metrics
    results['best_val_loss'] = float(min(history.history['val_loss']))
    results['best_val_accuracy'] = float(max(history.history['val_accuracy']))
    results['best_val_precision'] = float(max(history.history['val_precision']))
    results['best_val_recall'] = float(max(history.history['val_recall']))

    # T√≠nh best val F1-score
    val_precisions = history.history['val_precision']
    val_recalls = history.history['val_recall']
    val_f1_scores = [2 * (p * r) / (p + r + 1e-7) for p, r in zip(val_precisions, val_recalls)]
    results['best_val_f1_score'] = float(max(val_f1_scores))

    # Epoch n√†o ƒë·∫°t best val_loss
    results['best_val_loss_epoch'] = int(np.argmin(history.history['val_loss']) + 1)
    results['best_val_accuracy_epoch'] = int(np.argmax(history.history['val_accuracy']) + 1)

    with open(model_dir / 'evaluation_results.json', 'w') as f:
        json.dump(results, f, indent=4)
    print(f"   ‚úÖ Evaluation results: evaluation_results.json")

    # L∆∞u predictions n·∫øu c√≥
    if y_pred is not None:
        np.save(model_dir / 'y_pred.npy', y_pred)
        print(f"   ‚úÖ Predictions: y_pred.npy")

    if y_pred_prob is not None:
        np.save(model_dir / 'y_pred_prob.npy', y_pred_prob)
        print(f"   ‚úÖ Prediction probabilities: y_pred_prob.npy")

    # L∆∞u c·∫•u h√¨nh training
    config = {
        'batch_size': BATCH_SIZE,
        'epochs': EPOCHS,
        'learning_rate': LEARNING_RATE,
        'dropout_rate': DROPOUT_RATE,
        'patience': PATIENCE,
        'random_seed': RANDOM_SEED,
        'tensorflow_version': tf.__version__,
        'created_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    with open(model_dir / 'training_config.json', 'w') as f:
        json.dump(config, f, indent=4)
    print(f"   ‚úÖ Training config: training_config.json")

    print(f"\nüìÅ T·∫•t c·∫£ file ƒë∆∞·ª£c l∆∞u t·∫°i: {model_dir}")


def plot_training_history(history, model_dir):
    """
    V·∫Ω bi·ªÉu ƒë·ªì training history

    Args:
        history: Training history
        model_dir: ƒê∆∞·ªùng d·∫´n l∆∞u h√¨nh
    """
    try:
        import matplotlib.pyplot as plt

        model_dir = Path(model_dir)
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))

        # 1. Loss
        axes[0, 0].plot(history.history['loss'], label='Train Loss')
        axes[0, 0].plot(history.history['val_loss'], label='Val Loss')
        axes[0, 0].set_title('Model Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)

        # 2. Accuracy
        axes[0, 1].plot(history.history['accuracy'], label='Train Accuracy')
        axes[0, 1].plot(history.history['val_accuracy'], label='Val Accuracy')
        axes[0, 1].set_title('Model Accuracy')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Accuracy')
        axes[0, 1].legend()
        axes[0, 1].grid(True)

        # 3. Precision
        axes[1, 0].plot(history.history['precision'], label='Train Precision')
        axes[1, 0].plot(history.history['val_precision'], label='Val Precision')
        axes[1, 0].set_title('Model Precision')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Precision')
        axes[1, 0].legend()
        axes[1, 0].grid(True)

        # 4. Recall
        axes[1, 1].plot(history.history['recall'], label='Train Recall')
        axes[1, 1].plot(history.history['val_recall'], label='Val Recall')
        axes[1, 1].set_title('Model Recall')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Recall')
        axes[1, 1].legend()
        axes[1, 1].grid(True)

        plt.tight_layout()
        plt.savefig(model_dir / 'training_history.png', dpi=150)
        plt.close()
        print(f"   ‚úÖ Training history plot: training_history.png")

    except ImportError:
        print("   ‚ö†Ô∏è matplotlib kh√¥ng c√≥ s·∫µn, b·ªè qua vi·ªác v·∫Ω bi·ªÉu ƒë·ªì")


def plot_confusion_matrix(cm, model_dir):
    """
    V·∫Ω confusion matrix d∆∞·ªõi d·∫°ng heatmap

    Args:
        cm: Confusion matrix (numpy array ho·∫∑c list)
        model_dir: ƒê∆∞·ªùng d·∫´n l∆∞u h√¨nh
    """
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns

        model_dir = Path(model_dir)

        # Convert to numpy array if needed
        if isinstance(cm, list):
            cm = np.array(cm)

        # V·∫Ω heatmap
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=['Benign', 'Attack'],
                   yticklabels=['Benign', 'Attack'],
                   cbar_kws={'label': 'Count'})
        plt.title('Confusion Matrix', fontsize=16, fontweight='bold')
        plt.ylabel('Actual', fontsize=12)
        plt.xlabel('Predicted', fontsize=12)
        plt.tight_layout()
        plt.savefig(model_dir / 'confusion_matrix.png', dpi=150, bbox_inches='tight')
        plt.close()
        print(f"   ‚úÖ Confusion matrix plot: confusion_matrix.png")

        # V·∫Ω normalized confusion matrix
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',
                   xticklabels=['Benign', 'Attack'],
                   yticklabels=['Benign', 'Attack'],
                   cbar_kws={'label': 'Percentage'})
        plt.title('Normalized Confusion Matrix', fontsize=16, fontweight='bold')
        plt.ylabel('Actual', fontsize=12)
        plt.xlabel('Predicted', fontsize=12)
        plt.tight_layout()
        plt.savefig(model_dir / 'confusion_matrix_normalized.png', dpi=150, bbox_inches='tight')
        plt.close()
        print(f"   ‚úÖ Normalized confusion matrix plot: confusion_matrix_normalized.png")

    except ImportError as e:
        print(f"   ‚ö†Ô∏è matplotlib/seaborn kh√¥ng c√≥ s·∫µn, b·ªè qua vi·ªác v·∫Ω confusion matrix: {e}")


def main():
    """H√†m ch√≠nh ƒë·ªÉ train model"""

    print("\n" + "="*80)
    print("üß† HU·∫§N LUY·ªÜN M√î H√åNH CNN - PH√ÅT HI·ªÜN L∆ØU L∆Ø·ª¢NG M·∫†NG B·∫§T TH∆Ø·ªúNG")
    print("   Binary Classification: Benign vs Attack")
    print("="*80)

    # B∆∞·ªõc 1: Load d·ªØ li·ªáu
    X_train, X_val, X_test, y_train, y_val, y_test, class_weights = load_training_data(TRAINING_DATA_DIR)

    # B∆∞·ªõc 2: X√¢y d·ª±ng m√¥ h√¨nh
    input_shape = (X_train.shape[1], X_train.shape[2])  # (n_features, 1)
    model = build_cnn_model(input_shape)

    # B∆∞·ªõc 3: T·∫°o callbacks
    callbacks = create_callbacks(MODEL_DIR, LOG_DIR)

    # B∆∞·ªõc 4: Hu·∫•n luy·ªán
    history, training_time = train_model(
        model, X_train, y_train, X_val, y_val, class_weights, callbacks
    )

    # B∆∞·ªõc 5: ƒê√°nh gi√°
    results, y_pred, y_pred_prob = evaluate_model(model, X_test, y_test)

    # B∆∞·ªõc 6: L∆∞u model v√† k·∫øt qu·∫£
    save_model_and_results(model, history, results, training_time, MODEL_DIR, y_pred, y_pred_prob)

    # B∆∞·ªõc 7: V·∫Ω bi·ªÉu ƒë·ªì
    plot_training_history(history, MODEL_DIR)

    # B∆∞·ªõc 8: V·∫Ω confusion matrix
    if 'confusion_matrix' in results:
        plot_confusion_matrix(results['confusion_matrix'], MODEL_DIR)

    print("\n" + "="*80)
    print("‚úÖ HO√ÄN TH√ÄNH HU·∫§N LUY·ªÜN!")
    print(f"   Test Accuracy:  {results['test_accuracy']*100:.2f}%")
    print(f"   Test Precision: {results['test_precision']*100:.2f}%")
    print(f"   Test Recall:    {results['test_recall']*100:.2f}%")
    print(f"   Test F1-Score:  {results['test_f1_score']*100:.2f}%")
    print("="*80)

    return model, history, results


if __name__ == "__main__":
    model, history, results = main()


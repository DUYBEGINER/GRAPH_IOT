{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 14307565,
     "sourceType": "datasetVersion",
     "datasetId": 9133596
    },
    {
     "sourceId": 672505,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 509533,
     "modelId": 524200
    }
   ],
   "dockerImageVersionId": 31193,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "\"\"\"\n======================================================================================\nB∆Ø·ªöC 1: CLEAN V√Ä TI·ªÄN X·ª¨ L√ù DATASET CICIDS2018 CHO M√î H√åNH CNN\n======================================================================================\n\nScript n√†y th·ª±c hi·ªán c√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu:\n1. ƒê·ªçc t·ª´ng file CSV theo chunks ƒë·ªÉ t·ªëi ∆∞u b·ªô nh·ªõ\n2. Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn thi·∫øt (IP, Port, Timestamp, Flow ID)\n3. Lo·∫°i b·ªè c√°c c·ªôt c√≥ variance = 0 (zero-variance columns)\n4. X·ª≠ l√Ω Infinity v√† NaN b·∫±ng Mode c·ªßa c·ªôt\n5. Lo·∫°i b·ªè c√°c h√†ng tr√πng l·∫∑p\n6. Chuy·ªÉn ƒë·ªïi nh√£n sang d·∫°ng binary (Benign=0, Attack=1)\n7. L∆∞u d·ªØ li·ªáu ƒë√£ clean v√†o folder ƒë·ªÉ s·ª≠ d·ª•ng sau\n\nC√≥ th·ªÉ ch·∫°y tr√™n c·∫£ Kaggle v√† Local\n\"\"\"\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport json\nimport gc\nfrom pathlib import Path\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Ki·ªÉm tra m√¥i tr∆∞·ªùng ch·∫°y (Kaggle ho·∫∑c Local)\nIS_KAGGLE = os.path.exists('/kaggle/input')\n\n# Progress bar\ntry:\n    from tqdm import tqdm\n    TQDM_AVAILABLE = True\nexcept ImportError:\n    TQDM_AVAILABLE = False\n    print(\"‚ö†Ô∏è  tqdm kh√¥ng c√≥ s·∫µn. C√†i ƒë·∫∑t b·∫±ng: pip install tqdm\")\n    tqdm = lambda x, **kwargs: x\n\n# ============================================================================\n# C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N\n# ============================================================================\nif IS_KAGGLE:\n    # ƒê∆∞·ªùng d·∫´n tr√™n Kaggle - thay ƒë·ªïi theo dataset c·ªßa b·∫°n\n    DATA_DIR = \"/kaggle/input/cicids2018\"  # Thay ƒë·ªïi n·∫øu t√™n dataset kh√°c\n    OUTPUT_DIR = \"/kaggle/working/cleaned_data\"\n    print(\"üåê ƒêang ch·∫°y tr√™n KAGGLE\")\nelse:\n    # ƒê∆∞·ªùng d·∫´n Local\n    DATA_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CICIDS2018-CSV\"\n    OUTPUT_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CNN\\cleaned_data\"\n    print(\"üíª ƒêang ch·∫°y tr√™n LOCAL\")\n\n# ============================================================================\n# C·∫§U H√åNH X·ª¨ L√ù D·ªÆ LI·ªÜU\n# ============================================================================\n\n# K√≠ch th∆∞·ªõc chunk khi ƒë·ªçc CSV (ƒëi·ªÅu ch·ªânh theo RAM c·ªßa m√°y)\nCHUNK_SIZE = 300000  # 300k rows m·ªói chunk\n\n# Random state ƒë·ªÉ t√°i t·∫°o k·∫øt qu·∫£\nRANDOM_STATE = 42\n\n# ============================================================================\n# DANH S√ÅCH C√ÅC C·ªòT C·∫¶N LO·∫†I B·ªé (Identification columns)\n# ============================================================================\n\nCOLUMNS_TO_DROP = [\n    'Flow ID',          # ID duy nh·∫•t cho m·ªói flow - kh√¥ng c√≥ √Ω nghƒ©a ph√¢n lo·∫°i\n    'Src IP',           # IP ngu·ªìn - kh√¥ng t·ªïng qu√°t\n    'Dst IP',           # IP ƒë√≠ch - kh√¥ng t·ªïng qu√°t\n    'Src Port',         # Port ngu·ªìn - c√≥ th·ªÉ b·ªã overfitting\n    'Dst Port',         # Port ƒë√≠ch - c√≥ th·ªÉ b·ªã overfitting\n    'Timestamp',        # Th·ªùi gian - kh√¥ng li√™n quan ƒë·∫øn pattern t·∫•n c√¥ng\n]\n\n# C·ªôt nh√£n\nLABEL_COLUMN = 'Label'\n\n# ============================================================================\n# CLASS X·ª¨ L√ù D·ªÆ LI·ªÜU\n# ============================================================================\n\nclass CICIDS2018_DataCleaner:\n    \"\"\"\n    Class clean d·ªØ li·ªáu CICIDS2018 cho m√¥ h√¨nh CNN\n\n    C√°c b∆∞·ªõc x·ª≠ l√Ω:\n    1. ƒê·ªçc d·ªØ li·ªáu theo chunks\n    2. Lo·∫°i b·ªè c·ªôt identification\n    3. Lo·∫°i b·ªè zero-variance columns\n    4. X·ª≠ l√Ω Infinity v√† NaN b·∫±ng Mode\n    5. Lo·∫°i b·ªè duplicate\n    6. Chuy·ªÉn ƒë·ªïi nh√£n sang binary\n    7. L∆∞u d·ªØ li·ªáu ƒë√£ clean\n    \"\"\"\n\n    def __init__(self, data_dir, output_dir, chunk_size=CHUNK_SIZE):\n        \"\"\"\n        Kh·ªüi t·∫°o data cleaner\n\n        Args:\n            data_dir: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a file CSV\n            output_dir: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£\n            chunk_size: S·ªë d√≤ng m·ªói chunk khi ƒë·ªçc CSV\n        \"\"\"\n        self.data_dir = Path(data_dir)\n        self.output_dir = Path(output_dir)\n        self.chunk_size = chunk_size\n\n        # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n        # L∆∞u t√™n c√°c features v√† th√¥ng tin x·ª≠ l√Ω\n        self.feature_names = None\n        self.zero_variance_cols = []\n        self.column_modes = {}  # L∆∞u mode c·ªßa t·ª´ng c·ªôt ƒë·ªÉ x·ª≠ l√Ω NaN/Inf\n\n        # Th·ªëng k√™\n        self.stats = {\n            'total_rows_read': 0,\n            'rows_after_cleaning': 0,\n            'duplicates_removed': 0,\n            'nan_replaced': 0,\n            'inf_replaced': 0,\n            'zero_variance_cols_removed': 0,\n            'benign_count': 0,\n            'attack_count': 0,\n            'feature_count': 0,\n            'processing_time': 0.0  # Float ƒë·ªÉ l∆∞u th·ªùi gian x·ª≠ l√Ω (gi√¢y)\n        }\n\n    def _get_csv_files(self):\n        \"\"\"L·∫•y danh s√°ch c√°c file CSV trong th∆∞ m·ª•c data\"\"\"\n        csv_files = list(self.data_dir.glob(\"*_TrafficForML_CICFlowMeter.csv\"))\n        if not csv_files:\n            # Th·ª≠ pattern kh√°c cho Kaggle\n            csv_files = list(self.data_dir.glob(\"*.csv\"))\n            # Lo·∫°i b·ªè file zip n·∫øu c√≥\n            csv_files = [f for f in csv_files if not f.name.endswith('.zip')]\n\n        if not csv_files:\n            raise FileNotFoundError(f\"Kh√¥ng t√¨m th·∫•y file CSV trong {self.data_dir}\")\n\n        print(f\"\\nüìÇ T√¨m th·∫•y {len(csv_files)} file CSV:\")\n        for f in sorted(csv_files):\n            print(f\"   - {f.name}\")\n        return sorted(csv_files)\n\n    def _clean_column_names(self, df):\n        \"\"\"Chu·∫©n h√≥a t√™n c·ªôt (lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a)\"\"\"\n        df.columns = df.columns.str.strip()\n        return df\n\n    def _drop_identification_columns(self, df):\n        \"\"\"Lo·∫°i b·ªè c√°c c·ªôt identification kh√¥ng c·∫ßn thi·∫øt cho hu·∫•n luy·ªán\"\"\"\n        columns_to_drop = [col for col in COLUMNS_TO_DROP if col in df.columns]\n        if columns_to_drop:\n            df = df.drop(columns=columns_to_drop)\n        return df\n\n    def _convert_to_numeric(self, df):\n        \"\"\"Chuy·ªÉn ƒë·ªïi c√°c c·ªôt v·ªÅ d·∫°ng s·ªë\"\"\"\n        feature_cols = [col for col in df.columns if col != LABEL_COLUMN]\n        for col in feature_cols:\n            if df[col].dtype == 'object':\n                df[col] = pd.to_numeric(df[col], errors='coerce')\n        return df\n\n    def _convert_to_binary_label(self, df):\n        \"\"\"\n        Chuy·ªÉn ƒë·ªïi nh√£n sang d·∫°ng binary:\n        - Benign -> 0 (l∆∞u l∆∞·ª£ng b√¨nh th∆∞·ªùng)\n        - T·∫•t c·∫£ c√°c lo·∫°i t·∫•n c√¥ng kh√°c -> 1 (l∆∞u l∆∞·ª£ng b·∫•t th∆∞·ªùng)\n        \"\"\"\n        if LABEL_COLUMN not in df.columns:\n            raise ValueError(f\"Kh√¥ng t√¨m th·∫•y c·ªôt '{LABEL_COLUMN}' trong d·ªØ li·ªáu\")\n\n        # Chu·∫©n h√≥a nh√£n (lo·∫°i b·ªè kho·∫£ng tr·∫Øng, lowercase)\n        df[LABEL_COLUMN] = df[LABEL_COLUMN].astype(str).str.strip().str.lower()\n\n        # Lo·∫°i b·ªè c√°c h√†ng c√≥ nh√£n l√† 'label' (header b·ªã l·∫´n v√†o data)\n        df = df[df[LABEL_COLUMN] != 'label']\n\n        # Chuy·ªÉn ƒë·ªïi sang binary: Benign=0, Attack=1\n        df['binary_label'] = (df[LABEL_COLUMN] != 'benign').astype(int)\n\n        # X√≥a c·ªôt Label g·ªëc, gi·ªØ l·∫°i binary_label\n        df = df.drop(columns=[LABEL_COLUMN])\n\n        return df\n\n    def _first_pass_collect_info(self, csv_files):\n        \"\"\"\n        L·∫ßn ƒë·ªçc ƒë·∫ßu ti√™n: Thu th·∫≠p th√¥ng tin v·ªÅ columns v√† t√≠nh mode\n\n        M·ª•c ƒë√≠ch:\n        - X√°c ƒë·ªãnh c√°c c·ªôt c√≥ variance = 0\n        - T√≠nh mode c·ªßa t·ª´ng c·ªôt ƒë·ªÉ thay th·∫ø NaN/Inf\n        \"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\"üìä B∆Ø·ªöC 1: THU TH·∫¨P TH√îNG TIN T·ª™ D·ªÆ LI·ªÜU\")\n        print(\"=\"*80)\n\n        all_columns = None\n        column_value_counts = {}  # ƒê·ªÉ t√≠nh mode\n        column_min_max = {}  # ƒê·ªÉ ki·ªÉm tra variance\n\n        for csv_file in csv_files:\n            print(f\"\\n   ƒêang scan: {csv_file.name}\")\n            chunk_iterator = pd.read_csv(csv_file, chunksize=self.chunk_size,\n                                        low_memory=False, encoding='utf-8')\n\n            for chunk in chunk_iterator:\n                chunk = self._clean_column_names(chunk)\n                chunk = self._drop_identification_columns(chunk)\n                chunk = self._convert_to_numeric(chunk)\n\n                if all_columns is None:\n                    all_columns = [col for col in chunk.columns if col != LABEL_COLUMN]\n                    for col in all_columns:\n                        column_value_counts[col] = {}\n                        column_min_max[col] = {'min': np.inf, 'max': -np.inf}\n\n                # Thu th·∫≠p th√¥ng tin cho m·ªói c·ªôt\n                for col in all_columns:\n                    if col in chunk.columns:\n                        # Thay th·∫ø inf tr∆∞·ªõc khi t√≠nh\n                        col_data = chunk[col].replace([np.inf, -np.inf], np.nan)\n                        valid_data = col_data.dropna()\n\n                        if len(valid_data) > 0:\n                            # C·∫≠p nh·∫≠t min/max\n                            col_min = valid_data.min()\n                            col_max = valid_data.max()\n                            column_min_max[col]['min'] = min(column_min_max[col]['min'], col_min)\n                            column_min_max[col]['max'] = max(column_min_max[col]['max'], col_max)\n\n                            # Thu th·∫≠p value counts cho mode (l·∫•y top 10 ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ)\n                            vc = valid_data.value_counts().head(10).to_dict()\n                            for val, count in vc.items():\n                                if val not in column_value_counts[col]:\n                                    column_value_counts[col][val] = 0\n                                column_value_counts[col][val] += count\n\n                gc.collect()\n\n        # X√°c ƒë·ªãnh zero-variance columns\n        print(\"\\n   ƒêang ph√¢n t√≠ch variance c·ªßa c√°c c·ªôt...\")\n        for col in all_columns:\n            if column_min_max[col]['min'] == column_min_max[col]['max']:\n                self.zero_variance_cols.append(col)\n\n        # T√≠nh mode cho m·ªói c·ªôt\n        print(\"   ƒêang t√≠nh mode cho m·ªói c·ªôt...\")\n        for col in all_columns:\n            if col not in self.zero_variance_cols:\n                if column_value_counts[col]:\n                    # Mode l√† gi√° tr·ªã xu·∫•t hi·ªán nhi·ªÅu nh·∫•t\n                    mode_val = max(column_value_counts[col], key=column_value_counts[col].get)\n                    self.column_modes[col] = mode_val\n                else:\n                    self.column_modes[col] = 0  # Fallback n·∫øu kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá\n\n        self.stats['zero_variance_cols_removed'] = len(self.zero_variance_cols)\n\n        print(f\"\\n   ‚úÖ S·ªë c·ªôt zero-variance s·∫Ω lo·∫°i b·ªè: {len(self.zero_variance_cols)}\")\n        if self.zero_variance_cols:\n            print(f\"      C√°c c·ªôt: {self.zero_variance_cols}\")\n        print(f\"   ‚úÖ S·ªë c·ªôt s·∫Ω gi·ªØ l·∫°i: {len(all_columns) - len(self.zero_variance_cols)}\")\n\n        return all_columns\n\n    def _handle_nan_inf_with_mode(self, df):\n        \"\"\"\n        X·ª≠ l√Ω NaN v√† Infinity b·∫±ng Mode c·ªßa c·ªôt\n\n        Replace Infinity and NaN with the Mode of the column\n        \"\"\"\n        feature_cols = [col for col in df.columns if col != 'binary_label']\n\n        for col in feature_cols:\n            if col in self.column_modes:\n                mode_val = self.column_modes[col]\n\n                # ƒê·∫øm s·ªë l∆∞·ª£ng inf v√† nan\n                inf_mask = np.isinf(df[col])\n                nan_mask = df[col].isna()\n\n                self.stats['inf_replaced'] += inf_mask.sum()\n                self.stats['nan_replaced'] += nan_mask.sum()\n\n                # Thay th·∫ø inf b·∫±ng nan tr∆∞·ªõc\n                df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n\n                # Thay th·∫ø t·∫•t c·∫£ nan b·∫±ng mode\n                df[col] = df[col].fillna(mode_val)\n\n        return df\n\n    def _drop_zero_variance_columns(self, df):\n        \"\"\"Lo·∫°i b·ªè c√°c c·ªôt c√≥ variance = 0\"\"\"\n        cols_to_drop = [col for col in self.zero_variance_cols if col in df.columns]\n        if cols_to_drop:\n            df = df.drop(columns=cols_to_drop)\n        return df\n\n    def _process_single_file(self, csv_file):\n        \"\"\"\n        X·ª≠ l√Ω m·ªôt file CSV theo chunks\n\n        Args:\n            csv_file: ƒê∆∞·ªùng d·∫´n file CSV\n\n        Returns:\n            DataFrame ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω\n        \"\"\"\n        print(f\"\\nüìÑ ƒêang x·ª≠ l√Ω: {csv_file.name}\")\n\n        processed_chunks = []\n        chunk_iterator = pd.read_csv(csv_file, chunksize=self.chunk_size,\n                                     low_memory=False, encoding='utf-8')\n\n        # Progress bar cho chunks\n        if TQDM_AVAILABLE:\n            file_size = csv_file.stat().st_size\n            estimated_chunks = max(1, file_size // (self.chunk_size * 500))\n            chunk_iterator = tqdm(chunk_iterator, desc=\"   Chunks\",\n                                  total=estimated_chunks, unit=\"chunk\")\n\n        for chunk in chunk_iterator:\n            self.stats['total_rows_read'] += len(chunk)\n\n            # B∆∞·ªõc 1: Chu·∫©n h√≥a t√™n c·ªôt\n            chunk = self._clean_column_names(chunk)\n\n            # B∆∞·ªõc 2: Lo·∫°i b·ªè c·ªôt identification\n            chunk = self._drop_identification_columns(chunk)\n\n            # B∆∞·ªõc 3: Chuy·ªÉn ƒë·ªïi sang d·∫°ng s·ªë\n            chunk = self._convert_to_numeric(chunk)\n\n            # B∆∞·ªõc 4: Chuy·ªÉn ƒë·ªïi nh√£n sang binary\n            chunk = self._convert_to_binary_label(chunk)\n\n            # B∆∞·ªõc 5: Lo·∫°i b·ªè zero-variance columns\n            chunk = self._drop_zero_variance_columns(chunk)\n\n            # B∆∞·ªõc 6: X·ª≠ l√Ω NaN v√† Inf b·∫±ng Mode\n            chunk = self._handle_nan_inf_with_mode(chunk)\n\n            processed_chunks.append(chunk)\n            gc.collect()\n\n        # G·ªôp c√°c chunks l·∫°i\n        if processed_chunks:\n            df = pd.concat(processed_chunks, ignore_index=True)\n            del processed_chunks\n            gc.collect()\n            return df\n\n        return None\n\n    def clean_all_files(self):\n        \"\"\"\n        Clean t·∫•t c·∫£ c√°c file CSV\n\n        Returns:\n            DataFrame ƒë√£ clean ho√†n ch·ªânh\n        \"\"\"\n        start_time = datetime.now()\n        print(\"\\n\" + \"=\"*80)\n        print(\" B·∫ÆT ƒê·∫¶U CLEAN D·ªÆ LI·ªÜU CICIDS2018\")\n        print(\"=\"*80)\n\n        csv_files = self._get_csv_files()\n\n        # B∆∞·ªõc 1: Thu th·∫≠p th√¥ng tin (mode, zero-variance)\n        all_columns = self._first_pass_collect_info(csv_files)\n\n        # B∆∞·ªõc 2: X·ª≠ l√Ω t·ª´ng file\n        print(\"\\n\" + \"=\"*80)\n        print(\" B∆Ø·ªöC 2: CLEAN D·ªÆ LI·ªÜU\")\n        print(\"=\"*80)\n\n        all_dataframes = []\n        for csv_file in csv_files:\n            df = self._process_single_file(csv_file)\n            if df is not None:\n                all_dataframes.append(df)\n                print(f\"   ‚úÖ ƒê√£ x·ª≠ l√Ω: {len(df):,} m·∫´u\")\n\n        # G·ªôp t·∫•t c·∫£ l·∫°i\n        print(\"\\n\" + \"-\"*80)\n        print(\" ƒêANG G·ªòP D·ªÆ LI·ªÜU...\")\n\n        df_combined = pd.concat(all_dataframes, ignore_index=True)\n        del all_dataframes\n        gc.collect()\n\n        print(f\"   T·ªïng s·ªë m·∫´u sau khi g·ªôp: {len(df_combined):,}\")\n\n        # Lo·∫°i b·ªè duplicate tr√™n to√†n b·ªô dataset\n        print(\"   ƒêang lo·∫°i b·ªè duplicate...\")\n        rows_before = len(df_combined)\n        df_combined = df_combined.drop_duplicates()\n        rows_after = len(df_combined)\n        self.stats['duplicates_removed'] = rows_before - rows_after\n        print(f\"   S·ªë m·∫´u sau khi lo·∫°i duplicate: {len(df_combined):,}\")\n        print(f\"   S·ªë duplicate ƒë√£ lo·∫°i: {self.stats['duplicates_removed']:,}\")\n\n        # ƒê·∫øm s·ªë l∆∞·ª£ng m·ªói class\n        self.stats['benign_count'] = int((df_combined['binary_label'] == 0).sum())\n        self.stats['attack_count'] = int((df_combined['binary_label'] == 1).sum())\n\n        # C·∫≠p nh·∫≠t th·ªëng k√™\n        self.stats['rows_after_cleaning'] = len(df_combined)\n        self.stats['feature_count'] = len(df_combined.columns) - 1  # Tr·ª´ c·ªôt label\n\n        # L∆∞u t√™n features\n        self.feature_names = [col for col in df_combined.columns if col != 'binary_label']\n\n        end_time = datetime.now()\n        self.stats['processing_time'] = (end_time - start_time).total_seconds()\n\n        return df_combined\n\n    def save_cleaned_data(self, df):\n        \"\"\"\n        L∆∞u d·ªØ li·ªáu ƒë√£ clean\n\n        L∆∞u th√†nh c√°c file:\n        - cleaned_data.parquet (d·ªØ li·ªáu ƒë√£ clean, ch∆∞a normalize)\n        - feature_names.txt\n        - cleaning_metadata.json\n        \"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\" ƒêANG L∆ØU D·ªÆ LI·ªÜU ƒê√É CLEAN...\")\n        print(\"=\"*80)\n\n        # L∆∞u d·ªØ li·ªáu d·∫°ng parquet (nhanh v√† nh·ªè g·ªçn)\n        parquet_path = self.output_dir / 'cleaned_data.parquet'\n        df.to_parquet(parquet_path, index=False)\n        print(f\"   ‚úÖ ƒê√£ l∆∞u: {parquet_path}\")\n        print(f\"      K√≠ch th∆∞·ªõc file: {parquet_path.stat().st_size / (1024*1024):.2f} MB\")\n\n        # C≈©ng l∆∞u d·∫°ng CSV ƒë·ªÉ d·ªÖ ki·ªÉm tra (optional, c√≥ th·ªÉ comment n·∫øu file qu√° l·ªõn)\n        # csv_path = self.output_dir / 'cleaned_data.csv'\n        # df.to_csv(csv_path, index=False)\n        # print(f\"   ‚úÖ ƒê√£ l∆∞u: {csv_path}\")\n\n        # L∆∞u feature names\n        with open(self.output_dir / 'feature_names.txt', 'w') as f:\n            for name in self.feature_names:\n                f.write(name + '\\n')\n        print(f\"   ‚úÖ ƒê√£ l∆∞u: feature_names.txt\")\n\n        # L∆∞u column modes (ƒë·ªÉ c√≥ th·ªÉ s·ª≠ d·ª•ng cho d·ªØ li·ªáu m·ªõi)\n        with open(self.output_dir / 'column_modes.pkl', 'wb') as f:\n            pickle.dump(self.column_modes, f)\n        print(f\"   ‚úÖ ƒê√£ l∆∞u: column_modes.pkl\")\n\n        # L∆∞u zero-variance columns\n        with open(self.output_dir / 'zero_variance_cols.pkl', 'wb') as f:\n            pickle.dump(self.zero_variance_cols, f)\n        print(f\"   ‚úÖ ƒê√£ l∆∞u: zero_variance_cols.pkl\")\n\n        # Chuy·ªÉn ƒë·ªïi stats sang ki·ªÉu Python native\n        stats_native = {}\n        for key, value in self.stats.items():\n            if hasattr(value, 'item'):\n                stats_native[key] = value.item()\n            elif isinstance(value, (np.integer, np.floating)):\n                stats_native[key] = int(value) if isinstance(value, np.integer) else float(value)\n            else:\n                stats_native[key] = value\n\n        # L∆∞u metadata\n        metadata = {\n            'n_features': len(self.feature_names),\n            'feature_names': self.feature_names,\n            'total_samples': int(len(df)),\n            'benign_count': self.stats['benign_count'],\n            'attack_count': self.stats['attack_count'],\n            'benign_ratio': self.stats['benign_count'] / len(df),\n            'attack_ratio': self.stats['attack_count'] / len(df),\n            'zero_variance_cols': self.zero_variance_cols,\n            'stats': stats_native,\n            'created_at': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        }\n\n        with open(self.output_dir / 'cleaning_metadata.json', 'w', encoding='utf-8') as f:\n            json.dump(metadata, f, indent=4, ensure_ascii=False)\n        print(f\"   ‚úÖ ƒê√£ l∆∞u: cleaning_metadata.json\")\n\n        print(f\"\\nüìÅ T·∫•t c·∫£ file ƒë∆∞·ª£c l∆∞u t·∫°i: {self.output_dir}\")\n\n    def print_summary(self):\n        \"\"\"In t√≥m t·∫Øt qu√° tr√¨nh x·ª≠ l√Ω\"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\" T√ìM T·∫ÆT CLEAN D·ªÆ LI·ªÜU\")\n        print(\"=\"*80)\n        print(f\"   T·ªïng s·ªë d√≤ng ƒë·ªçc ƒë∆∞·ª£c:        {self.stats['total_rows_read']:,}\")\n        print(f\"   S·ªë d√≤ng sau khi clean:        {self.stats['rows_after_cleaning']:,}\")\n        print(f\"   S·ªë duplicate ƒë√£ lo·∫°i:         {self.stats['duplicates_removed']:,}\")\n        print(f\"   S·ªë NaN ƒë√£ thay b·∫±ng mode:     {self.stats['nan_replaced']:,}\")\n        print(f\"   S·ªë Inf ƒë√£ thay b·∫±ng mode:     {self.stats['inf_replaced']:,}\")\n        print(f\"   S·ªë c·ªôt zero-variance ƒë√£ lo·∫°i: {self.stats['zero_variance_cols_removed']}\")\n        print(f\"   S·ªë features c√≤n l·∫°i:          {self.stats['feature_count']}\")\n        print(f\"\\n   üìà PH√ÇN B·ªê NH√ÉN:\")\n        print(f\"   S·ªë m·∫´u Benign (0):  {self.stats['benign_count']:,} ({self.stats['benign_count']/self.stats['rows_after_cleaning']*100:.1f}%)\")\n        print(f\"   S·ªë m·∫´u Attack (1):  {self.stats['attack_count']:,} ({self.stats['attack_count']/self.stats['rows_after_cleaning']*100:.1f}%)\")\n        print(f\"\\n   Th·ªùi gian x·ª≠ l√Ω: {self.stats['processing_time']:.2f} gi√¢y\")\n        print(\"=\"*80)\n\n\ndef main():\n    \"\"\"H√†m ch√≠nh ƒë·ªÉ ch·∫°y cleaning\"\"\"\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"üßπ B∆Ø·ªöC 1: CLEAN D·ªÆ LI·ªÜU CICIDS2018 CHO CNN\")\n    print(\"   Ph√°t hi·ªán l∆∞u l∆∞·ª£ng m·∫°ng IoT b·∫•t th∆∞·ªùng\")\n    print(\"=\"*80)\n\n    # Kh·ªüi t·∫°o cleaner\n    cleaner = CICIDS2018_DataCleaner(\n        data_dir=DATA_DIR,\n        output_dir=OUTPUT_DIR,\n        chunk_size=CHUNK_SIZE\n    )\n\n    # Clean t·∫•t c·∫£ c√°c file\n    df = cleaner.clean_all_files()\n\n    # L∆∞u d·ªØ li·ªáu ƒë√£ clean\n    cleaner.save_cleaned_data(df)\n\n    # In t√≥m t·∫Øt\n    cleaner.print_summary()\n\n    print(\"\\n‚úÖ HO√ÄN TH√ÄNH B∆Ø·ªöC 1!\")\n    print(\"   D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c clean v√† l∆∞u v√†o folder.\")\n    print(\"   Ch·∫°y step2_prepare_training_data.py ƒë·ªÉ chia train/val/test v√† c√¢n b·∫±ng d·ªØ li·ªáu.\")\n\n    return cleaner\n\n\nif __name__ == \"__main__\":\n    cleaner = main()\n\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-28T11:09:37.691763Z",
     "iopub.execute_input": "2025-12-28T11:09:37.692006Z",
     "iopub.status.idle": "2025-12-28T11:21:34.418223Z",
     "shell.execute_reply.started": "2025-12-28T11:09:37.691986Z",
     "shell.execute_reply": "2025-12-28T11:21:34.417554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "üåê ƒêang ch·∫°y tr√™n KAGGLE\n\n================================================================================\nüßπ B∆Ø·ªöC 1: CLEAN D·ªÆ LI·ªÜU CICIDS2018 CHO CNN\n   Ph√°t hi·ªán l∆∞u l∆∞·ª£ng m·∫°ng IoT b·∫•t th∆∞·ªùng\n================================================================================\n\n================================================================================\n B·∫ÆT ƒê·∫¶U CLEAN D·ªÆ LI·ªÜU CICIDS2018\n================================================================================\n\nüìÇ T√¨m th·∫•y 10 file CSV:\n   - Friday-02-03-2018_TrafficForML_CICFlowMeter.csv\n   - Friday-16-02-2018_TrafficForML_CICFlowMeter.csv\n   - Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\n   - Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv\n   - Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv\n   - Thursday-15-02-2018_TrafficForML_CICFlowMeter.csv\n   - Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv\n   - Wednesday-14-02-2018_TrafficForML_CICFlowMeter.csv\n   - Wednesday-21-02-2018_TrafficForML_CICFlowMeter.csv\n   - Wednesday-28-02-2018_TrafficForML_CICFlowMeter.csv\n\n================================================================================\nüìä B∆Ø·ªöC 1: THU TH·∫¨P TH√îNG TIN T·ª™ D·ªÆ LI·ªÜU\n================================================================================\n\n   ƒêang scan: Friday-02-03-2018_TrafficForML_CICFlowMeter.csv\n\n   ƒêang scan: Friday-16-02-2018_TrafficForML_CICFlowMeter.csv\n\n   ƒêang scan: Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\n\n   ƒêang scan: Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv\n\n   ƒêang scan: Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv\n\n   ƒêang scan: Thursday-15-02-2018_TrafficForML_CICFlowMeter.csv\n\n   ƒêang scan: Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv\n\n   ƒêang scan: Wednesday-14-02-2018_TrafficForML_CICFlowMeter.csv\n\n   ƒêang scan: Wednesday-21-02-2018_TrafficForML_CICFlowMeter.csv\n\n   ƒêang scan: Wednesday-28-02-2018_TrafficForML_CICFlowMeter.csv\n\n   ƒêang ph√¢n t√≠ch variance c·ªßa c√°c c·ªôt...\n   ƒêang t√≠nh mode cho m·ªói c·ªôt...\n\n   ‚úÖ S·ªë c·ªôt zero-variance s·∫Ω lo·∫°i b·ªè: 8\n      C√°c c·ªôt: ['Bwd PSH Flags', 'Bwd URG Flags', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg']\n   ‚úÖ S·ªë c·ªôt s·∫Ω gi·ªØ l·∫°i: 69\n\n================================================================================\n B∆Ø·ªöC 2: CLEAN D·ªÆ LI·ªÜU\n================================================================================\n\nüìÑ ƒêang x·ª≠ l√Ω: Friday-02-03-2018_TrafficForML_CICFlowMeter.csv\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "   Chunks: 4chunk [00:13,  3.30s/chunk]                    \n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "   ‚úÖ ƒê√£ x·ª≠ l√Ω: 1,048,575 m·∫´u\n\nüìÑ ƒêang x·ª≠ l√Ω: Friday-16-02-2018_TrafficForML_CICFlowMeter.csv\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "   Chunks: 4chunk [00:17,  4.43s/chunk]                    \n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "   ‚úÖ ƒê√£ x·ª≠ l√Ω: 1,048,574 m·∫´u\n\nüìÑ ƒêang x·ª≠ l√Ω: Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "   Chunks: 4chunk [00:13,  3.32s/chunk]                    \n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "   ‚úÖ ƒê√£ x·ª≠ l√Ω: 1,048,575 m·∫´u\n\nüìÑ ƒêang x·ª≠ l√Ω: Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "   Chunks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [02:10<00:00,  4.84s/chunk]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "   ‚úÖ ƒê√£ x·ª≠ l√Ω: 7,948,748 m·∫´u\n\nüìÑ ƒêang x·ª≠ l√Ω: Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "   Chunks: 2chunk [00:15,  7.85s/chunk]                    \n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "   ‚úÖ ƒê√£ x·ª≠ l√Ω: 331,100 m·∫´u\n\nüìÑ ƒêang x·ª≠ l√Ω: Thursday-15-02-2018_TrafficForML_CICFlowMeter.csv\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "   Chunks: 4chunk [00:11,  2.97s/chunk]                    \n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "   ‚úÖ ƒê√£ x·ª≠ l√Ω: 1,048,575 m·∫´u\n\nüìÑ ƒêang x·ª≠ l√Ω: Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "   Chunks: 4chunk [00:11,  2.98s/chunk]                    \n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "   ‚úÖ ƒê√£ x·ª≠ l√Ω: 1,048,575 m·∫´u\n\nüìÑ ƒêang x·ª≠ l√Ω: Wednesday-14-02-2018_TrafficForML_CICFlowMeter.csv\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "   Chunks: 4chunk [00:11,  2.80s/chunk]                    \n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "   ‚úÖ ƒê√£ x·ª≠ l√Ω: 1,048,575 m·∫´u\n\nüìÑ ƒêang x·ª≠ l√Ω: Wednesday-21-02-2018_TrafficForML_CICFlowMeter.csv\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "   Chunks: 4chunk [00:10,  2.65s/chunk]                    \n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "   ‚úÖ ƒê√£ x·ª≠ l√Ω: 1,048,575 m·∫´u\n\nüìÑ ƒêang x·ª≠ l√Ω: Wednesday-28-02-2018_TrafficForML_CICFlowMeter.csv\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "   Chunks: 3chunk [00:29,  9.83s/chunk]                    \n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "   ‚úÖ ƒê√£ x·ª≠ l√Ω: 613,071 m·∫´u\n\n--------------------------------------------------------------------------------\n ƒêANG G·ªòP D·ªÆ LI·ªÜU...\n   T·ªïng s·ªë m·∫´u sau khi g·ªôp: 16,232,943\n   ƒêang lo·∫°i b·ªè duplicate...\n   S·ªë m·∫´u sau khi lo·∫°i duplicate: 10,822,059\n   S·ªë duplicate ƒë√£ lo·∫°i: 5,410,884\n\n================================================================================\n ƒêANG L∆ØU D·ªÆ LI·ªÜU ƒê√É CLEAN...\n================================================================================\n   ‚úÖ ƒê√£ l∆∞u: /kaggle/working/cleaned_data/cleaned_data.parquet\n      K√≠ch th∆∞·ªõc file: 1479.20 MB\n   ‚úÖ ƒê√£ l∆∞u: feature_names.txt\n   ‚úÖ ƒê√£ l∆∞u: column_modes.pkl\n   ‚úÖ ƒê√£ l∆∞u: zero_variance_cols.pkl\n   ‚úÖ ƒê√£ l∆∞u: cleaning_metadata.json\n\nüìÅ T·∫•t c·∫£ file ƒë∆∞·ª£c l∆∞u t·∫°i: /kaggle/working/cleaned_data\n\n================================================================================\n T√ìM T·∫ÆT CLEAN D·ªÆ LI·ªÜU\n================================================================================\n   T·ªïng s·ªë d√≤ng ƒë·ªçc ƒë∆∞·ª£c:        16,233,002\n   S·ªë d√≤ng sau khi clean:        10,822,059\n   S·ªë duplicate ƒë√£ lo·∫°i:         5,410,884\n   S·ªë NaN ƒë√£ thay b·∫±ng mode:     59,721\n   S·ªë Inf ƒë√£ thay b·∫±ng mode:     131,799\n   S·ªë c·ªôt zero-variance ƒë√£ lo·∫°i: 8\n   S·ªë features c√≤n l·∫°i:          69\n\n   üìà PH√ÇN B·ªê NH√ÉN:\n   S·ªë m·∫´u Benign (0):  9,496,113 (87.7%)\n   S·ªë m·∫´u Attack (1):  1,325,946 (12.3%)\n\n   Th·ªùi gian x·ª≠ l√Ω: 690.30 gi√¢y\n================================================================================\n\n‚úÖ HO√ÄN TH√ÄNH B∆Ø·ªöC 1!\n   D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c clean v√† l∆∞u v√†o folder.\n   Ch·∫°y step2_prepare_training_data.py ƒë·ªÉ chia train/val/test v√† c√¢n b·∫±ng d·ªØ li·ªáu.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "======================================================================================\n",
    "B∆Ø·ªöC 2: CHU·∫®N B·ªä D·ªÆ LI·ªÜU TRAINING CHO CNN - C√ÇN B·∫∞NG V√Ä CHIA TRAIN/VAL/TEST\n",
    "======================================================================================\n",
    "\n",
    "Script n√†y th·ª±c hi·ªán:\n",
    "1. ƒê·ªçc d·ªØ li·ªáu ƒë√£ clean t·ª´ step1\n",
    "2. C√¢n b·∫±ng s·ªë l∆∞·ª£ng nh√£n (70% Benign, 30% Attack ho·∫∑c t·ª∑ l·ªá t√πy ch·ªânh)\n",
    "3. √Åp d·ª•ng Log Transform: log_e(1+x)\n",
    "4. Chu·∫©n h√≥a b·∫±ng StandardScaler\n",
    "5. Reshape cho CNN 1D\n",
    "6. Chia train/val/test v·ªõi stratify ƒë·ªÉ gi·ªØ t·ª∑ l·ªá\n",
    "7. L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω ƒë·ªÉ train\n",
    "\n",
    "C√≥ th·ªÉ ch·∫°y tr√™n c·∫£ Kaggle v√† Local\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# TH∆Ø VI·ªÜN CHU·∫®N H√ìA V√Ä X·ª¨ L√ù D·ªÆ LI·ªÜU\n",
    "# ============================================================================\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ki·ªÉm tra m√¥i tr∆∞·ªùng ch·∫°y (Kaggle ho·∫∑c Local)\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
    "\n",
    "# ============================================================================\n",
    "# C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N\n",
    "# ============================================================================\n",
    "if IS_KAGGLE:\n",
    "    CLEANED_DATA_DIR = \"/kaggle/working/cleaned_data\"\n",
    "    OUTPUT_DIR = \"/kaggle/working/training_data\"\n",
    "    print(\"üåê ƒêang ch·∫°y tr√™n KAGGLE\")\n",
    "else:\n",
    "    CLEANED_DATA_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CNN\\cleaned_data\"\n",
    "    OUTPUT_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CNN\\training_data\"\n",
    "    print(\"üíª ƒêang ch·∫°y tr√™n LOCAL\")\n",
    "\n",
    "# ============================================================================\n",
    "# C·∫§U H√åNH C√ÇN B·∫∞NG D·ªÆ LI·ªÜU\n",
    "# ============================================================================\n",
    "\n",
    "# Random state ƒë·ªÉ t√°i t·∫°o k·∫øt qu·∫£\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# T·ªïng s·ªë m·∫´u mong mu·ªën (train + val + test)\n",
    "TOTAL_SAMPLES = 3000000  # 3 tri·ªáu m·∫´u\n",
    "\n",
    "# T·ª∑ l·ªá ph·∫ßn trƒÉm cho m·ªói class\n",
    "BENIGN_RATIO = 0.70  # 70% Benign\n",
    "ATTACK_RATIO = 0.30  # 30% Attack\n",
    "\n",
    "# T√≠nh s·ªë l∆∞·ª£ng m·∫´u cho m·ªói class\n",
    "TARGET_BENIGN = int(TOTAL_SAMPLES * BENIGN_RATIO)  # 2,100,000\n",
    "TARGET_ATTACK = int(TOTAL_SAMPLES * ATTACK_RATIO)  # 900,000\n",
    "\n",
    "# T·ª∑ l·ªá chia train/val/test\n",
    "TEST_SIZE = 0.20   # 20% cho test\n",
    "VAL_SIZE = 0.10    # 10% cho validation (t·ª´ t·ªïng)\n",
    "# Train s·∫Ω l√† 70%\n",
    "\n",
    "# ============================================================================\n",
    "# CLASS CHU·∫®N B·ªä D·ªÆ LI·ªÜU TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "class TrainingDataPreparer:\n",
    "    \"\"\"\n",
    "    Class chu·∫©n b·ªã d·ªØ li·ªáu training cho CNN\n",
    "\n",
    "    C√°c b∆∞·ªõc:\n",
    "    1. ƒê·ªçc d·ªØ li·ªáu ƒë√£ clean\n",
    "    2. C√¢n b·∫±ng d·ªØ li·ªáu theo t·ª∑ l·ªá mong mu·ªën\n",
    "    3. √Åp d·ª•ng log transform: log_e(1+x)\n",
    "    4. Chu·∫©n h√≥a b·∫±ng StandardScaler\n",
    "    5. Reshape cho CNN\n",
    "    6. Chia train/val/test\n",
    "    7. L∆∞u d·ªØ li·ªáu\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cleaned_data_dir, output_dir,\n",
    "                 total_samples=TOTAL_SAMPLES,\n",
    "                 benign_ratio=BENIGN_RATIO,\n",
    "                 attack_ratio=ATTACK_RATIO,\n",
    "                 test_size=TEST_SIZE,\n",
    "                 val_size=VAL_SIZE):\n",
    "        \"\"\"\n",
    "        Kh·ªüi t·∫°o preparer\n",
    "\n",
    "        Args:\n",
    "            cleaned_data_dir: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu ƒë√£ clean\n",
    "            output_dir: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£\n",
    "            total_samples: T·ªïng s·ªë m·∫´u mong mu·ªën\n",
    "            benign_ratio: T·ª∑ l·ªá Benign (0-1)\n",
    "            attack_ratio: T·ª∑ l·ªá Attack (0-1)\n",
    "            test_size: T·ª∑ l·ªá test set\n",
    "            val_size: T·ª∑ l·ªá validation set\n",
    "        \"\"\"\n",
    "        self.cleaned_data_dir = Path(cleaned_data_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.total_samples = total_samples\n",
    "        self.benign_ratio = benign_ratio\n",
    "        self.attack_ratio = attack_ratio\n",
    "        self.test_size = test_size\n",
    "        self.val_size = val_size\n",
    "\n",
    "        # T√≠nh target cho m·ªói class\n",
    "        self.target_benign = int(total_samples * benign_ratio)\n",
    "        self.target_attack = int(total_samples * attack_ratio)\n",
    "\n",
    "        # Kh·ªüi t·∫°o scaler\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "        # T·∫°o th∆∞ m·ª•c output\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # L∆∞u t√™n features\n",
    "        self.feature_names = None\n",
    "\n",
    "        # Th·ªëng k√™\n",
    "        self.stats = {\n",
    "            'original_benign': 0,\n",
    "            'original_attack': 0,\n",
    "            'sampled_benign': 0,\n",
    "            'sampled_attack': 0,\n",
    "            'train_samples': 0,\n",
    "            'val_samples': 0,\n",
    "            'test_samples': 0,\n",
    "            'n_features': 0\n",
    "        }\n",
    "\n",
    "    def load_cleaned_data(self):\n",
    "        \"\"\"ƒê·ªçc d·ªØ li·ªáu ƒë√£ clean t·ª´ step1\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìÇ ƒêANG ƒê·ªåC D·ªÆ LI·ªÜU ƒê√É CLEAN...\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        parquet_path = self.cleaned_data_dir / 'cleaned_data.parquet'\n",
    "\n",
    "        if not parquet_path.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"Kh√¥ng t√¨m th·∫•y file {parquet_path}\\n\"\n",
    "                f\"H√£y ch·∫°y step1_clean_data.py tr∆∞·ªõc!\"\n",
    "            )\n",
    "\n",
    "        df = pd.read_parquet(parquet_path)\n",
    "\n",
    "        # ƒê·ªçc feature names\n",
    "        feature_names_path = self.cleaned_data_dir / 'feature_names.txt'\n",
    "        if feature_names_path.exists():\n",
    "            with open(feature_names_path, 'r') as f:\n",
    "                self.feature_names = [line.strip() for line in f.readlines()]\n",
    "        else:\n",
    "            self.feature_names = [col for col in df.columns if col != 'binary_label']\n",
    "\n",
    "        # Th·ªëng k√™\n",
    "        self.stats['original_benign'] = int((df['binary_label'] == 0).sum())\n",
    "        self.stats['original_attack'] = int((df['binary_label'] == 1).sum())\n",
    "        self.stats['n_features'] = len(self.feature_names)\n",
    "\n",
    "        print(f\"   ‚úÖ ƒê√£ ƒë·ªçc: {len(df):,} m·∫´u\")\n",
    "        print(f\"   üìä Ph√¢n b·ªë g·ªëc:\")\n",
    "        print(f\"      - Benign: {self.stats['original_benign']:,} ({self.stats['original_benign']/len(df)*100:.1f}%)\")\n",
    "        print(f\"      - Attack: {self.stats['original_attack']:,} ({self.stats['original_attack']/len(df)*100:.1f}%)\")\n",
    "        print(f\"   üìã S·ªë features: {self.stats['n_features']}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def balanced_sample(self, df):\n",
    "        \"\"\"\n",
    "        Sample d·ªØ li·ªáu v·ªõi t·ª∑ l·ªá c√¢n b·∫±ng mong mu·ªën\n",
    "\n",
    "        Chi·∫øn l∆∞·ª£c:\n",
    "        - N·∫øu c√≥ ƒë·ªß m·∫´u: l·∫•y ƒë√∫ng s·ªë l∆∞·ª£ng target\n",
    "        - N·∫øu kh√¥ng ƒë·ªß Attack: gi·∫£m Benign t∆∞∆°ng ·ª©ng ƒë·ªÉ gi·ªØ t·ª∑ l·ªá\n",
    "        - N·∫øu kh√¥ng ƒë·ªß c·∫£ hai: l·∫•y t·ªëi ƒëa c√≥ th·ªÉ v·ªõi t·ª∑ l·ªá ƒë√∫ng\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚öñÔ∏è ƒêANG C√ÇN B·∫∞NG D·ªÆ LI·ªÜU...\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # T√°ch theo class\n",
    "        df_benign = df[df['binary_label'] == 0]\n",
    "        df_attack = df[df['binary_label'] == 1]\n",
    "\n",
    "        n_benign = len(df_benign)\n",
    "        n_attack = len(df_attack)\n",
    "\n",
    "        print(f\"\\n   üéØ Target mong mu·ªën:\")\n",
    "        print(f\"      - T·ªïng: {self.total_samples:,}\")\n",
    "        print(f\"      - Benign: {self.target_benign:,} ({self.benign_ratio*100:.0f}%)\")\n",
    "        print(f\"      - Attack: {self.target_attack:,} ({self.attack_ratio*100:.0f}%)\")\n",
    "\n",
    "        # X√°c ƒë·ªãnh s·ªë l∆∞·ª£ng th·ª±c t·∫ø c√≥ th·ªÉ l·∫•y\n",
    "        # ∆Øu ti√™n gi·ªØ ƒë√∫ng t·ª∑ l·ªá\n",
    "        actual_attack = min(self.target_attack, n_attack)\n",
    "        # T√≠nh Benign d·ª±a tr√™n Attack th·ª±c t·∫ø ƒë·ªÉ gi·ªØ t·ª∑ l·ªá\n",
    "        actual_benign = int(actual_attack * (self.benign_ratio / self.attack_ratio))\n",
    "        actual_benign = min(actual_benign, n_benign)\n",
    "\n",
    "        # N·∫øu Benign b·ªã gi·ªõi h·∫°n, ƒëi·ªÅu ch·ªânh Attack\n",
    "        if actual_benign < int(actual_attack * (self.benign_ratio / self.attack_ratio)):\n",
    "            actual_attack = int(actual_benign * (self.attack_ratio / self.benign_ratio))\n",
    "\n",
    "        print(f\"\\n   üìä S·ªë l∆∞·ª£ng th·ª±c t·∫ø s·∫Ω l·∫•y:\")\n",
    "        print(f\"      - Benign: {actual_benign:,}\")\n",
    "        print(f\"      - Attack: {actual_attack:,}\")\n",
    "        print(f\"      - T·ªïng: {actual_benign + actual_attack:,}\")\n",
    "        print(f\"      - T·ª∑ l·ªá th·ª±c t·∫ø: {actual_benign/(actual_benign+actual_attack)*100:.1f}% - {actual_attack/(actual_benign+actual_attack)*100:.1f}%\")\n",
    "\n",
    "        if actual_benign < self.target_benign or actual_attack < self.target_attack:\n",
    "            print(f\"\\n   ‚ö†Ô∏è Kh√¥ng ƒë·ªß m·∫´u ƒë·ªÉ ƒë·∫°t target!\")\n",
    "            print(f\"      C√≥ s·∫µn: Benign={n_benign:,}, Attack={n_attack:,}\")\n",
    "\n",
    "        # Random sample t·ª´ m·ªói class\n",
    "        print(f\"\\n   üîÑ ƒêang sample...\")\n",
    "\n",
    "        # S·ª≠ d·ª•ng random sampling\n",
    "        df_benign_sampled = df_benign.sample(n=actual_benign, random_state=RANDOM_STATE)\n",
    "        df_attack_sampled = df_attack.sample(n=actual_attack, random_state=RANDOM_STATE)\n",
    "\n",
    "        # G·ªôp l·∫°i v√† shuffle\n",
    "        df_balanced = pd.concat([df_benign_sampled, df_attack_sampled], ignore_index=True)\n",
    "        df_balanced = df_balanced.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "        # C·∫≠p nh·∫≠t stats\n",
    "        self.stats['sampled_benign'] = actual_benign\n",
    "        self.stats['sampled_attack'] = actual_attack\n",
    "\n",
    "        print(f\"\\n   ‚úÖ K·∫øt qu·∫£ sau khi c√¢n b·∫±ng:\")\n",
    "        print(f\"      - Benign: {actual_benign:,} ({actual_benign/(actual_benign+actual_attack)*100:.1f}%)\")\n",
    "        print(f\"      - Attack: {actual_attack:,} ({actual_attack/(actual_benign+actual_attack)*100:.1f}%)\")\n",
    "        print(f\"      - T·ªïng: {len(df_balanced):,}\")\n",
    "\n",
    "        # Gi·∫£i ph√≥ng b·ªô nh·ªõ\n",
    "        del df_benign, df_attack, df_benign_sampled, df_attack_sampled\n",
    "        gc.collect()\n",
    "\n",
    "        return df_balanced\n",
    "\n",
    "    def apply_log_transform(self, X):\n",
    "        \"\"\"\n",
    "        √Åp d·ª•ng Log Transform: log_e(1+x)\n",
    "\n",
    "        L∆∞u √Ω: log(1+x) gi√∫p:\n",
    "        - Gi·∫£m skewness c·ªßa d·ªØ li·ªáu\n",
    "        - X·ª≠ l√Ω c√°c gi√° tr·ªã l·ªõn\n",
    "        - B·∫£o to√†n gi√° tr·ªã 0 (log(1+0) = 0)\n",
    "        \"\"\"\n",
    "        print(\"\\nüî¢ ƒêANG √ÅP D·ª§NG LOG TRANSFORM: log_e(1+x)...\")\n",
    "\n",
    "        # ƒê·∫£m b·∫£o kh√¥ng c√≥ gi√° tr·ªã √¢m (log kh√¥ng x√°c ƒë·ªãnh cho s·ªë √¢m)\n",
    "        # V·ªõi d·ªØ li·ªáu network flow, c√°c gi√° tr·ªã th∆∞·ªùng >= 0\n",
    "        # N·∫øu c√≥ gi√° tr·ªã √¢m, ta shift ƒë·ªÉ min = 0\n",
    "        min_val = X.min()\n",
    "        if min_val < 0:\n",
    "            print(f\"   ‚ö†Ô∏è Ph√°t hi·ªán gi√° tr·ªã √¢m (min={min_val:.4f}), ƒëang shift...\")\n",
    "            X = X - min_val  # Shift ƒë·ªÉ min = 0\n",
    "\n",
    "        # √Åp d·ª•ng log(1+x)\n",
    "        X_log = np.log1p(X)  # log1p(x) = log(1+x), ·ªïn ƒë·ªãnh h∆°n v·ªõi x nh·ªè\n",
    "\n",
    "        print(f\"   ‚úÖ Log transform ho√†n t·∫•t\")\n",
    "        print(f\"      Range tr∆∞·ªõc: [{X.min():.4f}, {X.max():.4f}]\")\n",
    "        print(f\"      Range sau:   [{X_log.min():.4f}, {X_log.max():.4f}]\")\n",
    "\n",
    "        return X_log\n",
    "\n",
    "    def normalize_features(self, X):\n",
    "        \"\"\"\n",
    "        Chu·∫©n h√≥a features b·∫±ng StandardScaler\n",
    "        \"\"\"\n",
    "        print(\"\\nüìê ƒêANG CHU·∫®N H√ìA B·∫∞NG STANDARDSCALER...\")\n",
    "\n",
    "        X_normalized = self.scaler.fit_transform(X)\n",
    "\n",
    "        print(f\"   ‚úÖ StandardScaler ho√†n t·∫•t\")\n",
    "        print(f\"      Mean: {X_normalized.mean():.6f}\")\n",
    "        print(f\"      Std:  {X_normalized.std():.6f}\")\n",
    "\n",
    "        return X_normalized\n",
    "\n",
    "    def reshape_for_cnn(self, X):\n",
    "        \"\"\"\n",
    "        Reshape d·ªØ li·ªáu cho CNN 1D\n",
    "        CNN 1D y√™u c·∫ßu input shape: (samples, features, channels)\n",
    "        \"\"\"\n",
    "        print(\"\\nüîÑ ƒêANG RESHAPE CHO CNN 1D...\")\n",
    "\n",
    "        X_reshaped = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "        print(f\"   ‚úÖ Shape: {X.shape} -> {X_reshaped.shape}\")\n",
    "        print(f\"      (samples, features, channels)\")\n",
    "\n",
    "        return X_reshaped\n",
    "\n",
    "    def split_data(self, X, y):\n",
    "        \"\"\"\n",
    "        Chia d·ªØ li·ªáu th√†nh train/val/test\n",
    "\n",
    "        Th√™m validation: Train 70%, Val 10%, Test 20%\n",
    "\n",
    "        S·ª≠ d·ª•ng stratify ƒë·ªÉ gi·ªØ t·ª∑ l·ªá class trong t·∫•t c·∫£ c√°c t·∫≠p\n",
    "        \"\"\"\n",
    "        print(\"\\nüìä ƒêANG CHIA D·ªÆ LI·ªÜU TRAIN/VAL/TEST...\")\n",
    "\n",
    "        # B∆∞·ªõc 1: Chia train+val / test (80/20)\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            X, y,\n",
    "            test_size=self.test_size,\n",
    "            random_state=RANDOM_STATE,\n",
    "            stratify=y  # Gi·ªØ t·ª∑ l·ªá class\n",
    "        )\n",
    "\n",
    "        # B∆∞·ªõc 2: Chia train / val\n",
    "        val_ratio_from_temp = self.val_size / (1 - self.test_size)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp,\n",
    "            test_size=val_ratio_from_temp,\n",
    "            random_state=RANDOM_STATE,\n",
    "            stratify=y_temp\n",
    "        )\n",
    "\n",
    "        # C·∫≠p nh·∫≠t stats\n",
    "        self.stats['train_samples'] = len(X_train)\n",
    "        self.stats['val_samples'] = len(X_val)\n",
    "        self.stats['test_samples'] = len(X_test)\n",
    "\n",
    "        print(f\"\\n   üìà K·∫æT QU·∫¢ CHIA D·ªÆ LI·ªÜU:\")\n",
    "        print(f\"   {'='*50}\")\n",
    "        print(f\"   {'Set':<10} {'Samples':>12} {'Benign':>12} {'Attack':>12}\")\n",
    "        print(f\"   {'-'*50}\")\n",
    "        print(f\"   {'Train':<10} {len(X_train):>12,} {(y_train==0).sum():>12,} {(y_train==1).sum():>12,}\")\n",
    "        print(f\"   {'Val':<10} {len(X_val):>12,} {(y_val==0).sum():>12,} {(y_val==1).sum():>12,}\")\n",
    "        print(f\"   {'Test':<10} {len(X_test):>12,} {(y_test==0).sum():>12,} {(y_test==1).sum():>12,}\")\n",
    "        print(f\"   {'-'*50}\")\n",
    "        print(f\"   {'Total':<10} {len(X_train)+len(X_val)+len(X_test):>12,}\")\n",
    "\n",
    "        # Ki·ªÉm tra t·ª∑ l·ªá\n",
    "        print(f\"\\n   üìä T·ª∂ L·ªÜ ATTACK TRONG M·ªñI T·∫¨P:\")\n",
    "        print(f\"      Train: {(y_train==1).sum()/len(y_train)*100:.1f}%\")\n",
    "        print(f\"      Val:   {(y_val==1).sum()/len(y_val)*100:.1f}%\")\n",
    "        print(f\"      Test:  {(y_test==1).sum()/len(y_test)*100:.1f}%\")\n",
    "\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "    def save_training_data(self, X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "        \"\"\"\n",
    "        L∆∞u d·ªØ li·ªáu training\n",
    "\n",
    "        L∆∞u c√°c file:\n",
    "        - X_train.npy, X_val.npy, X_test.npy\n",
    "        - y_train.npy, y_val.npy, y_test.npy\n",
    "        - scaler.pkl\n",
    "        - training_metadata.json\n",
    "        - feature_names.txt\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üíæ ƒêANG L∆ØU D·ªÆ LI·ªÜU TRAINING...\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        # L∆∞u numpy arrays\n",
    "        np.save(self.output_dir / 'X_train.npy', X_train)\n",
    "        np.save(self.output_dir / 'X_val.npy', X_val)\n",
    "        np.save(self.output_dir / 'X_test.npy', X_test)\n",
    "        np.save(self.output_dir / 'y_train.npy', y_train)\n",
    "        np.save(self.output_dir / 'y_val.npy', y_val)\n",
    "        np.save(self.output_dir / 'y_test.npy', y_test)\n",
    "\n",
    "        print(f\"   ‚úÖ X_train.npy: {X_train.shape}\")\n",
    "        print(f\"   ‚úÖ X_val.npy:   {X_val.shape}\")\n",
    "        print(f\"   ‚úÖ X_test.npy:  {X_test.shape}\")\n",
    "        print(f\"   ‚úÖ y_train.npy: {y_train.shape}\")\n",
    "        print(f\"   ‚úÖ y_val.npy:   {y_val.shape}\")\n",
    "        print(f\"   ‚úÖ y_test.npy:  {y_test.shape}\")\n",
    "\n",
    "        # L∆∞u scaler\n",
    "        with open(self.output_dir / 'scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(self.scaler, f)\n",
    "        print(f\"   ‚úÖ scaler.pkl\")\n",
    "\n",
    "        # L∆∞u feature names\n",
    "        with open(self.output_dir / 'feature_names.txt', 'w') as f:\n",
    "            for name in self.feature_names:\n",
    "                f.write(name + '\\n')\n",
    "        print(f\"   ‚úÖ feature_names.txt\")\n",
    "\n",
    "        # Chu·∫©n b·ªã metadata\n",
    "        metadata = {\n",
    "            'n_features': len(self.feature_names),\n",
    "            'input_shape': [int(X_train.shape[1]), int(X_train.shape[2])],\n",
    "            'train_samples': int(X_train.shape[0]),\n",
    "            'val_samples': int(X_val.shape[0]),\n",
    "            'test_samples': int(X_test.shape[0]),\n",
    "            'total_samples': int(X_train.shape[0] + X_val.shape[0] + X_test.shape[0]),\n",
    "            'class_distribution': {\n",
    "                'train': {\n",
    "                    'benign': int((y_train == 0).sum()),\n",
    "                    'attack': int((y_train == 1).sum())\n",
    "                },\n",
    "                'val': {\n",
    "                    'benign': int((y_val == 0).sum()),\n",
    "                    'attack': int((y_val == 1).sum())\n",
    "                },\n",
    "                'test': {\n",
    "                    'benign': int((y_test == 0).sum()),\n",
    "                    'attack': int((y_test == 1).sum())\n",
    "                }\n",
    "            },\n",
    "            'benign_ratio': float(self.benign_ratio),\n",
    "            'attack_ratio': float(self.attack_ratio),\n",
    "            'preprocessing': {\n",
    "                'log_transform': 'log_e(1+x)',\n",
    "                'normalization': 'StandardScaler'\n",
    "            },\n",
    "            'created_at': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "\n",
    "        with open(self.output_dir / 'training_metadata.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"   ‚úÖ training_metadata.json\")\n",
    "\n",
    "        print(f\"\\nüìÅ T·∫•t c·∫£ file ƒë∆∞·ª£c l∆∞u t·∫°i: {self.output_dir}\")\n",
    "\n",
    "    def calculate_class_weights(self, y_train):\n",
    "        \"\"\"\n",
    "        T√≠nh class weights cho training\n",
    "\n",
    "        S·ª≠ d·ª•ng khi d·ªØ li·ªáu v·∫´n c√≤n imbalanced\n",
    "        \"\"\"\n",
    "        from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "        classes = np.unique(y_train)\n",
    "        weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "        class_weights = dict(zip(classes, weights))\n",
    "\n",
    "        print(f\"\\n‚öñÔ∏è CLASS WEIGHTS (cho training):\")\n",
    "        print(f\"   Class 0 (Benign): {class_weights[0]:.4f}\")\n",
    "        print(f\"   Class 1 (Attack): {class_weights[1]:.4f}\")\n",
    "\n",
    "        # L∆∞u class weights\n",
    "        with open(self.output_dir / 'class_weights.pkl', 'wb') as f:\n",
    "            pickle.dump(class_weights, f)\n",
    "        print(f\"   ‚úÖ ƒê√£ l∆∞u class_weights.pkl\")\n",
    "\n",
    "        return class_weights\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"H√†m ch√≠nh\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä B∆Ø·ªöC 2: CHU·∫®N B·ªä D·ªÆ LI·ªÜU TRAINING CHO CNN\")\n",
    "    print(\"   C√¢n b·∫±ng v√† chia train/val/test\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(f\"\\nüìã C·∫§U H√åNH:\")\n",
    "    print(f\"   - T·ªïng m·∫´u mong mu·ªën: {TOTAL_SAMPLES:,}\")\n",
    "    print(f\"   - T·ª∑ l·ªá Benign: {BENIGN_RATIO*100:.0f}%\")\n",
    "    print(f\"   - T·ª∑ l·ªá Attack: {ATTACK_RATIO*100:.0f}%\")\n",
    "    print(f\"   - Train/Val/Test: {(1-TEST_SIZE-VAL_SIZE)*100:.0f}%/{VAL_SIZE*100:.0f}%/{TEST_SIZE*100:.0f}%\")\n",
    "\n",
    "    # Kh·ªüi t·∫°o preparer\n",
    "    preparer = TrainingDataPreparer(\n",
    "        cleaned_data_dir=CLEANED_DATA_DIR,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        total_samples=TOTAL_SAMPLES,\n",
    "        benign_ratio=BENIGN_RATIO,\n",
    "        attack_ratio=ATTACK_RATIO,\n",
    "        test_size=TEST_SIZE,\n",
    "        val_size=VAL_SIZE\n",
    "    )\n",
    "\n",
    "    # B∆∞·ªõc 1: ƒê·ªçc d·ªØ li·ªáu ƒë√£ clean\n",
    "    df = preparer.load_cleaned_data()\n",
    "\n",
    "    # B∆∞·ªõc 2: C√¢n b·∫±ng d·ªØ li·ªáu\n",
    "    df = preparer.balanced_sample(df)\n",
    "\n",
    "    # T√°ch features v√† labels\n",
    "    X = df.drop(columns=['binary_label']).values\n",
    "    y = df['binary_label'].values\n",
    "\n",
    "    # Gi·∫£i ph√≥ng b·ªô nh·ªõ DataFrame\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    # B∆∞·ªõc 3: √Åp d·ª•ng Log Transform\n",
    "    X = preparer.apply_log_transform(X)\n",
    "\n",
    "    # B∆∞·ªõc 4: Chu·∫©n h√≥a\n",
    "    X = preparer.normalize_features(X)\n",
    "\n",
    "    # B∆∞·ªõc 5: Reshape cho CNN\n",
    "    X = preparer.reshape_for_cnn(X)\n",
    "\n",
    "    # B∆∞·ªõc 6: Chia train/val/test\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = preparer.split_data(X, y)\n",
    "\n",
    "    # Gi·∫£i ph√≥ng b·ªô nh·ªõ\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "\n",
    "    # B∆∞·ªõc 7: T√≠nh class weights\n",
    "    class_weights = preparer.calculate_class_weights(y_train)\n",
    "\n",
    "    # B∆∞·ªõc 8: L∆∞u d·ªØ li·ªáu\n",
    "    preparer.save_training_data(X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ HO√ÄN TH√ÄNH B∆Ø·ªöC 2!\")\n",
    "    print(\"   D·ªØ li·ªáu ƒë√£ s·∫µn s√†ng cho vi·ªác hu·∫•n luy·ªán CNN.\")\n",
    "    print(\"   Ch·∫°y step3_train_cnn.py ƒë·ªÉ train m√¥ h√¨nh.\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    return preparer\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preparer = main()\n",
    "\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-28T11:26:15.058839Z",
     "iopub.execute_input": "2025-12-28T11:26:15.059476Z",
     "iopub.status.idle": "2025-12-28T11:26:48.765794Z",
     "shell.execute_reply.started": "2025-12-28T11:26:15.059450Z",
     "shell.execute_reply": "2025-12-28T11:26:48.765118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "üåê ƒêang ch·∫°y tr√™n KAGGLE\n\n================================================================================\nüìä B∆Ø·ªöC 2: CHU·∫®N B·ªä D·ªÆ LI·ªÜU TRAINING CHO CNN\n   C√¢n b·∫±ng v√† chia train/val/test\n================================================================================\n\nüìã C·∫§U H√åNH:\n   - T·ªïng m·∫´u mong mu·ªën: 3,000,000\n   - T·ª∑ l·ªá Benign: 70%\n   - T·ª∑ l·ªá Attack: 30%\n   - Train/Val/Test: 70%/10%/20%\n\n================================================================================\nüìÇ ƒêANG ƒê·ªåC D·ªÆ LI·ªÜU ƒê√É CLEAN...\n================================================================================\n   ‚úÖ ƒê√£ ƒë·ªçc: 10,822,059 m·∫´u\n   üìä Ph√¢n b·ªë g·ªëc:\n      - Benign: 9,496,113 (87.7%)\n      - Attack: 1,325,946 (12.3%)\n   üìã S·ªë features: 69\n\n================================================================================\n‚öñÔ∏è ƒêANG C√ÇN B·∫∞NG D·ªÆ LI·ªÜU...\n================================================================================\n\n   üéØ Target mong mu·ªën:\n      - T·ªïng: 3,000,000\n      - Benign: 2,100,000 (70%)\n      - Attack: 900,000 (30%)\n\n   üìä S·ªë l∆∞·ª£ng th·ª±c t·∫ø s·∫Ω l·∫•y:\n      - Benign: 2,100,000\n      - Attack: 900,000\n      - T·ªïng: 3,000,000\n      - T·ª∑ l·ªá th·ª±c t·∫ø: 70.0% - 30.0%\n\n   üîÑ ƒêang sample...\n\n   ‚úÖ K·∫øt qu·∫£ sau khi c√¢n b·∫±ng:\n      - Benign: 2,100,000 (70.0%)\n      - Attack: 900,000 (30.0%)\n      - T·ªïng: 3,000,000\n\nüî¢ ƒêANG √ÅP D·ª§NG LOG TRANSFORM: log_e(1+x)...\n   ‚ö†Ô∏è Ph√°t hi·ªán gi√° tr·ªã √¢m (min=-947405000000.0000), ƒëang shift...\n   ‚úÖ Log transform ho√†n t·∫•t\n      Range tr∆∞·ªõc: [0.0000, 1927186000000.0000]\n      Range sau:   [0.0000, 28.2871]\n\nüìê ƒêANG CHU·∫®N H√ìA B·∫∞NG STANDARDSCALER...\n   ‚úÖ StandardScaler ho√†n t·∫•t\n      Mean: -0.000000\n      Std:  0.701964\n\nüîÑ ƒêANG RESHAPE CHO CNN 1D...\n   ‚úÖ Shape: (3000000, 69) -> (3000000, 69, 1)\n      (samples, features, channels)\n\nüìä ƒêANG CHIA D·ªÆ LI·ªÜU TRAIN/VAL/TEST...\n\n   üìà K·∫æT QU·∫¢ CHIA D·ªÆ LI·ªÜU:\n   ==================================================\n   Set             Samples       Benign       Attack\n   --------------------------------------------------\n   Train         2,100,000    1,470,000      630,000\n   Val             300,000      210,000       90,000\n   Test            600,000      420,000      180,000\n   --------------------------------------------------\n   Total         3,000,000\n\n   üìä T·ª∂ L·ªÜ ATTACK TRONG M·ªñI T·∫¨P:\n      Train: 30.0%\n      Val:   30.0%\n      Test:  30.0%\n\n‚öñÔ∏è CLASS WEIGHTS (cho training):\n   Class 0 (Benign): 0.7143\n   Class 1 (Attack): 1.6667\n   ‚úÖ ƒê√£ l∆∞u class_weights.pkl\n\n================================================================================\nüíæ ƒêANG L∆ØU D·ªÆ LI·ªÜU TRAINING...\n================================================================================\n   ‚úÖ X_train.npy: (2100000, 69, 1)\n   ‚úÖ X_val.npy:   (300000, 69, 1)\n   ‚úÖ X_test.npy:  (600000, 69, 1)\n   ‚úÖ y_train.npy: (2100000,)\n   ‚úÖ y_val.npy:   (300000,)\n   ‚úÖ y_test.npy:  (600000,)\n   ‚úÖ scaler.pkl\n   ‚úÖ feature_names.txt\n   ‚úÖ training_metadata.json\n\nüìÅ T·∫•t c·∫£ file ƒë∆∞·ª£c l∆∞u t·∫°i: /kaggle/working/training_data\n\n================================================================================\n‚úÖ HO√ÄN TH√ÄNH B∆Ø·ªöC 2!\n   D·ªØ li·ªáu ƒë√£ s·∫µn s√†ng cho vi·ªác hu·∫•n luy·ªán CNN.\n   Ch·∫°y step3_train_cnn.py ƒë·ªÉ train m√¥ h√¨nh.\n================================================================================\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\n======================================================================================\nB∆Ø·ªöC 3: TRAIN M√î H√åNH CNN CHO PH√ÅT HI·ªÜN L∆ØU L∆Ø·ª¢NG M·∫†NG IOT B·∫§T TH∆Ø·ªúNG\n======================================================================================\n\nKi·∫øn tr√∫c CNN theo y√™u c·∫ßu:\n- Input Layer: Shape (num_features, 1)\n- Conv1D (32 filters, kernel 2) -> MaxPooling1D (2)\n- Conv1D (32 filters, kernel 2) -> MaxPooling1D (2)\n- Conv1D (64 filters, kernel 2) -> MaxPooling1D (2)\n- Conv1D (64 filters, kernel 2) -> MaxPooling1D (2)\n- Conv1D (64 filters, kernel 2) -> MaxPooling1D (2)\n- BatchNormalization + Dropout (0.5)\n- Flatten\n- Dense(1, activation='sigmoid')\n\nLoss: binary_crossentropy\nOptimizer: Adam\nMetrics: Accuracy, Precision, Recall\n\nC√≥ th·ªÉ ch·∫°y tr√™n c·∫£ Kaggle v√† Local\n\"\"\"\n\nimport os\nimport numpy as np\nimport pickle\nimport json\nfrom pathlib import Path\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ============================================================================\n# TENSORFLOW/KERAS\n# ============================================================================\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import (\n    Conv1D, MaxPooling1D, Flatten, Dense,\n    Dropout, BatchNormalization, Input\n)\nfrom tensorflow.keras.callbacks import (\n    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n)\nfrom tensorflow.keras.metrics import Precision, Recall\n\n# Ki·ªÉm tra GPU\nprint(\"=\"*80)\nprint(\"üñ•Ô∏è TH√îNG TIN H·ªÜ TH·ªêNG\")\nprint(\"=\"*80)\nprint(f\"TensorFlow version: {tf.__version__}\")\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    print(f\"‚úÖ GPU available: {len(gpus)} GPU(s)\")\n    for gpu in gpus:\n        print(f\"   - {gpu}\")\n    # C·∫•u h√¨nh GPU memory growth ƒë·ªÉ tr√°nh chi·∫øm h·∫øt b·ªô nh·ªõ\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu, True)\nelse:\n    print(\"‚ö†Ô∏è Kh√¥ng c√≥ GPU, s·∫Ω s·ª≠ d·ª•ng CPU\")\n\n# Ki·ªÉm tra m√¥i tr∆∞·ªùng ch·∫°y\nIS_KAGGLE = os.path.exists('/kaggle/input')\n\n# ============================================================================\n# C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N\n# ============================================================================\nif IS_KAGGLE:\n    TRAINING_DATA_DIR = \"/kaggle/working/training_data\"\n    MODEL_DIR = \"/kaggle/working/models\"\n    LOG_DIR = \"/kaggle/working/logs\"\n    print(\"üåê ƒêang ch·∫°y tr√™n KAGGLE\")\nelse:\n    TRAINING_DATA_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CNN\\training_data\"\n    MODEL_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CNN\\models\"\n    LOG_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CNN\\logs\"\n    print(\"üíª ƒêang ch·∫°y tr√™n LOCAL\")\n\n# ============================================================================\n# C·∫§U H√åNH HU·∫§N LUY·ªÜN\n# ============================================================================\n\n# Hyperparameters\nBATCH_SIZE = 256        # Batch size cho training\nEPOCHS = 50             # S·ªë epochs t·ªëi ƒëa\nLEARNING_RATE = 0.001   # Learning rate ban ƒë·∫ßu\n\n# Regularization\nDROPOUT_RATE = 0.5      # Dropout rate tr∆∞·ªõc Flatten\n\n# Early stopping\nPATIENCE = 10           # S·ªë epochs ch·ªù tr∆∞·ªõc khi d·ª´ng\n\n# Random seed\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)\n\n\n# ============================================================================\n# H√ÄM X√ÇY D·ª∞NG M√î H√åNH CNN\n# ============================================================================\n\ndef build_cnn_model(input_shape):\n    \"\"\"\n    X√¢y d·ª±ng m√¥ h√¨nh CNN cho ph√¢n lo·∫°i binary\n\n    Ki·∫øn tr√∫c theo y√™u c·∫ßu:\n    - 5 l·ªõp Conv1D v·ªõi MaxPooling\n    - BatchNormalization v√† Dropout tr∆∞·ªõc Flatten\n    - Output layer v·ªõi sigmoid activation\n\n    Args:\n        input_shape: Shape c·ªßa input (n_features, 1)\n\n    Returns:\n        model: Keras Sequential model\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"üèóÔ∏è ƒêANG X√ÇY D·ª∞NG M√î H√åNH CNN\")\n    print(\"=\"*80)\n    print(f\"   Input shape: {input_shape}\")\n\n    model = Sequential(name='CNN_Binary_Classification')\n\n    # Input layer\n    model.add(Input(shape=input_shape))\n\n    # ========== KH·ªêI CONV 1 ==========\n    # Conv1D (32 filters, kernel 2x1) -> MaxPooling1D (2)\n    model.add(Conv1D(\n        filters=32,\n        kernel_size=2,\n        activation='relu',\n        padding='same',  # Gi·ªØ nguy√™n k√≠ch th∆∞·ªõc\n        name='conv1d_1'\n    ))\n    model.add(MaxPooling1D(pool_size=2, name='maxpool_1'))\n\n    # ========== KH·ªêI CONV 2 ==========\n    # Conv1D (32 filters, kernel 2x1) -> MaxPooling1D (2)\n    model.add(Conv1D(\n        filters=32,\n        kernel_size=2,\n        activation='relu',\n        padding='same',\n        name='conv1d_2'\n    ))\n    model.add(MaxPooling1D(pool_size=2, name='maxpool_2'))\n\n    # ========== KH·ªêI CONV 3 ==========\n    # Conv1D (64 filters, kernel 2x1) -> MaxPooling1D (2)\n    model.add(Conv1D(\n        filters=64,\n        kernel_size=2,\n        activation='relu',\n        padding='same',\n        name='conv1d_3'\n    ))\n    model.add(MaxPooling1D(pool_size=2, name='maxpool_3'))\n\n    # ========== KH·ªêI CONV 4 ==========\n    # Conv1D (64 filters, kernel 2x1) -> MaxPooling1D (2)\n    model.add(Conv1D(\n        filters=64,\n        kernel_size=2,\n        activation='relu',\n        padding='same',\n        name='conv1d_4'\n    ))\n    model.add(MaxPooling1D(pool_size=2, name='maxpool_4'))\n\n    # ========== KH·ªêI CONV 5 ==========\n    # Conv1D (64 filters, kernel 2x1) -> MaxPooling1D (2)\n    model.add(Conv1D(\n        filters=64,\n        kernel_size=2,\n        activation='relu',\n        padding='same',\n        name='conv1d_5'\n    ))\n    model.add(MaxPooling1D(pool_size=2, name='maxpool_5'))\n\n    # ========== REGULARIZATION ==========\n    # BatchNormalization v√† Dropout tr∆∞·ªõc Flatten\n    model.add(BatchNormalization(name='batch_norm'))\n    model.add(Dropout(DROPOUT_RATE, name='dropout'))\n\n    # ========== FLATTEN ==========\n    model.add(Flatten(name='flatten'))\n\n    # ========== OUTPUT LAYER ==========\n    # Dense(1, activation='sigmoid') cho binary classification\n    model.add(Dense(1, activation='sigmoid', name='output'))\n\n    # ========== COMPILE ==========\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n        loss='binary_crossentropy',\n        metrics=[\n            'accuracy',\n            Precision(name='precision'),\n            Recall(name='recall')\n        ]\n    )\n\n    # In t√≥m t·∫Øt m√¥ h√¨nh\n    print(\"\\n   üìã KI·∫æN TR√öC M√î H√åNH:\")\n    model.summary()\n\n    return model\n\n\ndef load_training_data(data_dir):\n    \"\"\"\n    Load d·ªØ li·ªáu training ƒë√£ ƒë∆∞·ª£c chu·∫©n b·ªã t·ª´ step 2\n\n    Args:\n        data_dir: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu\n\n    Returns:\n        X_train, X_val, X_test, y_train, y_val, y_test, class_weights\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"üìÇ ƒêANG LOAD D·ªÆ LI·ªÜU TRAINING...\")\n    print(\"=\"*80)\n\n    data_dir = Path(data_dir)\n\n    # Load numpy arrays\n    X_train = np.load(data_dir / 'X_train.npy')\n    X_val = np.load(data_dir / 'X_val.npy')\n    X_test = np.load(data_dir / 'X_test.npy')\n    y_train = np.load(data_dir / 'y_train.npy')\n    y_val = np.load(data_dir / 'y_val.npy')\n    y_test = np.load(data_dir / 'y_test.npy')\n\n    print(f\"   ‚úÖ X_train: {X_train.shape}\")\n    print(f\"   ‚úÖ X_val:   {X_val.shape}\")\n    print(f\"   ‚úÖ X_test:  {X_test.shape}\")\n    print(f\"   ‚úÖ y_train: {y_train.shape}\")\n    print(f\"   ‚úÖ y_val:   {y_val.shape}\")\n    print(f\"   ‚úÖ y_test:  {y_test.shape}\")\n\n    # Load class weights n·∫øu c√≥\n    class_weights = None\n    class_weights_path = data_dir / 'class_weights.pkl'\n    if class_weights_path.exists():\n        with open(class_weights_path, 'rb') as f:\n            class_weights = pickle.load(f)\n        print(f\"\\n   ‚öñÔ∏è Class weights loaded:\")\n        print(f\"      Class 0 (Benign): {class_weights[0]:.4f}\")\n        print(f\"      Class 1 (Attack): {class_weights[1]:.4f}\")\n\n    # Th·ªëng k√™ ph√¢n b·ªë\n    print(f\"\\n   üìä PH√ÇN B·ªê D·ªÆ LI·ªÜU:\")\n    for name, y in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:\n        benign = (y == 0).sum()\n        attack = (y == 1).sum()\n        total = len(y)\n        print(f\"      {name}: Benign={benign:,} ({benign/total*100:.1f}%), Attack={attack:,} ({attack/total*100:.1f}%)\")\n\n    return X_train, X_val, X_test, y_train, y_val, y_test, class_weights\n\n\ndef create_callbacks(model_dir, log_dir):\n    \"\"\"\n    T·∫°o c√°c callbacks cho training\n\n    Callbacks:\n    - EarlyStopping: D·ª´ng s·ªõm khi val_loss kh√¥ng gi·∫£m\n    - ModelCheckpoint: L∆∞u model t·ªët nh·∫•t\n    - ReduceLROnPlateau: Gi·∫£m learning rate khi plateau\n    - TensorBoard: Logging cho visualization\n    \"\"\"\n    print(\"\\nüìå ƒêANG C·∫§U H√åNH CALLBACKS...\")\n\n    model_dir = Path(model_dir)\n    log_dir = Path(log_dir)\n    model_dir.mkdir(parents=True, exist_ok=True)\n    log_dir.mkdir(parents=True, exist_ok=True)\n\n    callbacks = []\n\n    # 1. Early Stopping\n    # D·ª´ng training khi val_loss kh√¥ng c·∫£i thi·ªán sau PATIENCE epochs\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=PATIENCE,\n        verbose=1,\n        mode='min',\n        restore_best_weights=True  # Kh√¥i ph·ª•c weights t·ªët nh·∫•t\n    )\n    callbacks.append(early_stopping)\n    print(f\"   ‚úÖ EarlyStopping: patience={PATIENCE}\")\n\n    # 2. Model Checkpoint\n    # L∆∞u model c√≥ val_loss th·∫•p nh·∫•t\n    checkpoint_path = model_dir / 'best_model.keras'\n    model_checkpoint = ModelCheckpoint(\n        filepath=str(checkpoint_path),\n        monitor='val_loss',\n        verbose=1,\n        save_best_only=True,\n        mode='min'\n    )\n    callbacks.append(model_checkpoint)\n    print(f\"   ‚úÖ ModelCheckpoint: {checkpoint_path}\")\n\n    # 3. Reduce Learning Rate on Plateau\n    # Gi·∫£m LR khi val_loss kh√¥ng gi·∫£m\n    reduce_lr = ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,        # Gi·∫£m LR c√≤n 1/2\n        patience=5,        # Ch·ªù 5 epochs\n        min_lr=1e-7,       # LR t·ªëi thi·ªÉu\n        verbose=1\n    )\n    callbacks.append(reduce_lr)\n    print(f\"   ‚úÖ ReduceLROnPlateau: factor=0.5, patience=5\")\n\n    # 4. TensorBoard (optional)\n    tensorboard_log = log_dir / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    tensorboard = TensorBoard(\n        log_dir=str(tensorboard_log),\n        histogram_freq=1\n    )\n    callbacks.append(tensorboard)\n    print(f\"   ‚úÖ TensorBoard: {tensorboard_log}\")\n\n    return callbacks\n\n\ndef train_model(model, X_train, y_train, X_val, y_val, class_weights, callbacks):\n    \"\"\"\n    Hu·∫•n luy·ªán m√¥ h√¨nh\n\n    Args:\n        model: Keras model\n        X_train, y_train: D·ªØ li·ªáu training\n        X_val, y_val: D·ªØ li·ªáu validation\n        class_weights: Dictionary class weights\n        callbacks: List c√°c callbacks\n\n    Returns:\n        history: Training history\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"üöÄ B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN M√î H√åNH\")\n    print(\"=\"*80)\n    print(f\"   Epochs: {EPOCHS}\")\n    print(f\"   Batch size: {BATCH_SIZE}\")\n    print(f\"   Learning rate: {LEARNING_RATE}\")\n    print(f\"   Class weights: {'C√≥' if class_weights else 'Kh√¥ng'}\")\n\n    start_time = datetime.now()\n\n    history = model.fit(\n        X_train, y_train,\n        batch_size=BATCH_SIZE,\n        epochs=EPOCHS,\n        validation_data=(X_val, y_val),\n        class_weight=class_weights,  # S·ª≠ d·ª•ng class weights ƒë·ªÉ x·ª≠ l√Ω imbalance\n        callbacks=callbacks,\n        verbose=1\n    )\n\n    end_time = datetime.now()\n    training_time = (end_time - start_time).total_seconds()\n\n    print(f\"\\n   ‚è±Ô∏è Th·ªùi gian training: {training_time/60:.2f} ph√∫t\")\n    print(f\"   üìà Best val_loss: {min(history.history['val_loss']):.4f}\")\n    print(f\"   üìà Best val_accuracy: {max(history.history['val_accuracy']):.4f}\")\n\n    return history, training_time\n\n\ndef evaluate_model(model, X_test, y_test):\n    \"\"\"\n    ƒê√°nh gi√° m√¥ h√¨nh tr√™n test set\n\n    Args:\n        model: Trained model\n        X_test, y_test: D·ªØ li·ªáu test\n\n    Returns:\n        results: Dictionary k·∫øt qu·∫£ ƒë√°nh gi√°\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"üìä ƒê√ÅNH GI√Å M√î H√åNH TR√äN TEST SET\")\n    print(\"=\"*80)\n\n    # Evaluate\n    loss, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=1)\n\n    # T√≠nh F1-score\n    f1_score = 2 * (precision * recall) / (precision + recall + 1e-7)\n\n    results = {\n        'test_loss': float(loss),\n        'test_accuracy': float(accuracy),\n        'test_precision': float(precision),\n        'test_recall': float(recall),\n        'test_f1_score': float(f1_score)\n    }\n\n    print(f\"\\n   üìä K·∫æT QU·∫¢:\")\n    print(f\"   {'='*40}\")\n    print(f\"   Loss:      {loss:.4f}\")\n    print(f\"   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n    print(f\"   Precision: {precision:.4f}\")\n    print(f\"   Recall:    {recall:.4f}\")\n    print(f\"   F1-Score:  {f1_score:.4f}\")\n    print(f\"   {'='*40}\")\n\n    # Predictions cho confusion matrix\n    y_pred_prob = model.predict(X_test, verbose=0)\n    y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n\n    # Confusion matrix\n    from sklearn.metrics import confusion_matrix, classification_report\n\n    cm = confusion_matrix(y_test, y_pred)\n    print(f\"\\n   üìã CONFUSION MATRIX:\")\n    print(f\"                 Predicted\")\n    print(f\"                 Benign  Attack\")\n    print(f\"   Actual Benign  {cm[0,0]:>6}  {cm[0,1]:>6}\")\n    print(f\"   Actual Attack  {cm[1,0]:>6}  {cm[1,1]:>6}\")\n\n    print(f\"\\n   üìã CLASSIFICATION REPORT:\")\n    report = classification_report(y_test, y_pred, target_names=['Benign', 'Attack'])\n    print(report)\n\n    # L∆∞u classification report d·∫°ng dictionary\n    report_dict = classification_report(y_test, y_pred, target_names=['Benign', 'Attack'], output_dict=True)\n\n    # Th√™m confusion matrix v√† c√°c metrics kh√°c v√†o results\n    results['confusion_matrix'] = cm.tolist()\n    results['classification_report'] = report_dict\n\n    # Th√™m c√°c metrics chi ti·∫øt cho t·ª´ng class\n    results['benign_precision'] = float(report_dict['Benign']['precision'])\n    results['benign_recall'] = float(report_dict['Benign']['recall'])\n    results['benign_f1'] = float(report_dict['Benign']['f1-score'])\n    results['attack_precision'] = float(report_dict['Attack']['precision'])\n    results['attack_recall'] = float(report_dict['Attack']['recall'])\n    results['attack_f1'] = float(report_dict['Attack']['f1-score'])\n\n    # T√≠nh th√™m m·ªôt s·ªë metrics b·ªï sung\n    tn, fp, fn, tp = cm.ravel()\n    results['true_negative'] = int(tn)\n    results['false_positive'] = int(fp)\n    results['false_negative'] = int(fn)\n    results['true_positive'] = int(tp)\n    results['specificity'] = float(tn / (tn + fp + 1e-7))  # True Negative Rate\n    results['false_positive_rate'] = float(fp / (fp + tn + 1e-7))\n    results['false_negative_rate'] = float(fn / (fn + tp + 1e-7))\n\n    return results, y_pred, y_pred_prob\n\n\ndef save_model_and_results(model, history, results, training_time, model_dir, y_pred=None, y_pred_prob=None):\n    \"\"\"\n    L∆∞u model v√† k·∫øt qu·∫£ training\n\n    Args:\n        model: Trained model\n        history: Training history\n        results: Evaluation results\n        training_time: Th·ªùi gian training (seconds)\n        model_dir: ƒê∆∞·ªùng d·∫´n l∆∞u\n        y_pred: Predictions (optional)\n        y_pred_prob: Prediction probabilities (optional)\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"üíæ ƒêANG L∆ØU MODEL V√Ä K·∫æT QU·∫¢...\")\n    print(\"=\"*80)\n\n    model_dir = Path(model_dir)\n    model_dir.mkdir(parents=True, exist_ok=True)\n\n    # L∆∞u model cu·ªëi c√πng\n    final_model_path = model_dir / 'final_model.keras'\n    model.save(final_model_path)\n    print(f\"   ‚úÖ Final model: {final_model_path}\")\n\n    # L∆∞u model weights\n    weights_path = model_dir / 'model_weights.weights.h5'\n    model.save_weights(weights_path)\n    print(f\"   ‚úÖ Model weights: {weights_path}\")\n\n    # L∆∞u training history\n    history_dict = {key: [float(v) for v in values] for key, values in history.history.items()}\n    with open(model_dir / 'training_history.json', 'w') as f:\n        json.dump(history_dict, f, indent=4)\n    print(f\"   ‚úÖ Training history: training_history.json\")\n\n    # L∆∞u k·∫øt qu·∫£ ƒë√°nh gi√° v·ªõi th√¥ng tin b·ªï sung\n    results['training_time_seconds'] = float(training_time)\n    results['training_time_minutes'] = float(training_time / 60)\n    results['epochs_trained'] = int(len(history.history['loss']))\n\n    # Th√™m best validation metrics\n    results['best_val_loss'] = float(min(history.history['val_loss']))\n    results['best_val_accuracy'] = float(max(history.history['val_accuracy']))\n    results['best_val_precision'] = float(max(history.history['val_precision']))\n    results['best_val_recall'] = float(max(history.history['val_recall']))\n\n    # T√≠nh best val F1-score\n    val_precisions = history.history['val_precision']\n    val_recalls = history.history['val_recall']\n    val_f1_scores = [2 * (p * r) / (p + r + 1e-7) for p, r in zip(val_precisions, val_recalls)]\n    results['best_val_f1_score'] = float(max(val_f1_scores))\n\n    # Epoch n√†o ƒë·∫°t best val_loss\n    results['best_val_loss_epoch'] = int(np.argmin(history.history['val_loss']) + 1)\n    results['best_val_accuracy_epoch'] = int(np.argmax(history.history['val_accuracy']) + 1)\n\n    with open(model_dir / 'evaluation_results.json', 'w') as f:\n        json.dump(results, f, indent=4)\n    print(f\"   ‚úÖ Evaluation results: evaluation_results.json\")\n\n    # L∆∞u predictions n·∫øu c√≥\n    if y_pred is not None:\n        np.save(model_dir / 'y_pred.npy', y_pred)\n        print(f\"   ‚úÖ Predictions: y_pred.npy\")\n\n    if y_pred_prob is not None:\n        np.save(model_dir / 'y_pred_prob.npy', y_pred_prob)\n        print(f\"   ‚úÖ Prediction probabilities: y_pred_prob.npy\")\n\n    # L∆∞u c·∫•u h√¨nh training\n    config = {\n        'batch_size': BATCH_SIZE,\n        'epochs': EPOCHS,\n        'learning_rate': LEARNING_RATE,\n        'dropout_rate': DROPOUT_RATE,\n        'patience': PATIENCE,\n        'random_seed': RANDOM_SEED,\n        'tensorflow_version': tf.__version__,\n        'created_at': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    }\n    with open(model_dir / 'training_config.json', 'w') as f:\n        json.dump(config, f, indent=4)\n    print(f\"   ‚úÖ Training config: training_config.json\")\n\n    print(f\"\\nüìÅ T·∫•t c·∫£ file ƒë∆∞·ª£c l∆∞u t·∫°i: {model_dir}\")\n\n\ndef plot_training_history(history, model_dir):\n    \"\"\"\n    V·∫Ω bi·ªÉu ƒë·ªì training history\n\n    Args:\n        history: Training history\n        model_dir: ƒê∆∞·ªùng d·∫´n l∆∞u h√¨nh\n    \"\"\"\n    try:\n        import matplotlib.pyplot as plt\n\n        model_dir = Path(model_dir)\n        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n        # 1. Loss\n        axes[0, 0].plot(history.history['loss'], label='Train Loss')\n        axes[0, 0].plot(history.history['val_loss'], label='Val Loss')\n        axes[0, 0].set_title('Model Loss')\n        axes[0, 0].set_xlabel('Epoch')\n        axes[0, 0].set_ylabel('Loss')\n        axes[0, 0].legend()\n        axes[0, 0].grid(True)\n\n        # 2. Accuracy\n        axes[0, 1].plot(history.history['accuracy'], label='Train Accuracy')\n        axes[0, 1].plot(history.history['val_accuracy'], label='Val Accuracy')\n        axes[0, 1].set_title('Model Accuracy')\n        axes[0, 1].set_xlabel('Epoch')\n        axes[0, 1].set_ylabel('Accuracy')\n        axes[0, 1].legend()\n        axes[0, 1].grid(True)\n\n        # 3. Precision\n        axes[1, 0].plot(history.history['precision'], label='Train Precision')\n        axes[1, 0].plot(history.history['val_precision'], label='Val Precision')\n        axes[1, 0].set_title('Model Precision')\n        axes[1, 0].set_xlabel('Epoch')\n        axes[1, 0].set_ylabel('Precision')\n        axes[1, 0].legend()\n        axes[1, 0].grid(True)\n\n        # 4. Recall\n        axes[1, 1].plot(history.history['recall'], label='Train Recall')\n        axes[1, 1].plot(history.history['val_recall'], label='Val Recall')\n        axes[1, 1].set_title('Model Recall')\n        axes[1, 1].set_xlabel('Epoch')\n        axes[1, 1].set_ylabel('Recall')\n        axes[1, 1].legend()\n        axes[1, 1].grid(True)\n\n        plt.tight_layout()\n        plt.savefig(model_dir / 'training_history.png', dpi=150)\n        plt.close()\n        print(f\"   ‚úÖ Training history plot: training_history.png\")\n\n    except ImportError:\n        print(\"   ‚ö†Ô∏è matplotlib kh√¥ng c√≥ s·∫µn, b·ªè qua vi·ªác v·∫Ω bi·ªÉu ƒë·ªì\")\n\n\ndef plot_confusion_matrix(cm, model_dir):\n    \"\"\"\n    V·∫Ω confusion matrix d∆∞·ªõi d·∫°ng heatmap\n\n    Args:\n        cm: Confusion matrix (numpy array ho·∫∑c list)\n        model_dir: ƒê∆∞·ªùng d·∫´n l∆∞u h√¨nh\n    \"\"\"\n    try:\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        model_dir = Path(model_dir)\n\n        # Convert to numpy array if needed\n        if isinstance(cm, list):\n            cm = np.array(cm)\n\n        # V·∫Ω heatmap\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                   xticklabels=['Benign', 'Attack'],\n                   yticklabels=['Benign', 'Attack'],\n                   cbar_kws={'label': 'Count'})\n        plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n        plt.ylabel('Actual', fontsize=12)\n        plt.xlabel('Predicted', fontsize=12)\n        plt.tight_layout()\n        plt.savefig(model_dir / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n        plt.close()\n        print(f\"   ‚úÖ Confusion matrix plot: confusion_matrix.png\")\n\n        # V·∫Ω normalized confusion matrix\n        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n                   xticklabels=['Benign', 'Attack'],\n                   yticklabels=['Benign', 'Attack'],\n                   cbar_kws={'label': 'Percentage'})\n        plt.title('Normalized Confusion Matrix', fontsize=16, fontweight='bold')\n        plt.ylabel('Actual', fontsize=12)\n        plt.xlabel('Predicted', fontsize=12)\n        plt.tight_layout()\n        plt.savefig(model_dir / 'confusion_matrix_normalized.png', dpi=150, bbox_inches='tight')\n        plt.close()\n        print(f\"   ‚úÖ Normalized confusion matrix plot: confusion_matrix_normalized.png\")\n\n    except ImportError as e:\n        print(f\"   ‚ö†Ô∏è matplotlib/seaborn kh√¥ng c√≥ s·∫µn, b·ªè qua vi·ªác v·∫Ω confusion matrix: {e}\")\n\n\ndef main():\n    \"\"\"H√†m ch√≠nh ƒë·ªÉ train model\"\"\"\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"üß† HU·∫§N LUY·ªÜN M√î H√åNH CNN - PH√ÅT HI·ªÜN L∆ØU L∆Ø·ª¢NG M·∫†NG B·∫§T TH∆Ø·ªúNG\")\n    print(\"   Binary Classification: Benign vs Attack\")\n    print(\"=\"*80)\n\n    # B∆∞·ªõc 1: Load d·ªØ li·ªáu\n    X_train, X_val, X_test, y_train, y_val, y_test, class_weights = load_training_data(TRAINING_DATA_DIR)\n\n    # B∆∞·ªõc 2: X√¢y d·ª±ng m√¥ h√¨nh\n    input_shape = (X_train.shape[1], X_train.shape[2])  # (n_features, 1)\n    model = build_cnn_model(input_shape)\n\n    # B∆∞·ªõc 3: T·∫°o callbacks\n    callbacks = create_callbacks(MODEL_DIR, LOG_DIR)\n\n    # B∆∞·ªõc 4: Hu·∫•n luy·ªán\n    history, training_time = train_model(\n        model, X_train, y_train, X_val, y_val, class_weights, callbacks\n    )\n\n    # B∆∞·ªõc 5: ƒê√°nh gi√°\n    results, y_pred, y_pred_prob = evaluate_model(model, X_test, y_test)\n\n    # B∆∞·ªõc 6: L∆∞u model v√† k·∫øt qu·∫£\n    save_model_and_results(model, history, results, training_time, MODEL_DIR, y_pred, y_pred_prob)\n\n    # B∆∞·ªõc 7: V·∫Ω bi·ªÉu ƒë·ªì\n    plot_training_history(history, MODEL_DIR)\n\n    # B∆∞·ªõc 8: V·∫Ω confusion matrix\n    if 'confusion_matrix' in results:\n        plot_confusion_matrix(results['confusion_matrix'], MODEL_DIR)\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"‚úÖ HO√ÄN TH√ÄNH HU·∫§N LUY·ªÜN!\")\n    print(f\"   Test Accuracy:  {results['test_accuracy']*100:.2f}%\")\n    print(f\"   Test Precision: {results['test_precision']*100:.2f}%\")\n    print(f\"   Test Recall:    {results['test_recall']*100:.2f}%\")\n    print(f\"   Test F1-Score:  {results['test_f1_score']*100:.2f}%\")\n    print(\"=\"*80)\n\n    return model, history, results\n\n\nif __name__ == \"__main__\":\n    model, history, results = main()\n\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-28T12:12:43.396114Z",
     "iopub.execute_input": "2025-12-28T12:12:43.396494Z",
     "iopub.status.idle": "2025-12-28T12:48:30.250915Z",
     "shell.execute_reply.started": "2025-12-28T12:12:43.396462Z",
     "shell.execute_reply": "2025-12-28T12:48:30.250208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "================================================================================\nüñ•Ô∏è TH√îNG TIN H·ªÜ TH·ªêNG\n================================================================================\nTensorFlow version: 2.18.0\n‚úÖ GPU available: 2 GPU(s)\n   - PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n   - PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\nüåê ƒêang ch·∫°y tr√™n KAGGLE\n\n================================================================================\nüß† HU·∫§N LUY·ªÜN M√î H√åNH CNN - PH√ÅT HI·ªÜN L∆ØU L∆Ø·ª¢NG M·∫†NG B·∫§T TH∆Ø·ªúNG\n   Binary Classification: Benign vs Attack\n================================================================================\n\n================================================================================\nüìÇ ƒêANG LOAD D·ªÆ LI·ªÜU TRAINING...\n================================================================================\n   ‚úÖ X_train: (2100000, 69, 1)\n   ‚úÖ X_val:   (300000, 69, 1)\n   ‚úÖ X_test:  (600000, 69, 1)\n   ‚úÖ y_train: (2100000,)\n   ‚úÖ y_val:   (300000,)\n   ‚úÖ y_test:  (600000,)\n\n   ‚öñÔ∏è Class weights loaded:\n      Class 0 (Benign): 0.7143\n      Class 1 (Attack): 1.6667\n\n   üìä PH√ÇN B·ªê D·ªÆ LI·ªÜU:\n      Train: Benign=1,470,000 (70.0%), Attack=630,000 (30.0%)\n      Val: Benign=210,000 (70.0%), Attack=90,000 (30.0%)\n      Test: Benign=420,000 (70.0%), Attack=180,000 (30.0%)\n\n================================================================================\nüèóÔ∏è ƒêANG X√ÇY D·ª∞NG M√î H√åNH CNN\n================================================================================\n   Input shape: (69, 1)\n\n   üìã KI·∫æN TR√öC M√î H√åNH:\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001B[1mModel: \"CNN_Binary_Classification\"\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"CNN_Binary_Classification\"</span>\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m‚îÉ\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m‚îÉ\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ conv1d_1 (\u001B[38;5;33mConv1D\u001B[0m)               ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m69\u001B[0m, \u001B[38;5;34m32\u001B[0m)         ‚îÇ            \u001B[38;5;34m96\u001B[0m ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ maxpool_1 (\u001B[38;5;33mMaxPooling1D\u001B[0m)        ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m34\u001B[0m, \u001B[38;5;34m32\u001B[0m)         ‚îÇ             \u001B[38;5;34m0\u001B[0m ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ conv1d_2 (\u001B[38;5;33mConv1D\u001B[0m)               ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m34\u001B[0m, \u001B[38;5;34m32\u001B[0m)         ‚îÇ         \u001B[38;5;34m2,080\u001B[0m ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ maxpool_2 (\u001B[38;5;33mMaxPooling1D\u001B[0m)        ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m17\u001B[0m, \u001B[38;5;34m32\u001B[0m)         ‚îÇ             \u001B[38;5;34m0\u001B[0m ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ conv1d_3 (\u001B[38;5;33mConv1D\u001B[0m)               ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m17\u001B[0m, \u001B[38;5;34m64\u001B[0m)         ‚îÇ         \u001B[38;5;34m4,160\u001B[0m ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ maxpool_3 (\u001B[38;5;33mMaxPooling1D\u001B[0m)        ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m8\u001B[0m, \u001B[38;5;34m64\u001B[0m)          ‚îÇ             \u001B[38;5;34m0\u001B[0m ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ conv1d_4 (\u001B[38;5;33mConv1D\u001B[0m)               ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m8\u001B[0m, \u001B[38;5;34m64\u001B[0m)          ‚îÇ         \u001B[38;5;34m8,256\u001B[0m ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ maxpool_4 (\u001B[38;5;33mMaxPooling1D\u001B[0m)        ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m4\u001B[0m, \u001B[38;5;34m64\u001B[0m)          ‚îÇ             \u001B[38;5;34m0\u001B[0m ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ conv1d_5 (\u001B[38;5;33mConv1D\u001B[0m)               ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m4\u001B[0m, \u001B[38;5;34m64\u001B[0m)          ‚îÇ         \u001B[38;5;34m8,256\u001B[0m ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ maxpool_5 (\u001B[38;5;33mMaxPooling1D\u001B[0m)        ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2\u001B[0m, \u001B[38;5;34m64\u001B[0m)          ‚îÇ             \u001B[38;5;34m0\u001B[0m ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ batch_norm (\u001B[38;5;33mBatchNormalization\u001B[0m) ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2\u001B[0m, \u001B[38;5;34m64\u001B[0m)          ‚îÇ           \u001B[38;5;34m256\u001B[0m ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dropout (\u001B[38;5;33mDropout\u001B[0m)               ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2\u001B[0m, \u001B[38;5;34m64\u001B[0m)          ‚îÇ             \u001B[38;5;34m0\u001B[0m ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ flatten (\u001B[38;5;33mFlatten\u001B[0m)               ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            ‚îÇ             \u001B[38;5;34m0\u001B[0m ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ output (\u001B[38;5;33mDense\u001B[0m)                  ‚îÇ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              ‚îÇ           \u001B[38;5;34m129\u001B[0m ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         ‚îÇ            <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ maxpool_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ maxpool_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ maxpool_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ maxpool_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          ‚îÇ         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ maxpool_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ batch_norm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>) ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m23,233\u001B[0m (90.75 KB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,233</span> (90.75 KB)\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m23,105\u001B[0m (90.25 KB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,105</span> (90.25 KB)\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m128\u001B[0m (512.00 B)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n</pre>\n"
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nüìå ƒêANG C·∫§U H√åNH CALLBACKS...\n   ‚úÖ EarlyStopping: patience=10\n   ‚úÖ ModelCheckpoint: /kaggle/working/models/best_model.keras\n   ‚úÖ ReduceLROnPlateau: factor=0.5, patience=5\n   ‚úÖ TensorBoard: /kaggle/working/logs/20251228-121244\n\n================================================================================\nüöÄ B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN M√î H√åNH\n================================================================================\n   Epochs: 50\n   Batch size: 256\n   Learning rate: 0.001\n   Class weights: C√≥\nEpoch 1/50\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - accuracy: 0.9208 - loss: 0.2457 - precision: 0.8701 - recall: 0.8715\nEpoch 1: val_loss improved from inf to 0.14378, saving model to /kaggle/working/models/best_model.keras\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m55s\u001B[0m 6ms/step - accuracy: 0.9208 - loss: 0.2457 - precision: 0.8701 - recall: 0.8715 - val_accuracy: 0.9604 - val_loss: 0.1438 - val_precision: 0.9612 - val_recall: 0.9046 - learning_rate: 0.0010\nEpoch 2/50\n\u001B[1m8203/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9619 - loss: 0.1650 - precision: 0.9625 - recall: 0.9085\nEpoch 2: val_loss improved from 0.14378 to 0.12872, saving model to /kaggle/working/models/best_model.keras\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m47s\u001B[0m 6ms/step - accuracy: 0.9619 - loss: 0.1650 - precision: 0.9625 - recall: 0.9085 - val_accuracy: 0.9652 - val_loss: 0.1287 - val_precision: 0.9747 - val_recall: 0.9076 - learning_rate: 0.0010\nEpoch 3/50\n\u001B[1m8200/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9660 - loss: 0.1524 - precision: 0.9739 - recall: 0.9111\nEpoch 3: val_loss improved from 0.12872 to 0.12308, saving model to /kaggle/working/models/best_model.keras\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m45s\u001B[0m 6ms/step - accuracy: 0.9660 - loss: 0.1524 - precision: 0.9739 - recall: 0.9111 - val_accuracy: 0.9667 - val_loss: 0.1231 - val_precision: 0.9853 - val_recall: 0.9026 - learning_rate: 0.0010\nEpoch 4/50\n\u001B[1m8200/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9678 - loss: 0.1468 - precision: 0.9785 - recall: 0.9126\nEpoch 4: val_loss did not improve from 0.12308\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m44s\u001B[0m 5ms/step - accuracy: 0.9678 - loss: 0.1468 - precision: 0.9785 - recall: 0.9126 - val_accuracy: 0.9652 - val_loss: 0.1257 - val_precision: 0.9895 - val_recall: 0.8935 - learning_rate: 0.0010\nEpoch 5/50\n\u001B[1m8203/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9687 - loss: 0.1440 - precision: 0.9805 - recall: 0.9138\nEpoch 5: val_loss did not improve from 0.12308\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m45s\u001B[0m 5ms/step - accuracy: 0.9687 - loss: 0.1440 - precision: 0.9805 - recall: 0.9138 - val_accuracy: 0.9686 - val_loss: 0.1238 - val_precision: 0.9907 - val_recall: 0.9037 - learning_rate: 0.0010\nEpoch 6/50\n\u001B[1m8201/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9691 - loss: 0.1421 - precision: 0.9814 - recall: 0.9142\nEpoch 6: val_loss did not improve from 0.12308\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m44s\u001B[0m 5ms/step - accuracy: 0.9691 - loss: 0.1421 - precision: 0.9814 - recall: 0.9142 - val_accuracy: 0.9661 - val_loss: 0.1389 - val_precision: 0.9678 - val_recall: 0.9174 - learning_rate: 0.0010\nEpoch 7/50\n\u001B[1m8199/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9697 - loss: 0.1401 - precision: 0.9829 - recall: 0.9150\nEpoch 7: val_loss improved from 0.12308 to 0.11348, saving model to /kaggle/working/models/best_model.keras\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m43s\u001B[0m 5ms/step - accuracy: 0.9697 - loss: 0.1401 - precision: 0.9829 - recall: 0.9150 - val_accuracy: 0.9715 - val_loss: 0.1135 - val_precision: 0.9892 - val_recall: 0.9150 - learning_rate: 0.0010\nEpoch 8/50\n\u001B[1m8194/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9700 - loss: 0.1391 - precision: 0.9837 - recall: 0.9153\nEpoch 8: val_loss did not improve from 0.11348\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m42s\u001B[0m 5ms/step - accuracy: 0.9700 - loss: 0.1391 - precision: 0.9837 - recall: 0.9153 - val_accuracy: 0.9719 - val_loss: 0.1142 - val_precision: 0.9890 - val_recall: 0.9165 - learning_rate: 0.0010\nEpoch 9/50\n\u001B[1m8203/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9702 - loss: 0.1388 - precision: 0.9841 - recall: 0.9154\nEpoch 9: val_loss did not improve from 0.11348\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m41s\u001B[0m 5ms/step - accuracy: 0.9702 - loss: 0.1388 - precision: 0.9841 - recall: 0.9154 - val_accuracy: 0.9659 - val_loss: 0.1213 - val_precision: 0.9911 - val_recall: 0.8943 - learning_rate: 0.0010\nEpoch 10/50\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9704 - loss: 0.1378 - precision: 0.9846 - recall: 0.9157\nEpoch 10: val_loss improved from 0.11348 to 0.10998, saving model to /kaggle/working/models/best_model.keras\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m41s\u001B[0m 5ms/step - accuracy: 0.9704 - loss: 0.1378 - precision: 0.9846 - recall: 0.9157 - val_accuracy: 0.9725 - val_loss: 0.1100 - val_precision: 0.9931 - val_recall: 0.9145 - learning_rate: 0.0010\nEpoch 11/50\n\u001B[1m8195/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9709 - loss: 0.1363 - precision: 0.9856 - recall: 0.9163\nEpoch 11: val_loss improved from 0.10998 to 0.10765, saving model to /kaggle/working/models/best_model.keras\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 5ms/step - accuracy: 0.9709 - loss: 0.1363 - precision: 0.9856 - recall: 0.9163 - val_accuracy: 0.9729 - val_loss: 0.1077 - val_precision: 0.9943 - val_recall: 0.9147 - learning_rate: 0.0010\nEpoch 12/50\n\u001B[1m8194/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9709 - loss: 0.1359 - precision: 0.9856 - recall: 0.9164\nEpoch 12: val_loss did not improve from 0.10765\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 5ms/step - accuracy: 0.9709 - loss: 0.1359 - precision: 0.9856 - recall: 0.9164 - val_accuracy: 0.9727 - val_loss: 0.1094 - val_precision: 0.9950 - val_recall: 0.9135 - learning_rate: 0.0010\nEpoch 13/50\n\u001B[1m8195/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.9711 - loss: 0.1353 - precision: 0.9859 - recall: 0.9166\nEpoch 13: val_loss improved from 0.10765 to 0.10705, saving model to /kaggle/working/models/best_model.keras\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 5ms/step - accuracy: 0.9711 - loss: 0.1353 - precision: 0.9859 - recall: 0.9166 - val_accuracy: 0.9725 - val_loss: 0.1071 - val_precision: 0.9923 - val_recall: 0.9155 - learning_rate: 0.0010\nEpoch 14/50\n\u001B[1m8203/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9712 - loss: 0.1349 - precision: 0.9864 - recall: 0.9167\nEpoch 14: val_loss did not improve from 0.10705\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 5ms/step - accuracy: 0.9712 - loss: 0.1349 - precision: 0.9864 - recall: 0.9167 - val_accuracy: 0.9722 - val_loss: 0.1099 - val_precision: 0.9946 - val_recall: 0.9123 - learning_rate: 0.0010\nEpoch 15/50\n\u001B[1m8195/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.9713 - loss: 0.1343 - precision: 0.9864 - recall: 0.9169\nEpoch 15: val_loss did not improve from 0.10705\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 5ms/step - accuracy: 0.9713 - loss: 0.1343 - precision: 0.9864 - recall: 0.9169 - val_accuracy: 0.9724 - val_loss: 0.1094 - val_precision: 0.9911 - val_recall: 0.9161 - learning_rate: 0.0010\nEpoch 16/50\n\u001B[1m8197/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.9714 - loss: 0.1340 - precision: 0.9864 - recall: 0.9172\nEpoch 16: val_loss did not improve from 0.10705\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 5ms/step - accuracy: 0.9714 - loss: 0.1340 - precision: 0.9864 - recall: 0.9172 - val_accuracy: 0.9725 - val_loss: 0.1077 - val_precision: 0.9925 - val_recall: 0.9155 - learning_rate: 0.0010\nEpoch 17/50\n\u001B[1m8195/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.9714 - loss: 0.1339 - precision: 0.9866 - recall: 0.9171\nEpoch 17: val_loss did not improve from 0.10705\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 5ms/step - accuracy: 0.9714 - loss: 0.1339 - precision: 0.9866 - recall: 0.9171 - val_accuracy: 0.9727 - val_loss: 0.1087 - val_precision: 0.9934 - val_recall: 0.9152 - learning_rate: 0.0010\nEpoch 18/50\n\u001B[1m8200/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9713 - loss: 0.1336 - precision: 0.9865 - recall: 0.9171\nEpoch 18: val_loss did not improve from 0.10705\n\nEpoch 18: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 5ms/step - accuracy: 0.9713 - loss: 0.1336 - precision: 0.9865 - recall: 0.9171 - val_accuracy: 0.9729 - val_loss: 0.1072 - val_precision: 0.9956 - val_recall: 0.9136 - learning_rate: 0.0010\nEpoch 19/50\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9721 - loss: 0.1310 - precision: 0.9881 - recall: 0.9180\nEpoch 19: val_loss improved from 0.10705 to 0.10415, saving model to /kaggle/working/models/best_model.keras\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m42s\u001B[0m 5ms/step - accuracy: 0.9721 - loss: 0.1310 - precision: 0.9881 - recall: 0.9180 - val_accuracy: 0.9735 - val_loss: 0.1041 - val_precision: 0.9953 - val_recall: 0.9161 - learning_rate: 5.0000e-04\nEpoch 20/50\n\u001B[1m8195/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9722 - loss: 0.1305 - precision: 0.9880 - recall: 0.9183\nEpoch 20: val_loss improved from 0.10415 to 0.10358, saving model to /kaggle/working/models/best_model.keras\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m42s\u001B[0m 5ms/step - accuracy: 0.9722 - loss: 0.1305 - precision: 0.9880 - recall: 0.9183 - val_accuracy: 0.9734 - val_loss: 0.1036 - val_precision: 0.9948 - val_recall: 0.9162 - learning_rate: 5.0000e-04\nEpoch 21/50\n\u001B[1m8194/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9723 - loss: 0.1301 - precision: 0.9880 - recall: 0.9187\nEpoch 21: val_loss improved from 0.10358 to 0.10318, saving model to /kaggle/working/models/best_model.keras\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 5ms/step - accuracy: 0.9723 - loss: 0.1301 - precision: 0.9880 - recall: 0.9187 - val_accuracy: 0.9735 - val_loss: 0.1032 - val_precision: 0.9948 - val_recall: 0.9165 - learning_rate: 5.0000e-04\nEpoch 22/50\n\u001B[1m8199/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9722 - loss: 0.1300 - precision: 0.9878 - recall: 0.9187\nEpoch 22: val_loss improved from 0.10318 to 0.10256, saving model to /kaggle/working/models/best_model.keras\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m41s\u001B[0m 5ms/step - accuracy: 0.9722 - loss: 0.1300 - precision: 0.9878 - recall: 0.9187 - val_accuracy: 0.9736 - val_loss: 0.1026 - val_precision: 0.9959 - val_recall: 0.9157 - learning_rate: 5.0000e-04\nEpoch 23/50\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.9722 - loss: 0.1300 - precision: 0.9879 - recall: 0.9186\nEpoch 23: val_loss did not improve from 0.10256\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 5ms/step - accuracy: 0.9722 - loss: 0.1300 - precision: 0.9879 - recall: 0.9186 - val_accuracy: 0.9734 - val_loss: 0.1039 - val_precision: 0.9953 - val_recall: 0.9157 - learning_rate: 5.0000e-04\nEpoch 24/50\n\u001B[1m8195/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9723 - loss: 0.1296 - precision: 0.9880 - recall: 0.9189\nEpoch 24: val_loss did not improve from 0.10256\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m41s\u001B[0m 5ms/step - accuracy: 0.9723 - loss: 0.1296 - precision: 0.9880 - recall: 0.9189 - val_accuracy: 0.9732 - val_loss: 0.1044 - val_precision: 0.9960 - val_recall: 0.9142 - learning_rate: 5.0000e-04\nEpoch 25/50\n\u001B[1m8200/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9722 - loss: 0.1297 - precision: 0.9875 - recall: 0.9189\nEpoch 25: val_loss did not improve from 0.10256\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 5ms/step - accuracy: 0.9722 - loss: 0.1297 - precision: 0.9875 - recall: 0.9189 - val_accuracy: 0.9735 - val_loss: 0.1027 - val_precision: 0.9960 - val_recall: 0.9155 - learning_rate: 5.0000e-04\nEpoch 26/50\n\u001B[1m8203/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9723 - loss: 0.1292 - precision: 0.9876 - recall: 0.9191\nEpoch 26: val_loss improved from 0.10256 to 0.10242, saving model to /kaggle/working/models/best_model.keras\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m41s\u001B[0m 5ms/step - accuracy: 0.9723 - loss: 0.1292 - precision: 0.9876 - recall: 0.9191 - val_accuracy: 0.9734 - val_loss: 0.1024 - val_precision: 0.9962 - val_recall: 0.9150 - learning_rate: 5.0000e-04\nEpoch 27/50\n\u001B[1m8203/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.9723 - loss: 0.1294 - precision: 0.9876 - recall: 0.9191\nEpoch 27: val_loss did not improve from 0.10242\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 5ms/step - accuracy: 0.9723 - loss: 0.1294 - precision: 0.9876 - recall: 0.9191 - val_accuracy: 0.9735 - val_loss: 0.1035 - val_precision: 0.9961 - val_recall: 0.9153 - learning_rate: 5.0000e-04\nEpoch 28/50\n\u001B[1m8193/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9722 - loss: 0.1291 - precision: 0.9876 - recall: 0.9189\nEpoch 28: val_loss did not improve from 0.10242\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 5ms/step - accuracy: 0.9722 - loss: 0.1291 - precision: 0.9876 - recall: 0.9189 - val_accuracy: 0.9736 - val_loss: 0.1027 - val_precision: 0.9959 - val_recall: 0.9157 - learning_rate: 5.0000e-04\nEpoch 29/50\n\u001B[1m8198/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.9723 - loss: 0.1290 - precision: 0.9876 - recall: 0.9192\nEpoch 29: val_loss did not improve from 0.10242\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 5ms/step - accuracy: 0.9723 - loss: 0.1290 - precision: 0.9876 - recall: 0.9192 - val_accuracy: 0.9735 - val_loss: 0.1025 - val_precision: 0.9959 - val_recall: 0.9155 - learning_rate: 5.0000e-04\nEpoch 30/50\n\u001B[1m8197/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9723 - loss: 0.1289 - precision: 0.9878 - recall: 0.9191\nEpoch 30: val_loss did not improve from 0.10242\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 5ms/step - accuracy: 0.9723 - loss: 0.1289 - precision: 0.9878 - recall: 0.9191 - val_accuracy: 0.9736 - val_loss: 0.1038 - val_precision: 0.9957 - val_recall: 0.9158 - learning_rate: 5.0000e-04\nEpoch 31/50\n\u001B[1m8196/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.9723 - loss: 0.1290 - precision: 0.9877 - recall: 0.9192\nEpoch 31: val_loss did not improve from 0.10242\n\nEpoch 31: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 5ms/step - accuracy: 0.9723 - loss: 0.1290 - precision: 0.9877 - recall: 0.9192 - val_accuracy: 0.9733 - val_loss: 0.1049 - val_precision: 0.9966 - val_recall: 0.9139 - learning_rate: 5.0000e-04\nEpoch 32/50\n\u001B[1m8194/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9725 - loss: 0.1277 - precision: 0.9878 - recall: 0.9197\nEpoch 32: val_loss improved from 0.10242 to 0.10139, saving model to /kaggle/working/models/best_model.keras\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m42s\u001B[0m 5ms/step - accuracy: 0.9725 - loss: 0.1277 - precision: 0.9878 - recall: 0.9197 - val_accuracy: 0.9737 - val_loss: 0.1014 - val_precision: 0.9964 - val_recall: 0.9156 - learning_rate: 2.5000e-04\nEpoch 33/50\n\u001B[1m8194/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9725 - loss: 0.1273 - precision: 0.9876 - recall: 0.9200\nEpoch 33: val_loss did not improve from 0.10139\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m42s\u001B[0m 5ms/step - accuracy: 0.9725 - loss: 0.1273 - precision: 0.9876 - recall: 0.9200 - val_accuracy: 0.9739 - val_loss: 0.1017 - val_precision: 0.9964 - val_recall: 0.9161 - learning_rate: 2.5000e-04\nEpoch 34/50\n\u001B[1m8199/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9725 - loss: 0.1273 - precision: 0.9876 - recall: 0.9199\nEpoch 34: val_loss improved from 0.10139 to 0.10067, saving model to /kaggle/working/models/best_model.keras\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m41s\u001B[0m 5ms/step - accuracy: 0.9725 - loss: 0.1273 - precision: 0.9876 - recall: 0.9199 - val_accuracy: 0.9738 - val_loss: 0.1007 - val_precision: 0.9964 - val_recall: 0.9158 - learning_rate: 2.5000e-04\nEpoch 35/50\n\u001B[1m8193/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9725 - loss: 0.1271 - precision: 0.9873 - recall: 0.9201\nEpoch 35: val_loss did not improve from 0.10067\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m42s\u001B[0m 5ms/step - accuracy: 0.9725 - loss: 0.1271 - precision: 0.9873 - recall: 0.9201 - val_accuracy: 0.9739 - val_loss: 0.1012 - val_precision: 0.9963 - val_recall: 0.9163 - learning_rate: 2.5000e-04\nEpoch 36/50\n\u001B[1m8203/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9725 - loss: 0.1268 - precision: 0.9872 - recall: 0.9201\nEpoch 36: val_loss did not improve from 0.10067\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m41s\u001B[0m 5ms/step - accuracy: 0.9725 - loss: 0.1268 - precision: 0.9872 - recall: 0.9201 - val_accuracy: 0.9737 - val_loss: 0.1015 - val_precision: 0.9967 - val_recall: 0.9153 - learning_rate: 2.5000e-04\nEpoch 37/50\n\u001B[1m8196/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9725 - loss: 0.1270 - precision: 0.9871 - recall: 0.9202\nEpoch 37: val_loss did not improve from 0.10067\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 5ms/step - accuracy: 0.9725 - loss: 0.1270 - precision: 0.9871 - recall: 0.9202 - val_accuracy: 0.9737 - val_loss: 0.1013 - val_precision: 0.9965 - val_recall: 0.9157 - learning_rate: 2.5000e-04\nEpoch 38/50\n\u001B[1m8193/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.9725 - loss: 0.1270 - precision: 0.9875 - recall: 0.9200\nEpoch 38: val_loss did not improve from 0.10067\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 5ms/step - accuracy: 0.9725 - loss: 0.1270 - precision: 0.9875 - recall: 0.9200 - val_accuracy: 0.9738 - val_loss: 0.1020 - val_precision: 0.9965 - val_recall: 0.9159 - learning_rate: 2.5000e-04\nEpoch 39/50\n\u001B[1m8194/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9726 - loss: 0.1269 - precision: 0.9876 - recall: 0.9203\nEpoch 39: val_loss did not improve from 0.10067\n\nEpoch 39: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 5ms/step - accuracy: 0.9726 - loss: 0.1269 - precision: 0.9876 - recall: 0.9203 - val_accuracy: 0.9735 - val_loss: 0.1022 - val_precision: 0.9970 - val_recall: 0.9146 - learning_rate: 2.5000e-04\nEpoch 40/50\n\u001B[1m8203/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9727 - loss: 0.1262 - precision: 0.9878 - recall: 0.9204\nEpoch 40: val_loss improved from 0.10067 to 0.10063, saving model to /kaggle/working/models/best_model.keras\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m41s\u001B[0m 5ms/step - accuracy: 0.9727 - loss: 0.1262 - precision: 0.9878 - recall: 0.9204 - val_accuracy: 0.9739 - val_loss: 0.1006 - val_precision: 0.9967 - val_recall: 0.9160 - learning_rate: 1.2500e-04\nEpoch 41/50\n\u001B[1m8199/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9727 - loss: 0.1258 - precision: 0.9873 - recall: 0.9207\nEpoch 41: val_loss did not improve from 0.10063\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m41s\u001B[0m 5ms/step - accuracy: 0.9727 - loss: 0.1258 - precision: 0.9873 - recall: 0.9207 - val_accuracy: 0.9737 - val_loss: 0.1014 - val_precision: 0.9966 - val_recall: 0.9156 - learning_rate: 1.2500e-04\nEpoch 42/50\n\u001B[1m8197/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9726 - loss: 0.1258 - precision: 0.9875 - recall: 0.9204\nEpoch 42: val_loss did not improve from 0.10063\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 5ms/step - accuracy: 0.9726 - loss: 0.1258 - precision: 0.9875 - recall: 0.9204 - val_accuracy: 0.9737 - val_loss: 0.1014 - val_precision: 0.9968 - val_recall: 0.9151 - learning_rate: 1.2500e-04\nEpoch 43/50\n\u001B[1m8200/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9727 - loss: 0.1256 - precision: 0.9873 - recall: 0.9209\nEpoch 43: val_loss did not improve from 0.10063\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m41s\u001B[0m 5ms/step - accuracy: 0.9727 - loss: 0.1256 - precision: 0.9873 - recall: 0.9209 - val_accuracy: 0.9737 - val_loss: 0.1010 - val_precision: 0.9969 - val_recall: 0.9151 - learning_rate: 1.2500e-04\nEpoch 44/50\n\u001B[1m8203/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.9726 - loss: 0.1256 - precision: 0.9871 - recall: 0.9206\nEpoch 44: val_loss did not improve from 0.10063\n\nEpoch 44: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 5ms/step - accuracy: 0.9726 - loss: 0.1256 - precision: 0.9871 - recall: 0.9206 - val_accuracy: 0.9737 - val_loss: 0.1009 - val_precision: 0.9970 - val_recall: 0.9152 - learning_rate: 1.2500e-04\nEpoch 45/50\n\u001B[1m8194/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9727 - loss: 0.1253 - precision: 0.9873 - recall: 0.9208\nEpoch 45: val_loss improved from 0.10063 to 0.10020, saving model to /kaggle/working/models/best_model.keras\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 5ms/step - accuracy: 0.9727 - loss: 0.1253 - precision: 0.9873 - recall: 0.9208 - val_accuracy: 0.9739 - val_loss: 0.1002 - val_precision: 0.9967 - val_recall: 0.9159 - learning_rate: 6.2500e-05\nEpoch 46/50\n\u001B[1m8203/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9727 - loss: 0.1250 - precision: 0.9874 - recall: 0.9209\nEpoch 46: val_loss improved from 0.10020 to 0.10013, saving model to /kaggle/working/models/best_model.keras\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 5ms/step - accuracy: 0.9727 - loss: 0.1250 - precision: 0.9874 - recall: 0.9209 - val_accuracy: 0.9740 - val_loss: 0.1001 - val_precision: 0.9975 - val_recall: 0.9157 - learning_rate: 6.2500e-05\nEpoch 47/50\n\u001B[1m8198/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.9727 - loss: 0.1251 - precision: 0.9872 - recall: 0.9211\nEpoch 47: val_loss improved from 0.10013 to 0.10002, saving model to /kaggle/working/models/best_model.keras\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 5ms/step - accuracy: 0.9727 - loss: 0.1251 - precision: 0.9872 - recall: 0.9211 - val_accuracy: 0.9741 - val_loss: 0.1000 - val_precision: 0.9975 - val_recall: 0.9158 - learning_rate: 6.2500e-05\nEpoch 48/50\n\u001B[1m8201/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - accuracy: 0.9728 - loss: 0.1250 - precision: 0.9874 - recall: 0.9211\nEpoch 48: val_loss improved from 0.10002 to 0.09981, saving model to /kaggle/working/models/best_model.keras\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 5ms/step - accuracy: 0.9728 - loss: 0.1250 - precision: 0.9874 - recall: 0.9211 - val_accuracy: 0.9739 - val_loss: 0.0998 - val_precision: 0.9966 - val_recall: 0.9160 - learning_rate: 6.2500e-05\nEpoch 49/50\n\u001B[1m8193/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.9728 - loss: 0.1250 - precision: 0.9873 - recall: 0.9210\nEpoch 49: val_loss improved from 0.09981 to 0.09955, saving model to /kaggle/working/models/best_model.keras\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 5ms/step - accuracy: 0.9728 - loss: 0.1250 - precision: 0.9873 - recall: 0.9210 - val_accuracy: 0.9739 - val_loss: 0.0995 - val_precision: 0.9969 - val_recall: 0.9157 - learning_rate: 6.2500e-05\nEpoch 50/50\n\u001B[1m8198/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m‚îÅ\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - accuracy: 0.9728 - loss: 0.1250 - precision: 0.9875 - recall: 0.9211\nEpoch 50: val_loss did not improve from 0.09955\n\u001B[1m8204/8204\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 5ms/step - accuracy: 0.9728 - loss: 0.1250 - precision: 0.9875 - recall: 0.9211 - val_accuracy: 0.9737 - val_loss: 0.1002 - val_precision: 0.9970 - val_recall: 0.9152 - learning_rate: 6.2500e-05\nRestoring model weights from the end of the best epoch: 49.\n\n   ‚è±Ô∏è Th·ªùi gian training: 34.39 ph√∫t\n   üìà Best val_loss: 0.0995\n   üìà Best val_accuracy: 0.9741\n\n================================================================================\nüìä ƒê√ÅNH GI√Å M√î H√åNH TR√äN TEST SET\n================================================================================\n\u001B[1m18750/18750\u001B[0m \u001B[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m\u001B[37m\u001B[0m \u001B[1m43s\u001B[0m 2ms/step - accuracy: 0.9732 - loss: 0.1009 - precision: 0.9968 - recall: 0.9136\n\n   üìä K·∫æT QU·∫¢:\n   ========================================\n   Loss:      0.1005\n   Accuracy:  0.9734 (97.34%)\n   Precision: 0.9967\n   Recall:    0.9143\n   F1-Score:  0.9537\n   ========================================\n\n   üìã CONFUSION MATRIX:\n                 Predicted\n                 Benign  Attack\n   Actual Benign  419453     547\n   Actual Attack   15428  164572\n\n   üìã CLASSIFICATION REPORT:\n              precision    recall  f1-score   support\n\n      Benign       0.96      1.00      0.98    420000\n      Attack       1.00      0.91      0.95    180000\n\n    accuracy                           0.97    600000\n   macro avg       0.98      0.96      0.97    600000\nweighted avg       0.97      0.97      0.97    600000\n\n\n================================================================================\nüíæ ƒêANG L∆ØU MODEL V√Ä K·∫æT QU·∫¢...\n================================================================================\n   ‚úÖ Final model: /kaggle/working/models/final_model.keras\n   ‚úÖ Model weights: /kaggle/working/models/model_weights.weights.h5\n   ‚úÖ Training history: training_history.json\n   ‚úÖ Evaluation results: evaluation_results.json\n   ‚úÖ Predictions: y_pred.npy\n   ‚úÖ Prediction probabilities: y_pred_prob.npy\n   ‚úÖ Training config: training_config.json\n\nüìÅ T·∫•t c·∫£ file ƒë∆∞·ª£c l∆∞u t·∫°i: /kaggle/working/models\n   ‚úÖ Training history plot: training_history.png\n   ‚úÖ Confusion matrix plot: confusion_matrix.png\n   ‚úÖ Normalized confusion matrix plot: confusion_matrix_normalized.png\n\n================================================================================\n‚úÖ HO√ÄN TH√ÄNH HU·∫§N LUY·ªÜN!\n   Test Accuracy:  97.34%\n   Test Precision: 99.67%\n   Test Recall:    91.43%\n   Test F1-Score:  95.37%\n================================================================================\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "!rm -rf /kaggle/working/models",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-28T12:08:38.588168Z",
     "iopub.execute_input": "2025-12-28T12:08:38.588875Z",
     "iopub.status.idle": "2025-12-28T12:08:38.878153Z",
     "shell.execute_reply.started": "2025-12-28T12:08:38.588842Z",
     "shell.execute_reply": "2025-12-28T12:08:38.877081Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\n======================================================================================\nTI·ªÄN X·ª¨ L√ù DATASET CICIDS2018 CHO M√î H√åNH CNN - PH√ÅT HI·ªÜN L∆ØU L∆Ø·ª¢NG M·∫†NG IOT B·∫§T TH∆Ø·ªúNG\n======================================================================================\n\nScript n√†y th·ª±c hi·ªán c√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu:\n1. ƒê·ªçc t·ª´ng file CSV theo chunks ƒë·ªÉ t·ªëi ∆∞u b·ªô nh·ªõ\n2. Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn thi·∫øt (IP, Port, Timestamp, Flow ID)\n3. X·ª≠ l√Ω missing values, NaN, Inf\n4. Lo·∫°i b·ªè c√°c h√†ng tr√πng l·∫∑p\n5. Chuy·ªÉn ƒë·ªïi nh√£n sang d·∫°ng binary (Benign=0, Attack=1)\n6. Chu·∫©n h√≥a d·ªØ li·ªáu b·∫±ng StandardScaler\n7. L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω sang ƒë·ªãnh d·∫°ng nhanh (parquet/npy)\n\nC√≥ th·ªÉ ch·∫°y tr√™n c·∫£ Kaggle v√† Local\n\"\"\"\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport json\nimport gc\nfrom pathlib import Path\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ============================================================================\n# TH∆Ø VI·ªÜN CHU·∫®N H√ìA V√Ä X·ª¨ L√ù D·ªÆ LI·ªÜU\n# ============================================================================\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\n# Ki·ªÉm tra m√¥i tr∆∞·ªùng ch·∫°y (Kaggle ho·∫∑c Local)\nIS_KAGGLE = os.path.exists('/kaggle/input')\n\n# Progress bar\ntry:\n    from tqdm import tqdm\n    TQDM_AVAILABLE = True\nexcept ImportError:\n    TQDM_AVAILABLE = False\n    print(\"‚ö†Ô∏è  tqdm kh√¥ng c√≥ s·∫µn. C√†i ƒë·∫∑t b·∫±ng: pip install tqdm\")\n    tqdm = lambda x, **kwargs: x\n\n# ============================================================================\n# C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N\n# ============================================================================\nif IS_KAGGLE:\n    # ƒê∆∞·ªùng d·∫´n tr√™n Kaggle - thay ƒë·ªïi theo dataset c·ªßa b·∫°n\n    DATA_DIR = \"/kaggle/input/cicids2018\"  # Thay ƒë·ªïi n·∫øu t√™n dataset kh√°c\n    OUTPUT_DIR = \"/kaggle/working/processed_data_cnn\"\n    print(\" ƒêang ch·∫°y tr√™n KAGGLE\")\nelse:\n    # ƒê∆∞·ªùng d·∫´n Local\n    DATA_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CICIDS2018-CSV\"\n    OUTPUT_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CNN\\processed_data_cnn\"\n    print(\" ƒêang ch·∫°y tr√™n LOCAL\")\n\n# ============================================================================\n# C·∫§U H√åNH X·ª¨ L√ù D·ªÆ LI·ªÜU\n# ============================================================================\n\n# K√≠ch th∆∞·ªõc chunk khi ƒë·ªçc CSV (ƒëi·ªÅu ch·ªânh theo RAM c·ªßa m√°y)\nCHUNK_SIZE = 300000  # 300k rows m·ªói chunk\n\n# Random state ƒë·ªÉ t√°i t·∫°o k·∫øt qu·∫£\nRANDOM_STATE = 42\n\n# Lo·∫°i scaler: 'standard' (StandardScaler) ho·∫∑c 'minmax' (MinMaxScaler)\nSCALER_TYPE = 'standard'\n\n# ============================================================================\n# C·∫§U H√åNH SAMPLE C√ÇN B·∫∞NG\n# ============================================================================\n# T·ªïng s·ªë m·∫´u mong mu·ªën (train + val + test)\nTOTAL_SAMPLES = 3500000  # 3 tri·ªáu m·∫´u\n\n# T·ª∑ l·ªá ph·∫ßn trƒÉm cho m·ªói class\nBENIGN_RATIO = 0.70  # 70% Benign = 2,100,000 m·∫´u\nATTACK_RATIO = 0.30  # 30% Attack = 900,000 m·∫´u\n\n# T√≠nh s·ªë l∆∞·ª£ng m·∫´u cho m·ªói class\nTARGET_BENIGN = int(TOTAL_SAMPLES * BENIGN_RATIO)  # 2,100,000\nTARGET_ATTACK = int(TOTAL_SAMPLES * ATTACK_RATIO)  # 900,000\n\n# ============================================================================\n# DANH S√ÅCH C√ÅC C·ªòT C·∫¶N LO·∫†I B·ªé\n# ============================================================================\n\n# C√°c c·ªôt kh√¥ng c·∫ßn thi·∫øt cho vi·ªác hu·∫•n luy·ªán CNN\nCOLUMNS_TO_DROP = [\n    # Th√¥ng tin ƒë·ªãnh danh - kh√¥ng mang t√≠nh t·ªïng qu√°t\n    'Flow ID',          # ID duy nh·∫•t cho m·ªói flow\n    'Src IP',           # IP ngu·ªìn\n    'Dst IP',           # IP ƒë√≠ch\n    'Src Port',         # Port ngu·ªìn\n    'Timestamp',        # Th·ªùi gian - kh√¥ng li√™n quan ƒë·∫øn pattern\n\n    # C√°c c·ªôt flag kh√¥ng mang nhi·ªÅu th√¥ng tin\n    # 'Bwd PSH Flags',    # Hi·∫øm khi c√≥ gi√° tr·ªã kh√°c 0\n    # 'Bwd URG Flags',    # Hi·∫øm khi c√≥ gi√° tr·ªã kh√°c 0\n    # 'Fwd URG Flags',    # Hi·∫øm khi c√≥ gi√° tr·ªã kh√°c 0\n]\n\n# C·ªôt nh√£n\nLABEL_COLUMN = 'Label'\n\n# ============================================================================\n# CLASS X·ª¨ L√ù D·ªÆ LI·ªÜU CHO CNN\n# ============================================================================\n\nclass CICIDS2018_CNN_Preprocessor:\n    \"\"\"\n    Class x·ª≠ l√Ω d·ªØ li·ªáu CICIDS2018 cho m√¥ h√¨nh CNN ph√°t hi·ªán b·∫•t th∆∞·ªùng\n\n    C√°c b∆∞·ªõc x·ª≠ l√Ω:\n    1. ƒê·ªçc d·ªØ li·ªáu theo chunks\n    2. Lo·∫°i b·ªè c·ªôt kh√¥ng c·∫ßn thi·∫øt\n    3. X·ª≠ l√Ω gi√° tr·ªã thi·∫øu, NaN, Inf\n    4. Lo·∫°i b·ªè duplicate\n    5. Chuy·ªÉn ƒë·ªïi nh√£n sang binary\n    6. Chu·∫©n h√≥a features\n    7. L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω\n    \"\"\"\n\n    def __init__(self, data_dir, output_dir, chunk_size=CHUNK_SIZE,\n                 scaler_type=SCALER_TYPE, target_benign=TARGET_BENIGN,\n                 target_attack=TARGET_ATTACK):\n        \"\"\"\n        Kh·ªüi t·∫°o preprocessor\n\n        Args:\n            data_dir: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a file CSV\n            output_dir: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£\n            chunk_size: S·ªë d√≤ng m·ªói chunk khi ƒë·ªçc CSV\n            scaler_type: Lo·∫°i scaler ('standard' ho·∫∑c 'minmax')\n            target_benign: S·ªë l∆∞·ª£ng m·∫´u Benign mong mu·ªën\n            target_attack: S·ªë l∆∞·ª£ng m·∫´u Attack mong mu·ªën\n        \"\"\"\n        self.data_dir = Path(data_dir)\n        self.output_dir = Path(output_dir)\n        self.chunk_size = chunk_size\n        self.scaler_type = scaler_type\n        self.target_benign = target_benign\n        self.target_attack = target_attack\n\n        # Kh·ªüi t·∫°o scaler\n        if scaler_type == 'minmax':\n            self.scaler = MinMaxScaler()\n        else:\n            self.scaler = StandardScaler()\n\n        # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n        # Th·ªëng k√™\n        self.stats = {\n            'total_rows_read': 0,\n            'rows_after_cleaning': 0,\n            'duplicates_removed': 0,\n            'nan_inf_replaced': 0,\n            'benign_count': 0,\n            'attack_count': 0,\n            'feature_count': 0,\n            'processing_time': 0\n        }\n\n        # L∆∞u t√™n c√°c features\n        self.feature_names = None\n\n    def _get_csv_files(self):\n        \"\"\"L·∫•y danh s√°ch c√°c file CSV trong th∆∞ m·ª•c data\"\"\"\n        csv_files = list(self.data_dir.glob(\"*_TrafficForML_CICFlowMeter.csv\"))\n        if not csv_files:\n            # Th·ª≠ pattern kh√°c cho Kaggle\n            csv_files = list(self.data_dir.glob(\"*.csv\"))\n\n        if not csv_files:\n            raise FileNotFoundError(f\"Kh√¥ng t√¨m th·∫•y file CSV trong {self.data_dir}\")\n\n        print(f\"\\nüìÇ T√¨m th·∫•y {len(csv_files)} file CSV:\")\n        for f in sorted(csv_files):\n            print(f\"   - {f.name}\")\n        return sorted(csv_files)\n\n    def _clean_column_names(self, df):\n        \"\"\"Chu·∫©n h√≥a t√™n c·ªôt (lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a)\"\"\"\n        df.columns = df.columns.str.strip()\n        return df\n\n    def _drop_unnecessary_columns(self, df):\n        \"\"\"Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn thi·∫øt cho hu·∫•n luy·ªán\"\"\"\n        columns_to_drop = [col for col in COLUMNS_TO_DROP if col in df.columns]\n\n        if columns_to_drop:\n            df = df.drop(columns=columns_to_drop)\n\n        return df\n\n    def _convert_to_numeric(self, df):\n        \"\"\"Chuy·ªÉn ƒë·ªïi c√°c c·ªôt v·ªÅ d·∫°ng s·ªë\"\"\"\n        # L·∫•y t·∫•t c·∫£ c·ªôt tr·ª´ Label\n        feature_cols = [col for col in df.columns if col != LABEL_COLUMN]\n\n        for col in feature_cols:\n            if df[col].dtype == 'object':\n                df[col] = pd.to_numeric(df[col], errors='coerce')\n\n        return df\n\n    def _handle_nan_inf(self, df):\n        \"\"\"X·ª≠ l√Ω gi√° tr·ªã NaN v√† Infinity\"\"\"\n        feature_cols = [col for col in df.columns if col != LABEL_COLUMN]\n\n        # ƒê·∫øm s·ªë l∆∞·ª£ng NaN v√† Inf tr∆∞·ªõc khi x·ª≠ l√Ω\n        nan_count = df[feature_cols].isna().sum().sum()\n        inf_count = np.isinf(df[feature_cols].select_dtypes(include=[np.number])).sum().sum()\n\n        self.stats['nan_inf_replaced'] += nan_count + inf_count\n\n        # Thay th·∫ø Infinity b·∫±ng NaN tr∆∞·ªõc\n        df[feature_cols] = df[feature_cols].replace([np.inf, -np.inf], np.nan)\n\n        # Thay th·∫ø NaN b·∫±ng 0 (ho·∫∑c c√≥ th·ªÉ d√πng median/mean)\n        df[feature_cols] = df[feature_cols].fillna(0)\n\n        return df\n\n    def _remove_duplicates(self, df):\n        \"\"\"Lo·∫°i b·ªè c√°c h√†ng tr√πng l·∫∑p\"\"\"\n        rows_before = len(df)\n        df = df.drop_duplicates()\n        rows_after = len(df)\n\n        self.stats['duplicates_removed'] += (rows_before - rows_after)\n\n        return df\n\n    def _convert_to_binary_label(self, df):\n        \"\"\"\n        Chuy·ªÉn ƒë·ªïi nh√£n sang d·∫°ng binary:\n        - Benign -> 0 (l∆∞u l∆∞·ª£ng b√¨nh th∆∞·ªùng)\n        - T·∫•t c·∫£ c√°c lo·∫°i t·∫•n c√¥ng kh√°c -> 1 (l∆∞u l∆∞·ª£ng b·∫•t th∆∞·ªùng)\n        \"\"\"\n        if LABEL_COLUMN not in df.columns:\n            raise ValueError(f\"Kh√¥ng t√¨m th·∫•y c·ªôt '{LABEL_COLUMN}' trong d·ªØ li·ªáu\")\n\n        # Chu·∫©n h√≥a nh√£n (lo·∫°i b·ªè kho·∫£ng tr·∫Øng, lowercase)\n        df[LABEL_COLUMN] = df[LABEL_COLUMN].astype(str).str.strip().str.lower()\n\n        # Lo·∫°i b·ªè c√°c h√†ng c√≥ nh√£n l√† 'label' (header b·ªã l·∫´n v√†o data)\n        df = df[df[LABEL_COLUMN] != 'label']\n\n        # Chuy·ªÉn ƒë·ªïi sang binary: Benign=0, Attack=1\n        df['binary_label'] = (df[LABEL_COLUMN] != 'benign').astype(int)\n\n        # ƒê·∫øm s·ªë l∆∞·ª£ng m·ªói class\n        benign_count = (df['binary_label'] == 0).sum()\n        attack_count = (df['binary_label'] == 1).sum()\n\n        self.stats['benign_count'] += benign_count\n        self.stats['attack_count'] += attack_count\n\n        # X√≥a c·ªôt Label g·ªëc, gi·ªØ l·∫°i binary_label\n        df = df.drop(columns=[LABEL_COLUMN])\n\n        return df\n\n    def _process_single_file(self, csv_file):\n        \"\"\"\n        X·ª≠ l√Ω m·ªôt file CSV theo chunks\n\n        Args:\n            csv_file: ƒê∆∞·ªùng d·∫´n file CSV\n\n        Returns:\n            DataFrame ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω\n        \"\"\"\n        print(f\"\\nüìÑ ƒêang x·ª≠ l√Ω: {csv_file.name}\")\n\n        processed_chunks = []\n        chunk_iterator = pd.read_csv(csv_file, chunksize=self.chunk_size,\n                                     low_memory=False, encoding='utf-8')\n\n        # Progress bar cho chunks\n        if TQDM_AVAILABLE:\n            # ∆Ø·ªõc t√≠nh s·ªë chunks d·ª±a tr√™n file size\n            file_size = csv_file.stat().st_size\n            estimated_chunks = max(1, file_size // (self.chunk_size * 500))  # ∆Ø·ªõc t√≠nh\n            chunk_iterator = tqdm(chunk_iterator, desc=\"   Chunks\",\n                                  total=estimated_chunks, unit=\"chunk\")\n\n        for chunk in chunk_iterator:\n            self.stats['total_rows_read'] += len(chunk)\n\n            # B∆∞·ªõc 1: Chu·∫©n h√≥a t√™n c·ªôt\n            chunk = self._clean_column_names(chunk)\n\n            # B∆∞·ªõc 2: Lo·∫°i b·ªè c·ªôt kh√¥ng c·∫ßn thi·∫øt\n            chunk = self._drop_unnecessary_columns(chunk)\n\n            # B∆∞·ªõc 3: Chuy·ªÉn ƒë·ªïi sang d·∫°ng s·ªë\n            chunk = self._convert_to_numeric(chunk)\n\n            # B∆∞·ªõc 4: X·ª≠ l√Ω NaN v√† Inf\n            chunk = self._handle_nan_inf(chunk)\n\n            # B∆∞·ªõc 5: Chuy·ªÉn ƒë·ªïi nh√£n sang binary\n            chunk = self._convert_to_binary_label(chunk)\n\n            processed_chunks.append(chunk)\n\n            # Gi·∫£i ph√≥ng b·ªô nh·ªõ\n            gc.collect()\n\n        # G·ªôp c√°c chunks l·∫°i\n        if processed_chunks:\n            df = pd.concat(processed_chunks, ignore_index=True)\n            del processed_chunks\n            gc.collect()\n            return df\n\n        return None\n\n    def process_all_files(self):\n        \"\"\"\n        X·ª≠ l√Ω t·∫•t c·∫£ c√°c file CSV v√† g·ªôp l·∫°i\n\n        Returns:\n            DataFrame ƒë√£ x·ª≠ l√Ω ho√†n ch·ªânh\n        \"\"\"\n        start_time = datetime.now()\n        print(\"\\n\" + \"=\"*80)\n        print(\"üöÄ B·∫ÆT ƒê·∫¶U X·ª¨ L√ù D·ªÆ LI·ªÜU CICIDS2018 CHO CNN\")\n        print(\"=\"*80)\n\n        csv_files = self._get_csv_files()\n\n        all_dataframes = []\n\n        # X·ª≠ l√Ω t·ª´ng file\n        for csv_file in csv_files:\n            df = self._process_single_file(csv_file)\n            if df is not None:\n                all_dataframes.append(df)\n                print(f\"   ‚úÖ ƒê√£ x·ª≠ l√Ω: {len(df):,} m·∫´u\")\n\n        # G·ªôp t·∫•t c·∫£ l·∫°i\n        print(\"\\n\" + \"-\"*80)\n        print(\"üìä ƒêANG G·ªòP V√Ä X·ª¨ L√ù CU·ªêI C√ôNG...\")\n\n        df_combined = pd.concat(all_dataframes, ignore_index=True)\n        del all_dataframes\n        gc.collect()\n\n        print(f\"   T·ªïng s·ªë m·∫´u sau khi g·ªôp: {len(df_combined):,}\")\n\n        # Lo·∫°i b·ªè duplicate tr√™n to√†n b·ªô dataset\n        print(\"   ƒêang lo·∫°i b·ªè duplicate...\")\n        df_combined = self._remove_duplicates(df_combined)\n        print(f\"   S·ªë m·∫´u sau khi lo·∫°i duplicate: {len(df_combined):,}\")\n\n        # C·∫≠p nh·∫≠t th·ªëng k√™\n        self.stats['rows_after_cleaning'] = len(df_combined)\n        self.stats['feature_count'] = len(df_combined.columns) - 1  # Tr·ª´ c·ªôt label\n\n        # L∆∞u t√™n features\n        self.feature_names = [col for col in df_combined.columns if col != 'binary_label']\n\n        end_time = datetime.now()\n        self.stats['processing_time'] = (end_time - start_time).total_seconds()\n\n        return df_combined\n\n    def balanced_sample(self, df):\n        \"\"\"\n        Sample d·ªØ li·ªáu v·ªõi s·ªë l∆∞·ª£ng c√¢n b·∫±ng theo target ƒë√£ ƒë·ªãnh\n\n        L·∫•y ch√≠nh x√°c:\n        - TARGET_BENIGN m·∫´u Benign (2,100,000)\n        - TARGET_ATTACK m·∫´u Attack (900,000)\n\n        Args:\n            df: DataFrame ƒë√£ clean\n\n        Returns:\n            DataFrame ƒë√£ ƒë∆∞·ª£c sample c√¢n b·∫±ng\n        \"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\"‚öñÔ∏è ƒêANG SAMPLE C√ÇN B·∫∞NG D·ªÆ LI·ªÜU\")\n        print(\"=\"*80)\n\n        # T√°ch theo class\n        df_benign = df[df['binary_label'] == 0]\n        df_attack = df[df['binary_label'] == 1]\n\n        n_benign = len(df_benign)\n        n_attack = len(df_attack)\n\n        print(f\"\\n   D·ªØ li·ªáu g·ªëc (sau khi clean):\")\n        print(f\"   - Benign: {n_benign:,}\")\n        print(f\"   - Attack: {n_attack:,}\")\n        print(f\"   - T·ªïng: {n_benign + n_attack:,}\")\n\n        print(f\"\\n   Target mong mu·ªën:\")\n        print(f\"   - Benign: {self.target_benign:,} ({BENIGN_RATIO*100:.0f}%)\")\n        print(f\"   - Attack: {self.target_attack:,} ({ATTACK_RATIO*100:.0f}%)\")\n        print(f\"   - T·ªïng: {self.target_benign + self.target_attack:,}\")\n\n        # Ki·ªÉm tra v√† ƒëi·ªÅu ch·ªânh n·∫øu kh√¥ng ƒë·ªß m·∫´u\n        actual_benign = min(self.target_benign, n_benign)\n        actual_attack = min(self.target_attack, n_attack)\n\n        if actual_benign < self.target_benign:\n            print(f\"\\n   ‚ö†Ô∏è Kh√¥ng ƒë·ªß Benign! Ch·ªâ c√≥ {n_benign:,}, c·∫ßn {self.target_benign:,}\")\n        if actual_attack < self.target_attack:\n            print(f\"\\n   ‚ö†Ô∏è Kh√¥ng ƒë·ªß Attack! Ch·ªâ c√≥ {n_attack:,}, c·∫ßn {self.target_attack:,}\")\n\n        # Random sample t·ª´ m·ªói class\n        print(f\"\\n   ƒêang sample...\")\n        df_benign_sampled = df_benign.sample(n=actual_benign, random_state=RANDOM_STATE)\n        df_attack_sampled = df_attack.sample(n=actual_attack, random_state=RANDOM_STATE)\n\n        # G·ªôp l·∫°i v√† shuffle\n        df_balanced = pd.concat([df_benign_sampled, df_attack_sampled], ignore_index=True)\n        df_balanced = df_balanced.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n\n        # Th·ªëng k√™ k·∫øt qu·∫£\n        final_benign = (df_balanced['binary_label'] == 0).sum()\n        final_attack = (df_balanced['binary_label'] == 1).sum()\n        total = len(df_balanced)\n\n        print(f\"\\n   ‚úÖ K·∫øt qu·∫£ sau khi sample:\")\n        print(f\"   - Benign: {final_benign:,} ({final_benign/total*100:.1f}%)\")\n        print(f\"   - Attack: {final_attack:,} ({final_attack/total*100:.1f}%)\")\n        print(f\"   - T·ªïng: {total:,}\")\n        print(f\"   - T·ª∑ l·ªá Benign:Attack = {final_benign/final_attack:.2f}:1\")\n\n        # C·∫≠p nh·∫≠t stats\n        self.stats['benign_count'] = final_benign\n        self.stats['attack_count'] = final_attack\n        self.stats['rows_after_cleaning'] = total\n\n        return df_balanced\n\n    def normalize_features(self, df):\n        \"\"\"\n        Chu·∫©n h√≥a c√°c features b·∫±ng scaler\n\n        Args:\n            df: DataFrame ch·ª©a features v√† label\n\n        Returns:\n            X_normalized: Features ƒë√£ chu·∫©n h√≥a\n            y: Labels\n        \"\"\"\n        print(\"\\nüîÑ ƒêANG CHU·∫®N H√ìA D·ªÆ LI·ªÜU...\")\n\n        # T√°ch features v√† label\n        X = df.drop(columns=['binary_label']).values\n        y = df['binary_label'].values\n\n        # Chu·∫©n h√≥a features\n        X_normalized = self.scaler.fit_transform(X)\n\n        print(f\"   Scaler type: {self.scaler_type}\")\n        print(f\"   Shape X: {X_normalized.shape}\")\n        print(f\"   Shape y: {y.shape}\")\n\n        return X_normalized, y\n\n    def reshape_for_cnn(self, X):\n        \"\"\"\n        Reshape d·ªØ li·ªáu cho CNN 1D\n\n        CNN 1D y√™u c·∫ßu input shape: (samples, features, channels)\n        Trong tr∆∞·ªùng h·ª£p n√†y: (samples, n_features, 1)\n\n        Args:\n            X: Features ƒë√£ chu·∫©n h√≥a, shape (samples, features)\n\n        Returns:\n            X_reshaped: Shape (samples, features, 1)\n        \"\"\"\n        print(\"\\nüîÑ ƒêANG RESHAPE D·ªÆ LI·ªÜU CHO CNN...\")\n\n        X_reshaped = X.reshape(X.shape[0], X.shape[1], 1)\n\n        print(f\"   Shape sau reshape: {X_reshaped.shape}\")\n\n        return X_reshaped\n\n    def split_data(self, X, y, test_size=0.2, val_size=0.1):\n        \"\"\"\n        Chia d·ªØ li·ªáu th√†nh train/val/test sets\n\n        Args:\n            X: Features\n            y: Labels\n            test_size: T·ª∑ l·ªá test set\n            val_size: T·ª∑ l·ªá validation set (t·ª´ train)\n\n        Returns:\n            X_train, X_val, X_test, y_train, y_val, y_test\n        \"\"\"\n        print(\"\\nüìä ƒêANG CHIA D·ªÆ LI·ªÜU TRAIN/VAL/TEST...\")\n\n        # Chia train+val / test\n        X_temp, X_test, y_temp, y_test = train_test_split(\n            X, y, test_size=test_size, random_state=RANDOM_STATE, stratify=y\n        )\n\n        # Chia train / val\n        val_ratio = val_size / (1 - test_size)\n        X_train, X_val, y_train, y_val = train_test_split(\n            X_temp, y_temp, test_size=val_ratio, random_state=RANDOM_STATE, stratify=y_temp\n        )\n\n        print(f\"   Train set: {X_train.shape[0]:,} m·∫´u\")\n        print(f\"   Val set:   {X_val.shape[0]:,} m·∫´u\")\n        print(f\"   Test set:  {X_test.shape[0]:,} m·∫´u\")\n\n        # Th·ªëng k√™ ph√¢n b·ªë class\n        print(f\"\\n   Ph√¢n b·ªë Train - Benign: {(y_train==0).sum():,}, Attack: {(y_train==1).sum():,}\")\n        print(f\"   Ph√¢n b·ªë Val   - Benign: {(y_val==0).sum():,}, Attack: {(y_val==1).sum():,}\")\n        print(f\"   Ph√¢n b·ªë Test  - Benign: {(y_test==0).sum():,}, Attack: {(y_test==1).sum():,}\")\n\n        return X_train, X_val, X_test, y_train, y_val, y_test\n\n\n    def save_processed_data(self, X_train, X_val, X_test, y_train, y_val, y_test):\n        \"\"\"\n        L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω sang ƒë·ªãnh d·∫°ng nhanh\n\n        L∆∞u th√†nh c√°c file:\n        - X_train.npy, X_val.npy, X_test.npy\n        - y_train.npy, y_val.npy, y_test.npy\n        - scaler.pkl\n        - metadata.json\n        \"\"\"\n        print(\"\\n ƒêANG L∆ØU D·ªÆ LI·ªÜU ƒê√É X·ª¨ L√ù...\")\n\n        # L∆∞u numpy arrays\n        np.save(self.output_dir / 'X_train.npy', X_train)\n        np.save(self.output_dir / 'X_val.npy', X_val)\n        np.save(self.output_dir / 'X_test.npy', X_test)\n        np.save(self.output_dir / 'y_train.npy', y_train)\n        np.save(self.output_dir / 'y_val.npy', y_val)\n        np.save(self.output_dir / 'y_test.npy', y_test)\n\n        print(f\"   ‚úÖ ƒê√£ l∆∞u X_train.npy: {X_train.shape}\")\n        print(f\"   ‚úÖ ƒê√£ l∆∞u X_val.npy: {X_val.shape}\")\n        print(f\"   ‚úÖ ƒê√£ l∆∞u X_test.npy: {X_test.shape}\")\n        print(f\"   ‚úÖ ƒê√£ l∆∞u y_train.npy: {y_train.shape}\")\n        print(f\"   ‚úÖ ƒê√£ l∆∞u y_val.npy: {y_val.shape}\")\n        print(f\"   ‚úÖ ƒê√£ l∆∞u y_test.npy: {y_test.shape}\")\n\n        # L∆∞u scaler\n        with open(self.output_dir / 'scaler.pkl', 'wb') as f:\n            pickle.dump(self.scaler, f)\n        print(f\"   ‚úÖ ƒê√£ l∆∞u scaler.pkl\")\n\n        # L∆∞u feature names\n        with open(self.output_dir / 'feature_names.txt', 'w') as f:\n            for name in self.feature_names:\n                f.write(name + '\\n')\n        print(f\"   ‚úÖ ƒê√£ l∆∞u feature_names.txt\")\n\n        # Chuy·ªÉn ƒë·ªïi stats sang ki·ªÉu Python native (ƒë·ªÉ tr√°nh l·ªói JSON v·ªõi numpy.int64)\n        stats_native = {}\n        for key, value in self.stats.items():\n            if hasattr(value, 'item'):  # Ki·ªÉm tra n·∫øu l√† numpy type\n                stats_native[key] = value.item()\n            elif isinstance(value, (np.integer, np.floating)):\n                stats_native[key] = int(value) if isinstance(value, np.integer) else float(value)\n            else:\n                stats_native[key] = value\n\n        # L∆∞u metadata\n        metadata = {\n            'n_features': len(self.feature_names),\n            'feature_names': self.feature_names,\n            'train_samples': int(X_train.shape[0]),\n            'val_samples': int(X_val.shape[0]),\n            'test_samples': int(X_test.shape[0]),\n            'input_shape': [int(x) for x in X_train.shape[1:]],\n            'scaler_type': self.scaler_type,\n            'stats': stats_native,\n            'created_at': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        }\n\n        with open(self.output_dir / 'metadata.json', 'w') as f:\n            json.dump(metadata, f, indent=4)\n        print(f\"   ‚úÖ ƒê√£ l∆∞u metadata.json\")\n\n        print(f\"\\nüìÅ T·∫•t c·∫£ file ƒë∆∞·ª£c l∆∞u t·∫°i: {self.output_dir}\")\n\n    def print_summary(self):\n        \"\"\"In t√≥m t·∫Øt qu√° tr√¨nh x·ª≠ l√Ω\"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\"üìä T√ìM T·∫ÆT X·ª¨ L√ù D·ªÆ LI·ªÜU\")\n        print(\"=\"*80)\n        print(f\"   T·ªïng s·ªë d√≤ng ƒë·ªçc ƒë∆∞·ª£c:     {self.stats['total_rows_read']:,}\")\n        print(f\"   S·ªë d√≤ng sau khi x·ª≠ l√Ω:     {self.stats['rows_after_cleaning']:,}\")\n        print(f\"   S·ªë duplicate ƒë√£ lo·∫°i:      {self.stats['duplicates_removed']:,}\")\n        print(f\"   S·ªë NaN/Inf ƒë√£ thay th·∫ø:    {self.stats['nan_inf_replaced']:,}\")\n        print(f\"   S·ªë features:               {self.stats['feature_count']}\")\n        print(f\"   S·ªë m·∫´u Benign:             {self.stats['benign_count']:,}\")\n        print(f\"   S·ªë m·∫´u Attack:             {self.stats['attack_count']:,}\")\n        print(f\"   Th·ªùi gian x·ª≠ l√Ω:           {self.stats['processing_time']:.2f} gi√¢y\")\n        print(\"=\"*80)\n\n\ndef main():\n    \"\"\"H√†m ch√≠nh ƒë·ªÉ ch·∫°y preprocessing\"\"\"\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"üîß TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU CICIDS2018 CHO M√î H√åNH CNN\")\n    print(\"   Ph√°t hi·ªán l∆∞u l∆∞·ª£ng m·∫°ng IoT b·∫•t th∆∞·ªùng\")\n    print(\"=\"*80)\n\n    print(f\"\\nüìã C·∫§U H√åNH:\")\n    print(f\"   - T·ªïng m·∫´u mong mu·ªën: {TOTAL_SAMPLES:,}\")\n    print(f\"   - Benign: {TARGET_BENIGN:,} ({BENIGN_RATIO*100:.0f}%)\")\n    print(f\"   - Attack: {TARGET_ATTACK:,} ({ATTACK_RATIO*100:.0f}%)\")\n\n    # Kh·ªüi t·∫°o preprocessor\n    preprocessor = CICIDS2018_CNN_Preprocessor(\n        data_dir=DATA_DIR,\n        output_dir=OUTPUT_DIR,\n        chunk_size=CHUNK_SIZE,\n        scaler_type=SCALER_TYPE,\n        target_benign=TARGET_BENIGN,\n        target_attack=TARGET_ATTACK\n    )\n\n    # B∆∞·ªõc 1: X·ª≠ l√Ω t·∫•t c·∫£ c√°c file CSV (clean data)\n    df = preprocessor.process_all_files()\n\n    # B∆∞·ªõc 2: SAMPLE C√ÇN B·∫∞NG TR∆Ø·ªöC KHI CHIA\n    # ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o train/val/test ƒë·ªÅu c√≥ t·ª∑ l·ªá 70-30\n    df = preprocessor.balanced_sample(df)\n\n    # B∆∞·ªõc 3: Chu·∫©n h√≥a features\n    X, y = preprocessor.normalize_features(df)\n\n    # Gi·∫£i ph√≥ng b·ªô nh·ªõ c·ªßa DataFrame\n    del df\n    gc.collect()\n\n    # B∆∞·ªõc 4: Reshape cho CNN\n    X = preprocessor.reshape_for_cnn(X)\n\n    # B∆∞·ªõc 5: Chia d·ªØ li·ªáu (stratify ƒë·ªÉ gi·ªØ t·ª∑ l·ªá 70-30 trong t·∫•t c·∫£ c√°c t·∫≠p)\n    X_train, X_val, X_test, y_train, y_val, y_test = preprocessor.split_data(X, y)\n\n    # Gi·∫£i ph√≥ng b·ªô nh·ªõ\n    del X, y\n    gc.collect()\n\n    # B∆∞·ªõc 6: L∆∞u d·ªØ li·ªáu\n    preprocessor.save_processed_data(X_train, X_val, X_test, y_train, y_val, y_test)\n\n    # In t√≥m t·∫Øt\n    preprocessor.print_summary()\n\n    print(\"\\n‚úÖ HO√ÄN TH√ÄNH! D·ªØ li·ªáu ƒë√£ s·∫µn s√†ng cho vi·ªác hu·∫•n luy·ªán CNN.\")\n\n    return preprocessor\n\n\nif __name__ == \"__main__\":\n    preprocessor = main()\n\n",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import shutil\nimport os\n\n# ƒê∆∞·ªùng d·∫´n chu·∫©n trong Kaggle\ndirectory_to_zip = '/kaggle/working/models'\noutput_filename = 'model_cnn_5layer' # T√™n file k·∫øt qu·∫£ (kh√¥ng c·∫ßn ƒëu√¥i .zip)\n\n# Ki·ªÉm tra xem c√≥ file n√†o trong ƒë√≥ kh√¥ng\nif os.path.exists(directory_to_zip):\n    print(\"ƒêang n√©n d·ªØ li·ªáu...\")\n    # N√©n th∆∞ m·ª•c\n    shutil.make_archive(output_filename, 'zip', directory_to_zip)\n    print(f\"‚úÖ Xong! File n·∫±m t·∫°i: {os.path.join(directory_to_zip, output_filename + '.zip')}\")\nelse:\n    print(\"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c working.\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-28T12:53:34.253334Z",
     "iopub.execute_input": "2025-12-28T12:53:34.253903Z",
     "iopub.status.idle": "2025-12-28T12:53:34.611601Z",
     "shell.execute_reply.started": "2025-12-28T12:53:34.253876Z",
     "shell.execute_reply": "2025-12-28T12:53:34.611030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "ƒêang n√©n d·ªØ li·ªáu...\n‚úÖ Xong! File n·∫±m t·∫°i: /kaggle/working/models/model_cnn_5layer.zip\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\n======================================================================================\nHU·∫§N LUY·ªÜN M√î H√åNH CNN - PH√ÅT HI·ªÜN L∆ØU L∆Ø·ª¢NG M·∫†NG IOT B·∫§T TH∆Ø·ªúNG\n======================================================================================\n\nScript n√†y hu·∫•n luy·ªán m√¥ h√¨nh CNN 1D cho b√†i to√°n ph√¢n lo·∫°i binary:\n- Benign (0): L∆∞u l∆∞·ª£ng m·∫°ng b√¨nh th∆∞·ªùng\n- Attack (1): L∆∞u l∆∞·ª£ng m·∫°ng b·∫•t th∆∞·ªùng/t·∫•n c√¥ng\n\nM√¥ h√¨nh CNN 1D ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ h·ªçc c√°c patterns t·ª´ network flow features.\n\nC√≥ th·ªÉ ch·∫°y tr√™n c·∫£ Kaggle v√† Local.\n\"\"\"\n\nimport os\nimport sys\nimport numpy as np\nimport json\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ============================================================================\n# KI·ªÇM TRA M√îI TR∆Ø·ªúNG V√Ä IMPORT TH∆Ø VI·ªÜN\n# ============================================================================\n\n# Ki·ªÉm tra m√¥i tr∆∞·ªùng ch·∫°y (Kaggle ho·∫∑c Local)\nIS_KAGGLE = os.path.exists('/kaggle/input')\n\nif IS_KAGGLE:\n    print(\"üåê ƒêang ch·∫°y tr√™n KAGGLE\")\nelse:\n    print(\"üíª ƒêang ch·∫°y tr√™n LOCAL\")\n\n# Import TensorFlow/Keras\ntry:\n    import tensorflow as tf\n    from tensorflow import keras\n    from tensorflow.keras import layers, models, callbacks\n    from tensorflow.keras.optimizers import Adam\n    print(f\"‚úÖ TensorFlow version: {tf.__version__}\")\nexcept ImportError:\n    print(\"‚ùå L·ªói: TensorFlow ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t!\")\n    print(\"   C√†i ƒë·∫∑t b·∫±ng: pip install tensorflow\")\n    sys.exit(1)\n\n# Import sklearn cho metrics\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix,\n    accuracy_score, precision_score, recall_score,\n    f1_score, roc_auc_score, roc_curve\n)\n\n# ============================================================================\n# C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N\n# ============================================================================\n\nif IS_KAGGLE:\n    # ƒê∆∞·ªùng d·∫´n tr√™n Kaggle\n    PROCESSED_DATA_DIR = \"/kaggle/working/processed_data_cnn\"\n    OUTPUT_DIR = \"/kaggle/working/cnn_results\"\nelse:\n    # ƒê∆∞·ªùng d·∫´n Local\n    PROCESSED_DATA_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CNN\\processed_data_cnn\"\n    OUTPUT_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CNN\\results\"\n\n# ============================================================================\n# C·∫§U H√åNH M√î H√åNH V√Ä HU·∫§N LUY·ªÜN\n# ============================================================================\n\n# Hyperparameters cho CNN\nCNN_CONFIG = {\n    # Ki·∫øn tr√∫c m·∫°ng\n    'conv_filters': [64, 128, 256],      # S·ªë filters cho m·ªói Conv layer\n    'kernel_size': 3,                     # K√≠ch th∆∞·ªõc kernel\n    'pool_size': 2,                       # K√≠ch th∆∞·ªõc pooling\n    'dense_units': [128, 64],             # S·ªë units cho Dense layers\n    'dropout_rate': 0.3,                  # T·ª∑ l·ªá dropout\n\n    # Hu·∫•n luy·ªán\n    'batch_size': 256,                    # Batch size\n    'epochs': 30,                         # S·ªë epochs t·ªëi ƒëa\n    'learning_rate': 0.001,               # Learning rate\n    'early_stopping_patience': 10,        # Patience cho early stopping\n    'reduce_lr_patience': 5,              # Patience cho reduce LR\n    'reduce_lr_factor': 0.5,              # Factor gi·∫£m LR\n    'min_lr': 1e-7,                       # LR t·ªëi thi·ªÉu\n\n    # Class weights ƒë·ªÉ x·ª≠ l√Ω imbalanced data\n    'use_class_weight': True,             # S·ª≠ d·ª•ng class weight\n}\n\n# ============================================================================\n# LOAD D·ªÆ LI·ªÜU ƒê√É X·ª¨ L√ù\n# ============================================================================\n\ndef load_processed_data(data_dir):\n    \"\"\"\n    Load d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c ti·ªÅn x·ª≠ l√Ω\n\n    Args:\n        data_dir: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu\n\n    Returns:\n        X_train, X_val, X_test, y_train, y_val, y_test, metadata\n    \"\"\"\n    data_dir = Path(data_dir)\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"üìÇ ƒêANG LOAD D·ªÆ LI·ªÜU ƒê√É X·ª¨ L√ù\")\n    print(\"=\"*80)\n\n    # Load numpy arrays\n    X_train = np.load(data_dir / 'X_train.npy')\n    X_val = np.load(data_dir / 'X_val.npy')\n    X_test = np.load(data_dir / 'X_test.npy')\n    y_train = np.load(data_dir / 'y_train.npy')\n    y_val = np.load(data_dir / 'y_val.npy')\n    y_test = np.load(data_dir / 'y_test.npy')\n\n    print(f\"   X_train: {X_train.shape}\")\n    print(f\"   X_val:   {X_val.shape}\")\n    print(f\"   X_test:  {X_test.shape}\")\n    print(f\"   y_train: {y_train.shape}\")\n    print(f\"   y_val:   {y_val.shape}\")\n    print(f\"   y_test:  {y_test.shape}\")\n\n    # Load metadata\n    with open(data_dir / 'metadata.json', 'r') as f:\n        metadata = json.load(f)\n\n    print(f\"\\n   S·ªë features: {metadata['n_features']}\")\n    print(f\"   Train samples: {metadata['train_samples']:,}\")\n    print(f\"   Val samples: {metadata['val_samples']:,}\")\n    print(f\"   Test samples: {metadata['test_samples']:,}\")\n\n    return X_train, X_val, X_test, y_train, y_val, y_test, metadata\n\n\n# ============================================================================\n# X√ÇY D·ª∞NG M√î H√åNH CNN\n# ============================================================================\n\ndef build_cnn_model(input_shape, config=CNN_CONFIG):\n    \"\"\"\n    X√¢y d·ª±ng m√¥ h√¨nh CNN 1D cho ph√¢n lo·∫°i binary\n\n    Ki·∫øn tr√∫c:\n    - Nhi·ªÅu block Conv1D + BatchNorm + ReLU + MaxPooling + Dropout\n    - Flatten\n    - Dense layers v·ªõi Dropout\n    - Output layer v·ªõi Sigmoid\n\n    Args:\n        input_shape: Shape c·ªßa input (n_features, 1)\n        config: Dictionary ch·ª©a c√°c hyperparameters\n\n    Returns:\n        model: M√¥ h√¨nh Keras ƒë√£ compile\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"üèóÔ∏è ƒêANG X√ÇY D·ª∞NG M√î H√åNH CNN\")\n    print(\"=\"*80)\n\n    model = models.Sequential(name=\"CNN_IDS\")\n\n    # Input layer\n    model.add(layers.InputLayer(input_shape=input_shape))\n\n    # Convolutional blocks\n    for i, filters in enumerate(config['conv_filters']):\n        # Conv1D layer\n        model.add(layers.Conv1D(\n            filters=filters,\n            kernel_size=config['kernel_size'],\n            padding='same',\n            name=f'conv1d_{i+1}'\n        ))\n\n        # Batch Normalization\n        model.add(layers.BatchNormalization(name=f'bn_{i+1}'))\n\n        # Activation\n        model.add(layers.Activation('relu', name=f'relu_{i+1}'))\n\n        # MaxPooling (ch·ªâ √°p d·ª•ng n·∫øu k√≠ch th∆∞·ªõc ƒë·ªß l·ªõn)\n        model.add(layers.MaxPooling1D(\n            pool_size=config['pool_size'],\n            padding='same',\n            name=f'maxpool_{i+1}'\n        ))\n\n        # Dropout\n        model.add(layers.Dropout(\n            config['dropout_rate'],\n            name=f'dropout_conv_{i+1}'\n        ))\n\n    # Flatten\n    model.add(layers.Flatten(name='flatten'))\n\n    # Dense layers\n    for i, units in enumerate(config['dense_units']):\n        model.add(layers.Dense(units, name=f'dense_{i+1}'))\n        model.add(layers.BatchNormalization(name=f'bn_dense_{i+1}'))\n        model.add(layers.Activation('relu', name=f'relu_dense_{i+1}'))\n        model.add(layers.Dropout(config['dropout_rate'], name=f'dropout_dense_{i+1}'))\n\n    # Output layer (binary classification)\n    model.add(layers.Dense(1, activation='sigmoid', name='output'))\n\n    # Compile model\n    optimizer = Adam(learning_rate=config['learning_rate'])\n\n    model.compile(\n        optimizer=optimizer,\n        loss='binary_crossentropy',\n        metrics=[\n            'accuracy',\n            keras.metrics.Precision(name='precision'),\n            keras.metrics.Recall(name='recall'),\n            keras.metrics.AUC(name='auc')\n        ]\n    )\n\n    # In summary\n    print(f\"\\n   Input shape: {input_shape}\")\n    print(f\"   Conv filters: {config['conv_filters']}\")\n    print(f\"   Dense units: {config['dense_units']}\")\n    print(f\"   Dropout rate: {config['dropout_rate']}\")\n    print(f\"   Learning rate: {config['learning_rate']}\")\n\n    model.summary()\n\n    return model\n\n\n# ============================================================================\n# T√çNH CLASS WEIGHTS\n# ============================================================================\n\ndef compute_class_weights(y_train):\n    \"\"\"\n    T√≠nh class weights ƒë·ªÉ x·ª≠ l√Ω imbalanced data\n\n    Args:\n        y_train: Labels c·ªßa training set\n\n    Returns:\n        class_weight: Dictionary {0: weight_0, 1: weight_1}\n    \"\"\"\n    # ƒê·∫øm s·ªë l∆∞·ª£ng m·ªói class\n    n_benign = (y_train == 0).sum()\n    n_attack = (y_train == 1).sum()\n    total = len(y_train)\n\n    # T√≠nh weights\n    weight_benign = total / (2 * n_benign)\n    weight_attack = total / (2 * n_attack)\n\n    class_weight = {\n        0: weight_benign,\n        1: weight_attack\n    }\n\n    print(f\"\\nüìä Class weights:\")\n    print(f\"   Benign (0): {n_benign:,} m·∫´u, weight = {weight_benign:.4f}\")\n    print(f\"   Attack (1): {n_attack:,} m·∫´u, weight = {weight_attack:.4f}\")\n\n    return class_weight\n\n\n# ============================================================================\n# CALLBACKS\n# ============================================================================\n\ndef get_callbacks(output_dir, config=CNN_CONFIG):\n    \"\"\"\n    T·∫°o c√°c callbacks cho qu√° tr√¨nh hu·∫•n luy·ªán\n\n    Args:\n        output_dir: ƒê∆∞·ªùng d·∫´n l∆∞u model\n        config: Dictionary ch·ª©a c√°c hyperparameters\n\n    Returns:\n        List c√°c callbacks\n    \"\"\"\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    callback_list = [\n        # Early stopping - d·ª´ng n·∫øu val_loss kh√¥ng c·∫£i thi·ªán\n        callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=config['early_stopping_patience'],\n            restore_best_weights=True,\n            verbose=1\n        ),\n\n        # Reduce learning rate khi val_loss kh√¥ng c·∫£i thi·ªán\n        callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=config['reduce_lr_factor'],\n            patience=config['reduce_lr_patience'],\n            min_lr=config['min_lr'],\n            verbose=1\n        ),\n\n        # Model checkpoint - l∆∞u model t·ªët nh·∫•t\n        callbacks.ModelCheckpoint(\n            filepath=str(output_dir / 'best_model.keras'),\n            monitor='val_auc',\n            mode='max',\n            save_best_only=True,\n            verbose=1\n        ),\n\n        # TensorBoard logs (optional)\n        callbacks.TensorBoard(\n            log_dir=str(output_dir / 'logs'),\n            histogram_freq=1\n        ),\n\n        # CSV logger - l∆∞u history ra file\n        callbacks.CSVLogger(\n            filename=str(output_dir / 'training_history.csv'),\n            separator=',',\n            append=False\n        )\n    ]\n\n    return callback_list\n\n\n# ============================================================================\n# HU·∫§N LUY·ªÜN M√î H√åNH\n# ============================================================================\n\ndef train_model(model, X_train, y_train, X_val, y_val,\n                output_dir, config=None):\n    \"\"\"\n    Hu·∫•n luy·ªán m√¥ h√¨nh CNN\n\n    Args:\n        model: M√¥ h√¨nh Keras\n        X_train, y_train: D·ªØ li·ªáu training\n        X_val, y_val: D·ªØ li·ªáu validation\n        output_dir: ƒê∆∞·ªùng d·∫´n l∆∞u k·∫øt qu·∫£\n        config: Dictionary ch·ª©a c√°c hyperparameters\n\n    Returns:\n        history: History object c·ªßa qu√° tr√¨nh training\n    \"\"\"\n    if config is None:\n        config = CNN_CONFIG\n    print(\"\\n\" + \"=\"*80)\n    print(\"üöÄ B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN M√î H√åNH\")\n    print(\"=\"*80)\n\n    # T√≠nh class weights n·∫øu c·∫ßn\n    class_weight = None\n    if config['use_class_weight']:\n        class_weight = compute_class_weights(y_train)\n\n    # L·∫•y callbacks\n    callback_list = get_callbacks(output_dir, config)\n\n    print(f\"\\n   Batch size: {config['batch_size']}\")\n    print(f\"   Max epochs: {config['epochs']}\")\n    print(f\"   Early stopping patience: {config['early_stopping_patience']}\")\n\n    # Hu·∫•n luy·ªán\n    start_time = datetime.now()\n\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        epochs=config['epochs'],\n        batch_size=config['batch_size'],\n        class_weight=class_weight,\n        callbacks=callback_list,\n        verbose=1\n    )\n\n    end_time = datetime.now()\n    training_time = (end_time - start_time).total_seconds()\n\n    print(f\"\\n‚è±Ô∏è Th·ªùi gian hu·∫•n luy·ªán: {training_time/60:.2f} ph√∫t\")\n\n    return history, training_time\n\n\n# ============================================================================\n# ƒê√ÅNH GI√Å M√î H√åNH\n# ============================================================================\n\ndef evaluate_model(model, X_test, y_test):\n    \"\"\"\n    ƒê√°nh gi√° m√¥ h√¨nh tr√™n test set\n\n    Args:\n        model: M√¥ h√¨nh ƒë√£ train\n        X_test, y_test: D·ªØ li·ªáu test\n\n    Returns:\n        results: Dictionary ch·ª©a c√°c metrics\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"üìä ƒê√ÅNH GI√Å M√î H√åNH TR√äN TEST SET\")\n    print(\"=\"*80)\n\n    # D·ª± ƒëo√°n\n    y_pred_prob = model.predict(X_test, verbose=0)\n    y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n\n    # T√≠nh c√°c metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    auc = roc_auc_score(y_test, y_pred_prob)\n\n    # In k·∫øt qu·∫£\n    print(f\"\\n   Accuracy:  {accuracy:.4f}\")\n    print(f\"   Precision: {precision:.4f}\")\n    print(f\"   Recall:    {recall:.4f}\")\n    print(f\"   F1-Score:  {f1:.4f}\")\n    print(f\"   AUC-ROC:   {auc:.4f}\")\n\n    # Classification report\n    print(\"\\n\" + \"-\"*60)\n    print(\"CLASSIFICATION REPORT:\")\n    print(\"-\"*60)\n    target_names = ['Benign', 'Attack']\n    print(classification_report(y_test, y_pred, target_names=target_names))\n\n    # Confusion matrix\n    print(\"-\"*60)\n    print(\"CONFUSION MATRIX:\")\n    print(\"-\"*60)\n    cm = confusion_matrix(y_test, y_pred)\n    print(f\"                Predicted\")\n    print(f\"              Benign  Attack\")\n    print(f\"Actual Benign   {cm[0,0]:6d}  {cm[0,1]:6d}\")\n    print(f\"       Attack   {cm[1,0]:6d}  {cm[1,1]:6d}\")\n\n    # T√≠nh detection rate v√† false alarm rate\n    tn, fp, fn, tp = cm.ravel()\n    detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 0\n    false_alarm_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n\n    print(f\"\\n   Detection Rate (True Positive Rate): {detection_rate:.4f}\")\n    print(f\"   False Alarm Rate (False Positive Rate): {false_alarm_rate:.4f}\")\n\n    results = {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1,\n        'auc_roc': auc,\n        'detection_rate': detection_rate,\n        'false_alarm_rate': false_alarm_rate,\n        'confusion_matrix': cm.tolist(),\n        'y_pred_prob': y_pred_prob,\n        'y_pred': y_pred\n    }\n\n    return results\n\n\n# ============================================================================\n# V·∫º BI·ªÇU ƒê·ªí\n# ============================================================================\n\ndef plot_training_history(history, output_dir):\n    \"\"\"\n    V·∫Ω bi·ªÉu ƒë·ªì training history\n\n    Args:\n        history: History object t·ª´ model.fit()\n        output_dir: ƒê∆∞·ªùng d·∫´n l∆∞u bi·ªÉu ƒë·ªì\n    \"\"\"\n    output_dir = Path(output_dir)\n\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n    # Plot Loss\n    axes[0, 0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n    axes[0, 0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n    axes[0, 0].set_title('Loss', fontsize=14, fontweight='bold')\n    axes[0, 0].set_xlabel('Epoch')\n    axes[0, 0].set_ylabel('Loss')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True, alpha=0.3)\n\n    # Plot Accuracy\n    axes[0, 1].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n    axes[0, 1].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n    axes[0, 1].set_title('Accuracy', fontsize=14, fontweight='bold')\n    axes[0, 1].set_xlabel('Epoch')\n    axes[0, 1].set_ylabel('Accuracy')\n    axes[0, 1].legend()\n    axes[0, 1].grid(True, alpha=0.3)\n\n    # Plot Precision & Recall\n    axes[1, 0].plot(history.history['precision'], label='Train Precision', linewidth=2)\n    axes[1, 0].plot(history.history['val_precision'], label='Val Precision', linewidth=2)\n    axes[1, 0].plot(history.history['recall'], label='Train Recall', linewidth=2)\n    axes[1, 0].plot(history.history['val_recall'], label='Val Recall', linewidth=2)\n    axes[1, 0].set_title('Precision & Recall', fontsize=14, fontweight='bold')\n    axes[1, 0].set_xlabel('Epoch')\n    axes[1, 0].set_ylabel('Score')\n    axes[1, 0].legend()\n    axes[1, 0].grid(True, alpha=0.3)\n\n    # Plot AUC\n    axes[1, 1].plot(history.history['auc'], label='Train AUC', linewidth=2)\n    axes[1, 1].plot(history.history['val_auc'], label='Val AUC', linewidth=2)\n    axes[1, 1].set_title('AUC-ROC', fontsize=14, fontweight='bold')\n    axes[1, 1].set_xlabel('Epoch')\n    axes[1, 1].set_ylabel('AUC')\n    axes[1, 1].legend()\n    axes[1, 1].grid(True, alpha=0.3)\n\n    plt.suptitle('Training History - CNN IoT Anomaly Detection',\n                 fontsize=16, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig(output_dir / 'training_history.png', dpi=150, bbox_inches='tight')\n    plt.close()\n\n    print(f\"\\nüìä ƒê√£ l∆∞u bi·ªÉu ƒë·ªì training history: {output_dir / 'training_history.png'}\")\n\n\ndef plot_confusion_matrix(cm, output_dir):\n    \"\"\"\n    V·∫Ω confusion matrix\n\n    Args:\n        cm: Confusion matrix\n        output_dir: ƒê∆∞·ªùng d·∫´n l∆∞u bi·ªÉu ƒë·ªì\n    \"\"\"\n    output_dir = Path(output_dir)\n\n    fig, ax = plt.subplots(figsize=(8, 6))\n\n    # V·∫Ω heatmap\n    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    ax.figure.colorbar(im, ax=ax)\n\n    # Labels\n    classes = ['Benign', 'Attack']\n    ax.set(xticks=[0, 1], yticks=[0, 1],\n           xticklabels=classes, yticklabels=classes,\n           title='Confusion Matrix',\n           ylabel='Actual',\n           xlabel='Predicted')\n\n    # Th√™m text v√†o c√°c √¥\n    thresh = cm.max() / 2.\n    for i in range(2):\n        for j in range(2):\n            ax.text(j, i, format(cm[i, j], 'd'),\n                   ha=\"center\", va=\"center\",\n                   color=\"white\" if cm[i, j] > thresh else \"black\",\n                   fontsize=14)\n\n    plt.tight_layout()\n    plt.savefig(output_dir / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n    plt.close()\n\n    print(f\"üìä ƒê√£ l∆∞u confusion matrix: {output_dir / 'confusion_matrix.png'}\")\n\n\ndef plot_roc_curve(y_test, y_pred_prob, output_dir):\n    \"\"\"\n    V·∫Ω ROC curve\n\n    Args:\n        y_test: Labels th·ª±c t·∫ø\n        y_pred_prob: X√°c su·∫•t d·ª± ƒëo√°n\n        output_dir: ƒê∆∞·ªùng d·∫´n l∆∞u bi·ªÉu ƒë·ªì\n    \"\"\"\n    output_dir = Path(output_dir)\n\n    # T√≠nh ROC curve\n    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n    auc = roc_auc_score(y_test, y_pred_prob)\n\n    fig, ax = plt.subplots(figsize=(8, 6))\n\n    ax.plot(fpr, tpr, color='blue', linewidth=2,\n            label=f'ROC curve (AUC = {auc:.4f})')\n    ax.plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=1)\n\n    ax.set_xlim([0.0, 1.0])\n    ax.set_ylim([0.0, 1.05])\n    ax.set_xlabel('False Positive Rate', fontsize=12)\n    ax.set_ylabel('True Positive Rate', fontsize=12)\n    ax.set_title('Receiver Operating Characteristic (ROC) Curve',\n                fontsize=14, fontweight='bold')\n    ax.legend(loc=\"lower right\")\n    ax.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.savefig(output_dir / 'roc_curve.png', dpi=150, bbox_inches='tight')\n    plt.close()\n\n    print(f\"üìä ƒê√£ l∆∞u ROC curve: {output_dir / 'roc_curve.png'}\")\n\n\n# ============================================================================\n# L∆ØU K·∫æT QU·∫¢\n# ============================================================================\n\ndef save_results(results, history, training_time, output_dir, config):\n    \"\"\"\n    L∆∞u k·∫øt qu·∫£ v√† th√¥ng tin m√¥ h√¨nh\n\n    Args:\n        results: Dictionary ch·ª©a metrics\n        history: Training history\n        training_time: Th·ªùi gian training (gi√¢y)\n        output_dir: ƒê∆∞·ªùng d·∫´n l∆∞u\n        config: Config c·ªßa m√¥ h√¨nh\n    \"\"\"\n    output_dir = Path(output_dir)\n\n    # T·∫°o summary\n    summary = {\n        'model_name': 'CNN_1D_IDS',\n        'task': 'Binary Classification - IoT Anomaly Detection',\n        'dataset': 'CICIDS2018',\n        'training_time_seconds': training_time,\n        'training_time_minutes': training_time / 60,\n        'config': config,\n        'results': {\n            'accuracy': float(results['accuracy']),\n            'precision': float(results['precision']),\n            'recall': float(results['recall']),\n            'f1_score': float(results['f1_score']),\n            'auc_roc': float(results['auc_roc']),\n            'detection_rate': float(results['detection_rate']),\n            'false_alarm_rate': float(results['false_alarm_rate']),\n            'confusion_matrix': results['confusion_matrix']\n        },\n        'final_epoch': len(history.history['loss']),\n        'best_val_loss': float(min(history.history['val_loss'])),\n        'best_val_auc': float(max(history.history['val_auc'])),\n        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    }\n\n    # L∆∞u summary\n    with open(output_dir / 'results_summary.json', 'w') as f:\n        json.dump(summary, f, indent=4)\n\n    print(f\"\\nüíæ ƒê√£ l∆∞u k·∫øt qu·∫£: {output_dir / 'results_summary.json'}\")\n\n\n# ============================================================================\n# H√ÄM CH√çNH\n# ============================================================================\n\ndef main():\n    \"\"\"H√†m ch√≠nh ƒë·ªÉ ch·∫°y to√†n b·ªô pipeline\"\"\"\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"üîß HU·∫§N LUY·ªÜN M√î H√åNH CNN - PH√ÅT HI·ªÜN L∆ØU L∆Ø·ª¢NG M·∫†NG IOT B·∫§T TH∆Ø·ªúNG\")\n    print(\"=\"*80)\n\n    # T·∫°o th∆∞ m·ª•c output\n    output_dir = Path(OUTPUT_DIR)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # B∆∞·ªõc 1: Load d·ªØ li·ªáu\n    X_train, X_val, X_test, y_train, y_val, y_test, metadata = load_processed_data(PROCESSED_DATA_DIR)\n\n    # B∆∞·ªõc 2: X√¢y d·ª±ng m√¥ h√¨nh\n    input_shape = (X_train.shape[1], X_train.shape[2])  # (n_features, 1)\n    model = build_cnn_model(input_shape)\n\n    # B∆∞·ªõc 3: Hu·∫•n luy·ªán m√¥ h√¨nh\n    history, training_time = train_model(\n        model, X_train, y_train, X_val, y_val,\n        output_dir, CNN_CONFIG\n    )\n\n    # B∆∞·ªõc 4: ƒê√°nh gi√° m√¥ h√¨nh\n    results = evaluate_model(model, X_test, y_test)\n\n    # B∆∞·ªõc 5: V·∫Ω bi·ªÉu ƒë·ªì\n    plot_training_history(history, output_dir)\n    plot_confusion_matrix(np.array(results['confusion_matrix']), output_dir)\n    plot_roc_curve(y_test, results['y_pred_prob'], output_dir)\n\n    # B∆∞·ªõc 6: L∆∞u k·∫øt qu·∫£\n    save_results(results, history, training_time, output_dir, CNN_CONFIG)\n\n    # L∆∞u model cu·ªëi c√πng\n    model.save(output_dir / 'final_model.keras')\n    print(f\"üíæ ƒê√£ l∆∞u model cu·ªëi c√πng: {output_dir / 'final_model.keras'}\")\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"‚úÖ HO√ÄN TH√ÄNH HU·∫§N LUY·ªÜN M√î H√åNH CNN!\")\n    print(\"=\"*80)\n    print(f\"\\nüìÅ T·∫•t c·∫£ k·∫øt qu·∫£ ƒë∆∞·ª£c l∆∞u t·∫°i: {output_dir}\")\n    print(f\"   - best_model.keras: Model t·ªët nh·∫•t (theo val_auc)\")\n    print(f\"   - final_model.keras: Model cu·ªëi c√πng\")\n    print(f\"   - training_history.png: Bi·ªÉu ƒë·ªì training\")\n    print(f\"   - confusion_matrix.png: Confusion matrix\")\n    print(f\"   - roc_curve.png: ROC curve\")\n    print(f\"   - results_summary.json: T√≥m t·∫Øt k·∫øt qu·∫£\")\n\n    return model, history, results\n\n\n# ============================================================================\n# CH·∫†Y SCRIPT\n# ============================================================================\n\nif __name__ == \"__main__\":\n    model, history, results = main()\n\n",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\nGraph Construction for Network Traffic Data\nX√¢y d·ª±ng ƒë·ªì th·ªã t·ª´ d·ªØ li·ªáu network traffic ƒë·ªÉ s·ª≠ d·ª•ng v·ªõi GNN\nOptimized for Kaggle with Garbage Collection\n\"\"\"\n\nimport numpy as np\nimport torch\nfrom torch_geometric.data import Data, InMemoryDataset\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport pickle\nimport os\nfrom tqdm import tqdm\nimport gc  # Th√™m garbage collection\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\nPROCESSED_DATA_DIR = \"/kaggle/input/preprocess-data/processed_data\"  # Dataset ƒë√£ upload\nGRAPH_DATA_DIR = \"/kaggle/working\"  # Output directory\n\n# Graph construction parameters\nK_NEIGHBORS = 8  # S·ªë l∆∞·ª£ng neighbors cho KNN graph\nSIMILARITY_THRESHOLD = 0.5  # Ng∆∞·ª°ng similarity ƒë·ªÉ t·∫°o edge\nGRAPH_TYPE = 'knn'  # 'knn' ho·∫∑c 'similarity'\nMAX_SAMPLES = 2000000  # Gi·ªõi h·∫°n s·ªë samples ƒë·ªÉ training nhanh\n\n# ============================================================================\n# GRAPH CONSTRUCTION CLASS\n# ============================================================================\n\nclass NetworkTrafficGraphBuilder:\n    \"\"\"Class ƒë·ªÉ x√¢y d·ª±ng ƒë·ªì th·ªã t·ª´ network traffic data\"\"\"\n\n    def __init__(self, k_neighbors=10, similarity_threshold=0.5, graph_type='knn'):\n        self.k_neighbors = k_neighbors\n        self.similarity_threshold = similarity_threshold\n        self.graph_type = graph_type\n\n    def build_knn_graph(self, X, batch_size=10000):\n        \"\"\"\n        X√¢y d·ª±ng KNN graph t·ª´ features\n\n        Args:\n            X: Feature matrix (n_samples, n_features)\n            batch_size: Batch size ƒë·ªÉ x·ª≠ l√Ω (tr√°nh memory overflow)\n\n        Returns:\n            edge_index: Edge indices (2, num_edges)\n        \"\"\"\n        print(f\"\\nX√¢y d·ª±ng KNN graph v·ªõi k={self.k_neighbors}...\")\n\n        n_samples = X.shape[0]\n\n        if n_samples <= batch_size:\n            # X·ª≠ l√Ω tr·ª±c ti·∫øp n·∫øu d·ªØ li·ªáu nh·ªè\n            adjacency = kneighbors_graph(\n                X,\n                n_neighbors=self.k_neighbors,\n                mode='connectivity',\n                include_self=False\n            )\n            \n            # GC sau khi t·∫°o adjacency matrix\n            gc.collect()\n            \n        else:\n            # X·ª≠ l√Ω theo batch n·∫øu d·ªØ li·ªáu l·ªõn - t·∫°o edge list thay v√¨ adjacency matrix\n            print(f\"  D·ªØ li·ªáu l·ªõn, x·ª≠ l√Ω theo batch ({batch_size} samples/batch)...\")\n            edges = []\n\n            # Fit NearestNeighbors m·ªôt l·∫ßn cho to√†n b·ªô dataset\n            from sklearn.neighbors import NearestNeighbors\n            print(\"  Fitting NearestNeighbors model...\")\n            nbrs = NearestNeighbors(n_neighbors=self.k_neighbors + 1, algorithm='auto').fit(X)\n            \n            # GC sau khi fit model\n            gc.collect()\n\n            for i in tqdm(range(0, n_samples, batch_size), desc=\"  Building KNN graph\"):\n                end_idx = min(i + batch_size, n_samples)\n                batch_X = X[i:end_idx]\n\n                # T√¨m k nearest neighbors\n                distances, indices = nbrs.kneighbors(batch_X)\n\n                # T·∫°o edges (b·ªè qua neighbor ƒë·∫ßu ti√™n v√¨ ƒë√≥ l√† ch√≠nh n√≥)\n                for local_idx in range(len(batch_X)):\n                    global_idx = i + local_idx\n                    for neighbor_idx in indices[local_idx][1:]:  # Skip first (itself)\n                        edges.append([global_idx, neighbor_idx])\n\n                # GC sau m·ªói 10 batches\n                if (i // batch_size) % 10 == 0:\n                    gc.collect()\n\n            # Chuy·ªÉn edge list sang tensor\n            print(\"  Converting edges to tensor...\")\n            edges = np.array(edges).T\n            edge_index = torch.tensor(edges, dtype=torch.long)\n            \n            # GC sau khi t·∫°o tensor\n            del edges\n            gc.collect()\n            \n            print(f\"‚úì Graph created: {n_samples} nodes, {edge_index.shape[1]} edges\")\n            return edge_index\n\n        # Chuy·ªÉn sang edge_index format (cho tr∆∞·ªùng h·ª£p kh√¥ng batch)\n        adjacency = adjacency.tocoo()\n        edge_index = torch.tensor(\n            np.vstack([adjacency.row, adjacency.col]),\n            dtype=torch.long\n        )\n        \n        # GC sau khi t·∫°o edge_index\n        del adjacency\n        gc.collect()\n\n        print(f\"‚úì Graph created: {n_samples} nodes, {edge_index.shape[1]} edges\")\n\n        return edge_index\n\n    def build_similarity_graph(self, X, batch_size=1000):\n        \"\"\"\n        X√¢y d·ª±ng graph d·ª±a tr√™n cosine similarity\n\n        Args:\n            X: Feature matrix\n            batch_size: Batch size\n\n        Returns:\n            edge_index: Edge indices\n        \"\"\"\n        print(f\"\\nX√¢y d·ª±ng Similarity graph (threshold={self.similarity_threshold})...\")\n\n        n_samples = X.shape[0]\n        edges = []\n\n        # X·ª≠ l√Ω theo batch\n        for i in tqdm(range(0, n_samples, batch_size), desc=\"  Computing similarity\"):\n            end_idx = min(i + batch_size, n_samples)\n\n            # T√≠nh similarity cho batch hi·ªán t·∫°i v·ªõi t·∫•t c·∫£ samples\n            similarities = cosine_similarity(X[i:end_idx], X)\n\n            # T√¨m c√°c edges c√≥ similarity > threshold\n            for local_idx in range(end_idx - i):\n                global_idx = i + local_idx\n                # L·∫•y indices c√≥ similarity > threshold (kh√¥ng bao g·ªìm ch√≠nh n√≥)\n                similar_indices = np.where(\n                    (similarities[local_idx] > self.similarity_threshold) &\n                    (np.arange(n_samples) != global_idx)\n                )[0]\n\n                # Th√™m edges\n                for j in similar_indices:\n                    edges.append([global_idx, j])\n\n            # GC sau m·ªói batch ƒë·ªÉ gi·∫£i ph√≥ng similarity matrix\n            del similarities\n            if i % (batch_size * 10) == 0:\n                gc.collect()\n\n        if len(edges) == 0:\n            print(\"  ‚ö† Kh√¥ng t√¨m th·∫•y edges, gi·∫£m threshold ho·∫∑c d√πng KNN graph\")\n            # Fallback to KNN\n            return self.build_knn_graph(X, batch_size)\n\n        # Chuy·ªÉn sang tensor\n        print(\"  Converting edges to tensor...\")\n        edge_index = torch.tensor(np.array(edges).T, dtype=torch.long)\n        \n        # GC sau khi t·∫°o tensor\n        del edges\n        gc.collect()\n\n        print(f\"‚úì Graph created: {n_samples} nodes, {edge_index.shape[1]} edges\")\n\n        return edge_index\n\n    def create_graph_data(self, X, y, edge_index):\n        \"\"\"\n        T·∫°o PyTorch Geometric Data object\n\n        Args:\n            X: Node features\n            y: Node labels\n            edge_index: Edge indices\n\n        Returns:\n            Data object\n        \"\"\"\n        # Chuy·ªÉn ƒë·ªïi sang tensor\n        x = torch.tensor(X, dtype=torch.float)\n        y = torch.tensor(y, dtype=torch.long)\n\n        # T·∫°o Data object\n        data = Data(x=x, edge_index=edge_index, y=y)\n        \n        # GC sau khi t·∫°o Data object\n        gc.collect()\n\n        return data\n\n\nclass CICIDSGraphDataset(InMemoryDataset):\n    \"\"\"Custom Dataset cho CICIDS2018 Graph Data\"\"\"\n\n    def __init__(self, root, data_list=None, transform=None, pre_transform=None):\n        self.data_list = data_list\n        super().__init__(root, transform, pre_transform)\n        self.data, self.slices = torch.load(self.processed_paths[0])\n\n    @property\n    def raw_file_names(self):\n        return []\n\n    @property\n    def processed_file_names(self):\n        return ['data.pt']\n\n    def download(self):\n        pass\n\n    def process(self):\n        if self.data_list is not None:\n            data_list = self.data_list\n\n            if self.pre_filter is not None:\n                data_list = [d for d in data_list if self.pre_filter(d)]\n\n            if self.pre_transform is not None:\n                data_list = [self.pre_transform(d) for d in data_list]\n\n            data, slices = self.collate(data_list)\n            torch.save((data, slices), self.processed_paths[0])\n\n\n# ============================================================================\n# MAIN FUNCTION\n# ============================================================================\n\ndef build_graph_dataset():\n    \"\"\"X√¢y d·ª±ng graph dataset t·ª´ processed data\"\"\"\n\n    print(\"=\" * 80)\n    print(\"BUILDING GRAPH DATASET FOR GNN\")\n    print(\"=\" * 80)\n\n    # T·∫°o output directory\n    os.makedirs(GRAPH_DATA_DIR, exist_ok=True)\n\n    # Load processed data\n    print(\"\\nLoading processed data...\")\n    X = np.load(os.path.join(PROCESSED_DATA_DIR, \"X_features.npy\"))\n    y_binary = np.load(os.path.join(PROCESSED_DATA_DIR, \"y_binary.npy\"))\n    y_multi = np.load(os.path.join(PROCESSED_DATA_DIR, \"y_multi.npy\"))\n\n    with open(os.path.join(PROCESSED_DATA_DIR, \"metadata.pkl\"), 'rb') as f:\n        metadata = pickle.load(f)\n\n    print(f\"‚úì Loaded data: {X.shape[0]:,} samples, {X.shape[1]} features\")\n    print(f\"‚úì Binary classes: {len(np.unique(y_binary))}\")\n    print(f\"‚úì Multi classes: {len(np.unique(y_multi))}\")\n    \n    # GC sau khi load data\n    gc.collect()\n\n    # Gi·∫£m k√≠ch th∆∞·ªõc n·∫øu qu√° l·ªõn (optional - ƒë·ªÉ training nhanh h∆°n)\n    if X.shape[0] > MAX_SAMPLES:\n        print(f\"\\n‚ö† Dataset l·ªõn ({X.shape[0]:,} samples), sampling {MAX_SAMPLES:,} samples...\")\n        indices = np.random.choice(X.shape[0], MAX_SAMPLES, replace=False)\n        X = X[indices]\n        y_binary = y_binary[indices]\n        y_multi = y_multi[indices]\n        print(f\"‚úì Sampled data: {X.shape[0]:,} samples\")\n        \n        # GC sau khi sampling\n        del indices\n        gc.collect()\n\n    # Build graph\n    builder = NetworkTrafficGraphBuilder(\n        k_neighbors=K_NEIGHBORS,\n        similarity_threshold=SIMILARITY_THRESHOLD,\n        graph_type=GRAPH_TYPE\n    )\n\n    if GRAPH_TYPE == 'knn':\n        edge_index = builder.build_knn_graph(X)\n    else:\n        edge_index = builder.build_similarity_graph(X)\n    \n    # GC sau khi build graph\n    gc.collect()\n\n    # Create graph data objects\n    print(\"\\nCreating graph data objects...\")\n\n    # Binary classification graph\n    print(\"  Creating binary classification graph...\")\n    graph_binary = builder.create_graph_data(X, y_binary, edge_index)\n    \n    # GC sau khi t·∫°o binary graph\n    gc.collect()\n\n    # Multi-class classification graph\n    print(\"  Creating multi-class classification graph...\")\n    graph_multi = builder.create_graph_data(X, y_multi, edge_index)\n    \n    # GC sau khi t·∫°o multi graph\n    gc.collect()\n\n    # Save graphs\n    print(\"\\nSaving graph data...\")\n    torch.save(graph_binary, os.path.join(GRAPH_DATA_DIR, \"graph_binary.pt\"))\n    torch.save(graph_multi, os.path.join(GRAPH_DATA_DIR, \"graph_multi.pt\"))\n\n    # Save edge_index separately\n    torch.save(edge_index, os.path.join(GRAPH_DATA_DIR, \"edge_index.pt\"))\n\n    # Save graph metadata\n    graph_metadata = {\n        'n_nodes': X.shape[0],\n        'n_features': X.shape[1],\n        'n_edges': edge_index.shape[1],\n        'k_neighbors': K_NEIGHBORS,\n        'graph_type': GRAPH_TYPE,\n        'avg_degree': edge_index.shape[1] / X.shape[0]\n    }\n\n    with open(os.path.join(GRAPH_DATA_DIR, \"graph_metadata.pkl\"), 'wb') as f:\n        pickle.dump(graph_metadata, f)\n    \n    # GC cu·ªëi c√πng\n    gc.collect()\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"GRAPH CONSTRUCTION COMPLETED!\")\n    print(\"=\" * 80)\n    print(f\"Nodes: {graph_metadata['n_nodes']:,}\")\n    print(f\"Features per node: {graph_metadata['n_features']}\")\n    print(f\"Edges: {graph_metadata['n_edges']:,}\")\n    print(f\"Average degree: {graph_metadata['avg_degree']:.2f}\")\n    print(f\"Output directory: {GRAPH_DATA_DIR}\")\n    print(\"=\" * 80)\n\n    return graph_binary, graph_multi, graph_metadata\n\n\nif __name__ == \"__main__\":\n    graph_binary, graph_multi, metadata = build_graph_dataset()",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\nGNN Training Script for Kaggle - IoT Network Anomaly Detection\nScript ho√†n ch·ªânh bao g·ªìm c·∫£ model definitions v√† training\nCh·ªâ c·∫ßn ch·∫°y file n√†y tr√™n Kaggle\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.nn import GCNConv, GATConv, SAGEConv, BatchNorm\nimport numpy as np\nimport pickle\nimport os\nimport shutil\nfrom datetime import datetime\nfrom sklearn.metrics import (\n    precision_score, recall_score, f1_score,\n    confusion_matrix, classification_report, roc_auc_score\n)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ============================================================================\n# GNN MODEL DEFINITIONS\n# ============================================================================\n\nclass GCN(nn.Module):\n    \"\"\"Graph Convolutional Network\"\"\"\n\n    def __init__(self, in_channels, hidden_channels, num_classes, num_layers=3, dropout=0.5):\n        super(GCN, self).__init__()\n        self.num_layers = num_layers\n        self.dropout = dropout\n\n        self.convs = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n\n        # First layer\n        self.convs.append(GCNConv(in_channels, hidden_channels))\n        self.batch_norms.append(BatchNorm(hidden_channels))\n\n        # Hidden layers\n        for _ in range(num_layers - 2):\n            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n            self.batch_norms.append(BatchNorm(hidden_channels))\n\n        # Output layer\n        self.convs.append(GCNConv(hidden_channels, hidden_channels))\n        self.batch_norms.append(BatchNorm(hidden_channels))\n\n        # Classifier\n        self.classifier = nn.Linear(hidden_channels, num_classes)\n\n    def forward(self, x, edge_index):\n        for i in range(self.num_layers):\n            x = self.convs[i](x, edge_index)\n            x = self.batch_norms[i](x)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.classifier(x)\n        return x\n\n\nclass GAT(nn.Module):\n    \"\"\"Graph Attention Network\"\"\"\n\n    def __init__(self, in_channels, hidden_channels, num_classes,\n                 num_layers=3, heads=4, dropout=0.5):\n        super(GAT, self).__init__()\n        self.num_layers = num_layers\n        self.dropout = dropout\n\n        self.convs = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n\n        # First layer\n        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout))\n        self.batch_norms.append(BatchNorm(hidden_channels * heads))\n\n        # Hidden layers\n        for _ in range(num_layers - 2):\n            self.convs.append(GATConv(hidden_channels * heads, hidden_channels,\n                                     heads=heads, dropout=dropout))\n            self.batch_norms.append(BatchNorm(hidden_channels * heads))\n\n        # Output layer\n        self.convs.append(GATConv(hidden_channels * heads, hidden_channels,\n                                 heads=1, concat=False, dropout=dropout))\n        self.batch_norms.append(BatchNorm(hidden_channels))\n\n        # Classifier\n        self.classifier = nn.Linear(hidden_channels, num_classes)\n\n    def forward(self, x, edge_index):\n        for i in range(self.num_layers):\n            x = self.convs[i](x, edge_index)\n            x = self.batch_norms[i](x)\n            x = F.elu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.classifier(x)\n        return x\n\n\nclass GraphSAGE(nn.Module):\n    \"\"\"GraphSAGE\"\"\"\n\n    def __init__(self, in_channels, hidden_channels, num_classes,\n                 num_layers=3, dropout=0.5, aggregator='mean'):\n        super(GraphSAGE, self).__init__()\n        self.num_layers = num_layers\n        self.dropout = dropout\n\n        self.convs = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n\n        # First layer\n        self.convs.append(SAGEConv(in_channels, hidden_channels, aggr=aggregator))\n        self.batch_norms.append(BatchNorm(hidden_channels))\n\n        # Hidden layers\n        for _ in range(num_layers - 2):\n            self.convs.append(SAGEConv(hidden_channels, hidden_channels, aggr=aggregator))\n            self.batch_norms.append(BatchNorm(hidden_channels))\n\n        # Output layer\n        self.convs.append(SAGEConv(hidden_channels, hidden_channels, aggr=aggregator))\n        self.batch_norms.append(BatchNorm(hidden_channels))\n\n        # Classifier\n        self.classifier = nn.Linear(hidden_channels, num_classes)\n\n    def forward(self, x, edge_index):\n        for i in range(self.num_layers):\n            x = self.convs[i](x, edge_index)\n            x = self.batch_norms[i](x)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.classifier(x)\n        return x\n\n\nclass HybridGNN(nn.Module):\n    \"\"\"Hybrid GNN combining GCN and GAT\"\"\"\n\n    def __init__(self, in_channels, hidden_channels, num_classes,\n                 num_layers=3, heads=4, dropout=0.5):\n        super(HybridGNN, self).__init__()\n        self.num_layers = num_layers\n        self.dropout = dropout\n\n        # GCN branch\n        self.gcn_convs = nn.ModuleList()\n        self.gcn_bns = nn.ModuleList()\n\n        # GAT branch\n        self.gat_convs = nn.ModuleList()\n        self.gat_bns = nn.ModuleList()\n\n        # First layer\n        self.gcn_convs.append(GCNConv(in_channels, hidden_channels))\n        self.gcn_bns.append(BatchNorm(hidden_channels))\n        self.gat_convs.append(GATConv(in_channels, hidden_channels // heads, heads=heads))\n        self.gat_bns.append(BatchNorm(hidden_channels))\n\n        # Hidden layers\n        for _ in range(num_layers - 2):\n            self.gcn_convs.append(GCNConv(hidden_channels, hidden_channels))\n            self.gcn_bns.append(BatchNorm(hidden_channels))\n            self.gat_convs.append(GATConv(hidden_channels, hidden_channels // heads, heads=heads))\n            self.gat_bns.append(BatchNorm(hidden_channels))\n\n        # Fusion\n        self.fusion = nn.Linear(hidden_channels * 2, hidden_channels)\n        self.fusion_bn = BatchNorm(hidden_channels)\n\n        # Classifier\n        self.classifier = nn.Linear(hidden_channels, num_classes)\n\n    def forward(self, x, edge_index):\n        # GCN branch\n        x_gcn = x\n        for i in range(self.num_layers - 1):\n            x_gcn = self.gcn_convs[i](x_gcn, edge_index)\n            x_gcn = self.gcn_bns[i](x_gcn)\n            x_gcn = F.relu(x_gcn)\n            x_gcn = F.dropout(x_gcn, p=self.dropout, training=self.training)\n\n        # GAT branch\n        x_gat = x\n        for i in range(self.num_layers - 1):\n            x_gat = self.gat_convs[i](x_gat, edge_index)\n            x_gat = self.gat_bns[i](x_gat)\n            x_gat = F.elu(x_gat)\n            x_gat = F.dropout(x_gat, p=self.dropout, training=self.training)\n\n        # Fusion\n        x = torch.cat([x_gcn, x_gat], dim=1)\n        x = self.fusion(x)\n        x = self.fusion_bn(x)\n        x = F.relu(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.classifier(x)\n        return x\n\n\ndef create_model(model_name, in_channels, hidden_channels, num_classes, **kwargs):\n    \"\"\"Factory function ƒë·ªÉ t·∫°o model\"\"\"\n    models = {\n        'GCN': GCN,\n        'GAT': GAT,\n        'GraphSAGE': GraphSAGE,\n        'Hybrid': HybridGNN\n    }\n    if model_name not in models:\n        raise ValueError(f\"Model {model_name} kh√¥ng h·ªó tr·ª£. Ch·ªçn: {list(models.keys())}\")\n    return models[model_name](in_channels, hidden_channels, num_classes, **kwargs)\n\n\ndef count_parameters(model):\n    \"\"\"ƒê·∫øm s·ªë parameters\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\n# ============================================================================\n# KAGGLE CONFIGURATION\n# ============================================================================\nWORKING_DIR = \"/kaggle/working\"\nGRAPH_DATA_DIR = \"/kaggle/working\"\nMODEL_DIR = os.path.join(WORKING_DIR, \"models\")\nRESULTS_DIR = os.path.join(WORKING_DIR, \"results\")\n\n# Model config\nMODEL_NAME = 'GAT'  # 'GCN', 'GAT', 'GraphSAGE', 'Hybrid'\nHIDDEN_CHANNELS = 128\nNUM_LAYERS = 3\nHEADS = 4\nDROPOUT = 0.3\n\n# Training config\nLEARNING_RATE = 0.001\nWEIGHT_DECAY = 5e-4\nNUM_EPOCHS = 30\nPATIENCE = 15\nTASK = 'binary'  # 'binary' ho·∫∑c 'multi'\n\n# Data split\nTRAIN_RATIO = 0.7\nVAL_RATIO = 0.15\nTEST_RATIO = 0.15\n\n# Device\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"üî• Using device: {DEVICE}\")\n\n# ============================================================================\n# TRAINING CLASS\n# ============================================================================\n\nclass GNNTrainer:\n    \"\"\"Class ƒë·ªÉ train GNN model\"\"\"\n\n    def __init__(self, model, device, task='binary'):\n        self.model = model.to(device)\n        self.device = device\n        self.task = task\n        self.history = {\n            'train_loss': [],\n            'train_acc': [],\n            'val_loss': [],\n            'val_acc': [],\n            'learning_rate': []\n        }\n        self.best_val_acc = 0\n        self.best_epoch = 0\n\n    def train_epoch(self, data, train_mask, optimizer):\n        \"\"\"Train m·ªôt epoch\"\"\"\n        self.model.train()\n        optimizer.zero_grad()\n\n        out = self.model(data.x, data.edge_index)\n        loss = F.cross_entropy(out[train_mask], data.y[train_mask])\n        loss.backward()\n        optimizer.step()\n\n        pred = out[train_mask].argmax(dim=1)\n        acc = (pred == data.y[train_mask]).float().mean()\n        return loss.item(), acc.item()\n\n    @torch.no_grad()\n    def evaluate(self, data, mask):\n        \"\"\"Evaluate model\"\"\"\n        self.model.eval()\n        out = self.model(data.x, data.edge_index)\n        loss = F.cross_entropy(out[mask], data.y[mask])\n        pred = out[mask].argmax(dim=1)\n        acc = (pred == data.y[mask]).float().mean()\n        return loss.item(), acc.item(), pred.cpu().numpy(), data.y[mask].cpu().numpy()\n\n    def train(self, data, train_mask, val_mask, optimizer, scheduler, num_epochs, patience):\n        \"\"\"Full training loop\"\"\"\n        print(\"\\n\" + \"=\" * 80)\n        print(\"üöÄ TRAINING GNN MODEL\")\n        print(\"=\" * 80)\n        print(f\"üìç Device: {self.device}\")\n        print(f\"üß† Model: {self.model.__class__.__name__}\")\n        print(f\"üìä Parameters: {count_parameters(self.model):,}\")\n        print(f\"‚è∞ Epochs: {num_epochs}\")\n        print(\"=\" * 80 + \"\\n\")\n\n        patience_counter = 0\n\n        for epoch in range(1, num_epochs + 1):\n            train_loss, train_acc = self.train_epoch(data, train_mask, optimizer)\n            val_loss, val_acc, _, _ = self.evaluate(data, val_mask)\n\n            if scheduler is not None:\n                scheduler.step(val_loss)\n\n            self.history['train_loss'].append(train_loss)\n            self.history['train_acc'].append(train_acc)\n            self.history['val_loss'].append(val_loss)\n            self.history['val_acc'].append(val_acc)\n            self.history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n\n            if val_acc > self.best_val_acc:\n                self.best_val_acc = val_acc\n                self.best_epoch = epoch\n                patience_counter = 0\n                self.save_model(os.path.join(MODEL_DIR, f'best_model_{self.task}.pt'))\n            else:\n                patience_counter += 1\n\n            if epoch % 10 == 0 or epoch == 1:\n                print(f\"Epoch {epoch:3d}/{num_epochs} | \"\n                      f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n                      f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n\n            if patience_counter >= patience:\n                print(f\"\\n‚èπÔ∏è  Early stopping at epoch {epoch}\")\n                break\n\n        print(f\"\\n‚úÖ Best Val Acc: {self.best_val_acc:.4f} at epoch {self.best_epoch}\")\n        return self.history\n\n    def test(self, data, test_mask):\n        \"\"\"Test model\"\"\"\n        print(\"\\n\" + \"=\" * 80)\n        print(\"üß™ TESTING MODEL\")\n        print(\"=\" * 80)\n\n        test_loss, test_acc, pred, true = self.evaluate(data, test_mask)\n\n        print(f\"üìä Test Accuracy: {test_acc:.4f}\")\n\n        precision = precision_score(true, pred, average='weighted', zero_division=0)\n        recall = recall_score(true, pred, average='weighted', zero_division=0)\n        f1 = f1_score(true, pred, average='weighted', zero_division=0)\n\n        print(f\"üìä Precision: {precision:.4f}\")\n        print(f\"üìä Recall: {recall:.4f}\")\n        print(f\"üìä F1-Score: {f1:.4f}\")\n\n        # ROC-AUC for binary\n        roc_auc = None\n        if self.task == 'binary':\n            try:\n                self.model.eval()\n                with torch.no_grad():\n                    out = self.model(data.x, data.edge_index)\n                    probs = F.softmax(out[test_mask], dim=1)[:, 1].cpu().numpy()\n                roc_auc = roc_auc_score(true, probs)\n                print(f\"üìä ROC-AUC: {roc_auc:.4f}\")\n            except:\n                pass\n\n        cm = confusion_matrix(true, pred)\n        print(\"\\n\" + \"-\" * 80)\n        print(\"üìã Classification Report:\")\n        print(\"-\" * 80)\n        print(classification_report(true, pred, zero_division=0))\n\n        return {\n            'test_loss': test_loss,\n            'test_acc': test_acc,\n            'precision': precision,\n            'recall': recall,\n            'f1': f1,\n            'roc_auc': roc_auc,\n            'confusion_matrix': cm,\n            'predictions': pred,\n            'true_labels': true\n        }\n\n    def save_model(self, path):\n        \"\"\"Save model\"\"\"\n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'best_val_acc': self.best_val_acc,\n            'best_epoch': self.best_epoch,\n            'history': self.history\n        }, path)\n\n    def load_model(self, path):\n        \"\"\"Load model\"\"\"\n        checkpoint = torch.load(path, map_location=self.device)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.best_val_acc = checkpoint['best_val_acc']\n        self.best_epoch = checkpoint['best_epoch']\n        self.history = checkpoint['history']\n\n\n# ============================================================================\n# VISUALIZATION\n# ============================================================================\n\ndef plot_training_history(history, save_path=None):\n    \"\"\"Plot training history\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss')\n    axes[0].set_title('Training and Validation Loss')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n\n    axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n    axes[1].plot(history['val_acc'], label='Val Accuracy', linewidth=2)\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].set_title('Training and Validation Accuracy')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.close()\n\n\ndef plot_confusion_matrix(cm, class_names=None, save_path=None):\n    \"\"\"Plot confusion matrix\"\"\"\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.tight_layout()\n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.close()\n\n\n# ============================================================================\n# MAIN\n# ============================================================================\n\ndef main():\n    \"\"\"Main training function\"\"\"\n    \n    os.makedirs(MODEL_DIR, exist_ok=True)\n    os.makedirs(RESULTS_DIR, exist_ok=True)\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"ü§ñ GNN TRAINING FOR IoT ANOMALY DETECTION - KAGGLE\")\n    print(\"=\" * 80 + \"\\n\")\n\n    # Load graph\n    print(\"üìÇ Loading graph data...\")\n    graph_file = f\"graph_{TASK}.pt\"\n    graph_path = os.path.join(GRAPH_DATA_DIR, graph_file)\n    \n    if not os.path.exists(graph_path):\n        print(f\"‚ùå ERROR: Graph file not found: {graph_path}\")\n        print(\"üìÅ Available files:\")\n        for f in os.listdir(WORKING_DIR):\n            print(f\"  - {f}\")\n        return\n    \n    data = torch.load(graph_path, weights_only=False)\n    data = data.to(DEVICE)\n\n    print(f\"‚úÖ Graph: {data.num_nodes:,} nodes, {data.num_edges:,} edges\")\n    print(f\"‚úÖ Features: {data.num_features}\")\n    print(f\"‚úÖ Classes: {len(torch.unique(data.y))}\")\n\n    # Load metadata\n    metadata_path = os.path.join(GRAPH_DATA_DIR, \"graph_metadata.pkl\")\n    class_names = None\n    if os.path.exists(metadata_path):\n        with open(metadata_path, 'rb') as f:\n            metadata = pickle.load(f)\n            if TASK in metadata and 'class_names' in metadata[TASK]:\n                class_names = metadata[TASK]['class_names']\n    \n    if class_names is None:\n        class_names = ['Benign', 'Attack'] if TASK == 'binary' else [f'Class_{i}' for i in range(len(torch.unique(data.y)))]\n\n    # Create masks\n    print(\"\\nüìä Creating data splits...\")\n    num_nodes = data.num_nodes\n    indices = torch.randperm(num_nodes)\n\n    train_size = int(num_nodes * TRAIN_RATIO)\n    val_size = int(num_nodes * VAL_RATIO)\n\n    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n\n    train_mask[indices[:train_size]] = True\n    val_mask[indices[train_size:train_size + val_size]] = True\n    test_mask[indices[train_size + val_size:]] = True\n\n    print(f\"‚úÖ Train: {train_mask.sum():,} | Val: {val_mask.sum():,} | Test: {test_mask.sum():,}\")\n\n    # Create model\n    print(f\"\\nüèóÔ∏è  Creating {MODEL_NAME} model...\")\n    num_classes = len(torch.unique(data.y))\n\n    model_kwargs = {'num_layers': NUM_LAYERS, 'dropout': DROPOUT}\n    if MODEL_NAME in ['GAT', 'Hybrid']:\n        model_kwargs['heads'] = HEADS\n\n    model = create_model(\n        MODEL_NAME,\n        in_channels=data.num_features,\n        hidden_channels=HIDDEN_CHANNELS,\n        num_classes=num_classes,\n        **model_kwargs\n    )\n\n    print(f\"‚úÖ Model: {count_parameters(model):,} parameters\")\n\n    # Train\n    trainer = GNNTrainer(model, DEVICE, task=TASK)\n    optimizer = Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n\n    history = trainer.train(data, train_mask, val_mask, optimizer, scheduler, NUM_EPOCHS, PATIENCE)\n\n    # Test\n    print(\"\\nüîÑ Loading best model...\")\n    trainer.load_model(os.path.join(MODEL_DIR, f'best_model_{TASK}.pt'))\n    results = trainer.test(data, test_mask)\n\n    # Save results\n    print(\"\\nüíæ Saving results...\")\n    plot_training_history(history, os.path.join(RESULTS_DIR, f'training_history_{TASK}.png'))\n    plot_confusion_matrix(results['confusion_matrix'], class_names, os.path.join(RESULTS_DIR, f'confusion_matrix_{TASK}.png'))\n\n    with open(os.path.join(RESULTS_DIR, f'results_{TASK}.pkl'), 'wb') as f:\n        pickle.dump(results, f)\n\n    config = {\n        'model_name': MODEL_NAME,\n        'hidden_channels': HIDDEN_CHANNELS,\n        'num_layers': NUM_LAYERS,\n        'dropout': DROPOUT,\n        'best_epoch': trainer.best_epoch,\n        'best_val_acc': trainer.best_val_acc,\n        'test_results': {\n            'accuracy': results['test_acc'],\n            'precision': results['precision'],\n            'recall': results['recall'],\n            'f1': results['f1'],\n            'roc_auc': results['roc_auc']\n        }\n    }\n\n    with open(os.path.join(RESULTS_DIR, f'config_{TASK}.pkl'), 'wb') as f:\n        pickle.dump(config, f)\n\n    # Summary\n    with open(os.path.join(RESULTS_DIR, 'summary.txt'), 'w') as f:\n        f.write(\"=\" * 80 + \"\\n\")\n        f.write(\"TRAINING SUMMARY\\n\")\n        f.write(\"=\" * 80 + \"\\n\\n\")\n        f.write(f\"Model: {MODEL_NAME}\\n\")\n        f.write(f\"Task: {TASK}\\n\")\n        f.write(f\"Best Epoch: {trainer.best_epoch}\\n\")\n        f.write(f\"Best Val Acc: {trainer.best_val_acc:.4f}\\n\\n\")\n        f.write(f\"Test Results:\\n\")\n        f.write(f\"  Accuracy: {results['test_acc']:.4f}\\n\")\n        f.write(f\"  Precision: {results['precision']:.4f}\\n\")\n        f.write(f\"  Recall: {results['recall']:.4f}\\n\")\n        f.write(f\"  F1-Score: {results['f1']:.4f}\\n\")\n        if results['roc_auc']:\n            f.write(f\"  ROC-AUC: {results['roc_auc']:.4f}\\n\")\n\n    # ZIP for download\n    print(\"\\n\" + \"=\" * 80)\n    print(\"üì¶ CREATING ZIP FILES\")\n    print(\"=\" * 80)\n\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    zip_results = f'gnn_results_{TASK}_{timestamp}'\n    zip_models = f'gnn_models_{TASK}_{timestamp}'\n\n    shutil.make_archive(os.path.join(WORKING_DIR, zip_results), 'zip', RESULTS_DIR)\n    shutil.make_archive(os.path.join(WORKING_DIR, zip_models), 'zip', MODEL_DIR)\n\n    print(f\"‚úÖ Results: {zip_results}.zip\")\n    print(f\"‚úÖ Models: {zip_models}.zip\")\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"üéâ TRAINING COMPLETED!\")\n    print(\"=\" * 80)\n    print(f\"üìä Test Accuracy: {results['test_acc']:.4f}\")\n    print(f\"üìä F1-Score: {results['f1']:.4f}\")\n    print(f\"\\nüì• Download: {zip_results}.zip & {zip_models}.zip\")\n    print(\"=\" * 80 + \"\\n\")\n\n\nif __name__ == \"__main__\":\n    main()",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.1.0+cu121.html",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Install PyTorch Geometric\n!pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n!pip install torch-geometric\n\n# Upgrade scikit-learn (n·∫øu c·∫ßn)\n!pip install --upgrade scikit-learn\n\n# Install tqdm\n!pip install tqdm",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\nGNN Training Script - FIXED VERSION with Mini-Batch Support\nKey fixes:\n1. Added NeighborLoader for mini-batch training\n2. Reduced memory footprint significantly\n3. Added gradient accumulation option\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.nn import GCNConv, GATConv, SAGEConv, BatchNorm\n# Removed: from torch_geometric.loader import NeighborLoader\nimport numpy as np\nimport pickle\nimport os\nimport shutil\nfrom datetime import datetime\nfrom sklearn.metrics import (\n    precision_score, recall_score, f1_score,\n    confusion_matrix, classification_report, roc_auc_score\n)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ============================================================================\n# MODEL DEFINITIONS (Same as before - these are fine)\n# ============================================================================\n\nclass GCN(nn.Module):\n    def __init__(self, in_channels, hidden_channels, num_classes, num_layers=3, dropout=0.5):\n        super(GCN, self).__init__()\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.convs = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n\n        self.convs.append(GCNConv(in_channels, hidden_channels))\n        self.batch_norms.append(BatchNorm(hidden_channels))\n\n        for _ in range(num_layers - 2):\n            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n            self.batch_norms.append(BatchNorm(hidden_channels))\n\n        self.convs.append(GCNConv(hidden_channels, hidden_channels))\n        self.batch_norms.append(BatchNorm(hidden_channels))\n        self.classifier = nn.Linear(hidden_channels, num_classes)\n\n    def forward(self, x, edge_index):\n        for i in range(self.num_layers):\n            x = self.convs[i](x, edge_index)\n            x = self.batch_norms[i](x)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.classifier(x)\n        return x\n\n\nclass GAT(nn.Module):\n    def __init__(self, in_channels, hidden_channels, num_classes,\n                 num_layers=3, heads=4, dropout=0.5):\n        super(GAT, self).__init__()\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.convs = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n\n        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout))\n        self.batch_norms.append(BatchNorm(hidden_channels * heads))\n\n        for _ in range(num_layers - 2):\n            self.convs.append(GATConv(hidden_channels * heads, hidden_channels,\n                                     heads=heads, dropout=dropout))\n            self.batch_norms.append(BatchNorm(hidden_channels * heads))\n\n        self.convs.append(GATConv(hidden_channels * heads, hidden_channels,\n                                 heads=1, concat=False, dropout=dropout))\n        self.batch_norms.append(BatchNorm(hidden_channels))\n        self.classifier = nn.Linear(hidden_channels, num_classes)\n\n    def forward(self, x, edge_index):\n        for i in range(self.num_layers):\n            x = self.convs[i](x, edge_index)\n            x = self.batch_norms[i](x)\n            x = F.elu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.classifier(x)\n        return x\n\n\nclass GraphSAGE(nn.Module):\n    def __init__(self, in_channels, hidden_channels, num_classes,\n                 num_layers=3, dropout=0.5, aggregator='mean'):\n        super(GraphSAGE, self).__init__()\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.convs = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n\n        self.convs.append(SAGEConv(in_channels, hidden_channels, aggr=aggregator))\n        self.batch_norms.append(BatchNorm(hidden_channels))\n\n        for _ in range(num_layers - 2):\n            self.convs.append(SAGEConv(hidden_channels, hidden_channels, aggr=aggregator))\n            self.batch_norms.append(BatchNorm(hidden_channels))\n\n        self.convs.append(SAGEConv(hidden_channels, hidden_channels, aggr=aggregator))\n        self.batch_norms.append(BatchNorm(hidden_channels))\n        self.classifier = nn.Linear(hidden_channels, num_classes)\n\n    def forward(self, x, edge_index):\n        for i in range(self.num_layers):\n            x = self.convs[i](x, edge_index)\n            x = self.batch_norms[i](x)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.classifier(x)\n        return x\n\n\ndef create_model(model_name, in_channels, hidden_channels, num_classes, **kwargs):\n    models = {'GCN': GCN, 'GAT': GAT, 'GraphSAGE': GraphSAGE}\n    if model_name not in models:\n        raise ValueError(f\"Model {model_name} not supported. Choose: {list(models.keys())}\")\n    return models[model_name](in_channels, hidden_channels, num_classes, **kwargs)\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\n# ============================================================================\n# CONFIGURATION - OPTIMIZED FOR MEMORY\n# ============================================================================\nWORKING_DIR = \"/kaggle/working\"\nGRAPH_DATA_DIR = \"/kaggle/input/model/pytorch/default/1\"\nMODEL_DIR = os.path.join(WORKING_DIR, \"models\")\nRESULTS_DIR = os.path.join(WORKING_DIR, \"results\")\n\n# Model config - Reduced for memory\nMODEL_NAME = 'GraphSAGE'  # GraphSAGE is most memory-efficient\nHIDDEN_CHANNELS = 64      # Reduced from 128\nNUM_LAYERS = 2            # Reduced from 3\nHEADS = 2                 # Reduced from 4 (for GAT)\nDROPOUT = 0.3\n\n# Training config\nLEARNING_RATE = 0.001\nWEIGHT_DECAY = 5e-4\nNUM_EPOCHS = 30\nPATIENCE = 15\nTASK = 'binary'\n\n# MINI-BATCH CONFIG\nBATCH_SIZE = 2048         # Process nodes in batches\nNUM_WORKERS = 0           # Not used in simple batching\n\n# Gradient accumulation (simulate larger batch)\nACCUMULATION_STEPS = 4\n\n# Data split\nTRAIN_RATIO = 0.7\nVAL_RATIO = 0.15\nTEST_RATIO = 0.15\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"üî• Using device: {DEVICE}\")\n\n# ============================================================================\n# MINI-BATCH TRAINER CLASS - COMPLETELY REWRITTEN\n# ============================================================================\n\nclass MiniBatchGNNTrainer:\n    \"\"\"Memory-efficient mini-batch trainer\"\"\"\n\n    def __init__(self, model, device, task='binary'):\n        self.model = model.to(device)\n        self.device = device\n        self.task = task\n        self.history = {\n            'train_loss': [], 'train_acc': [],\n            'val_loss': [], 'val_acc': [], 'learning_rate': []\n        }\n        self.best_val_acc = 0\n        self.best_epoch = 0\n\n    def train_epoch(self, train_batches, data, optimizer, accumulation_steps=1):\n        \"\"\"Train one epoch with simple batches\"\"\"\n        self.model.train()\n        total_loss = 0\n        total_correct = 0\n        total_samples = 0\n        \n        optimizer.zero_grad()\n        \n        # Move full graph to device once\n        data = data.to(self.device)\n        \n        for i, batch_idx in enumerate(train_batches):\n            batch_idx = batch_idx.to(self.device)\n            \n            # Forward pass on full graph\n            out = self.model(data.x, data.edge_index)\n            \n            # Compute loss only on batch nodes\n            loss = F.cross_entropy(out[batch_idx], data.y[batch_idx])\n            loss = loss / accumulation_steps\n            \n            # Backward pass\n            loss.backward()\n            \n            # Update weights\n            if (i + 1) % accumulation_steps == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n            \n            # Metrics\n            with torch.no_grad():\n                pred = out[batch_idx].argmax(dim=1)\n                total_correct += (pred == data.y[batch_idx]).sum().item()\n                total_loss += loss.item() * accumulation_steps * len(batch_idx)\n                total_samples += len(batch_idx)\n        \n        # Final update\n        if (i + 1) % accumulation_steps != 0:\n            optimizer.step()\n            optimizer.zero_grad()\n        \n        return total_loss / total_samples, total_correct / total_samples\n\n    @torch.no_grad()\n    def evaluate(self, batches, data):\n        \"\"\"Evaluate with simple batches\"\"\"\n        self.model.eval()\n        total_loss = 0\n        total_correct = 0\n        total_samples = 0\n        all_preds = []\n        all_labels = []\n        \n        data = data.to(self.device)\n        \n        for batch_idx in batches:\n            batch_idx = batch_idx.to(self.device)\n            \n            # Forward on full graph\n            out = self.model(data.x, data.edge_index)\n            \n            # Loss and predictions on batch\n            loss = F.cross_entropy(out[batch_idx], data.y[batch_idx])\n            pred = out[batch_idx].argmax(dim=1)\n            \n            total_correct += (pred == data.y[batch_idx]).sum().item()\n            total_loss += loss.item() * len(batch_idx)\n            total_samples += len(batch_idx)\n            \n            all_preds.append(pred.cpu())\n            all_labels.append(data.y[batch_idx].cpu())\n        \n        all_preds = torch.cat(all_preds).numpy()\n        all_labels = torch.cat(all_labels).numpy()\n        \n        return total_loss / total_samples, total_correct / total_samples, all_preds, all_labels\n\n    def train(self, train_batches, val_batches, data, optimizer, scheduler, num_epochs, patience):\n        \"\"\"Full training loop\"\"\"\n        print(\"\\n\" + \"=\" * 80)\n        print(\"üöÄ TRAINING GNN MODEL (BATCHED)\")\n        print(\"=\" * 80)\n        print(f\"üìç Device: {self.device}\")\n        print(f\"üß† Model: {self.model.__class__.__name__}\")\n        print(f\"üìä Parameters: {count_parameters(self.model):,}\")\n        print(f\"üéØ Batch size: {BATCH_SIZE}\")\n        print(f\"‚è∞ Epochs: {num_epochs}\")\n        print(\"=\" * 80 + \"\\n\")\n\n        patience_counter = 0\n\n        for epoch in range(1, num_epochs + 1):\n            train_loss, train_acc = self.train_epoch(train_batches, data, optimizer, ACCUMULATION_STEPS)\n            val_loss, val_acc, _, _ = self.evaluate(val_batches, data)\n\n            if scheduler is not None:\n                scheduler.step(val_loss)\n\n            self.history['train_loss'].append(train_loss)\n            self.history['train_acc'].append(train_acc)\n            self.history['val_loss'].append(val_loss)\n            self.history['val_acc'].append(val_acc)\n            self.history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n\n            if val_acc > self.best_val_acc:\n                self.best_val_acc = val_acc\n                self.best_epoch = epoch\n                patience_counter = 0\n                self.save_model(os.path.join(MODEL_DIR, f'best_model_{self.task}.pt'))\n            else:\n                patience_counter += 1\n\n            if epoch % 5 == 0 or epoch == 1:\n                print(f\"Epoch {epoch:3d}/{num_epochs} | \"\n                      f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n                      f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n\n            if patience_counter >= patience:\n                print(f\"\\n‚èπÔ∏è  Early stopping at epoch {epoch}\")\n                break\n\n        print(f\"\\n‚úÖ Best Val Acc: {self.best_val_acc:.4f} at epoch {self.best_epoch}\")\n        return self.history\n\n    def test(self, test_batches, data):\n        \"\"\"Test model\"\"\"\n        print(\"\\n\" + \"=\" * 80)\n        print(\"üß™ TESTING MODEL\")\n        print(\"=\" * 80)\n\n        test_loss, test_acc, pred, true = self.evaluate(test_batches, data)\n\n        print(f\"üìä Test Accuracy: {test_acc:.4f}\")\n\n        precision = precision_score(true, pred, average='weighted', zero_division=0)\n        recall = recall_score(true, pred, average='weighted', zero_division=0)\n        f1 = f1_score(true, pred, average='weighted', zero_division=0)\n\n        print(f\"üìä Precision: {precision:.4f}\")\n        print(f\"üìä Recall: {recall:.4f}\")\n        print(f\"üìä F1-Score: {f1:.4f}\")\n\n        cm = confusion_matrix(true, pred)\n        print(\"\\n\" + \"-\" * 80)\n        print(\"üìã Classification Report:\")\n        print(\"-\" * 80)\n        print(classification_report(true, pred, zero_division=0))\n\n        return {\n            'test_loss': test_loss, 'test_acc': test_acc,\n            'precision': precision, 'recall': recall, 'f1': f1,\n            'confusion_matrix': cm, 'predictions': pred, 'true_labels': true\n        }\n\n    def save_model(self, path):\n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'best_val_acc': self.best_val_acc,\n            'best_epoch': self.best_epoch,\n            'history': self.history\n        }, path)\n\n    def load_model(self, path):\n        checkpoint = torch.load(path, map_location=self.device)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.best_val_acc = checkpoint['best_val_acc']\n        self.best_epoch = checkpoint['best_epoch']\n        self.history = checkpoint['history']\n\n\n# ============================================================================\n# VISUALIZATION (Same as before)\n# ============================================================================\n\ndef plot_training_history(history, save_path=None):\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss')\n    axes[0].set_title('Training and Validation Loss')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n\n    axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n    axes[1].plot(history['val_acc'], label='Val Accuracy', linewidth=2)\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].set_title('Training and Validation Accuracy')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.close()\n\n\ndef plot_confusion_matrix(cm, class_names=None, save_path=None):\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.tight_layout()\n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.close()\n\n\n# ============================================================================\n# MAIN - REWRITTEN WITH MINI-BATCH LOADERS\n# ============================================================================\n\ndef main():\n    os.makedirs(MODEL_DIR, exist_ok=True)\n    os.makedirs(RESULTS_DIR, exist_ok=True)\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"ü§ñ GNN TRAINING (MEMORY-OPTIMIZED)\")\n    print(\"=\" * 80 + \"\\n\")\n\n    # Load graph\n    print(\"üìÇ Loading graph data...\")\n    graph_file = f\"graph_{TASK}.pt\"\n    graph_path = os.path.join(GRAPH_DATA_DIR, graph_file)\n    \n    if not os.path.exists(graph_path):\n        print(f\"‚ùå ERROR: Graph file not found: {graph_path}\")\n        return\n    \n    data = torch.load(graph_path, weights_only=False)\n\n    print(f\"‚úÖ Graph: {data.num_nodes:,} nodes, {data.num_edges:,} edges\")\n\n    # Create masks\n    print(\"\\nüìä Creating data splits...\")\n    num_nodes = data.num_nodes\n    indices = torch.randperm(num_nodes)\n\n    train_size = int(num_nodes * TRAIN_RATIO)\n    val_size = int(num_nodes * VAL_RATIO)\n\n    train_idx = indices[:train_size]\n    val_idx = indices[train_size:train_size + val_size]\n    test_idx = indices[train_size + val_size:]\n\n    print(f\"‚úÖ Train: {len(train_idx):,} | Val: {len(val_idx):,} | Test: {len(test_idx):,}\")\n\n    # Create simple batched indices (no neighbor sampling)\n    print(f\"\\nüîß Creating simple batched processing...\")\n    \n    def create_simple_batches(indices, batch_size):\n        \"\"\"Create simple batches without neighbor sampling\"\"\"\n        return [indices[i:i+batch_size] for i in range(0, len(indices), batch_size)]\n    \n    train_batches = create_simple_batches(train_idx, BATCH_SIZE)\n    val_batches = create_simple_batches(val_idx, BATCH_SIZE)\n    test_batches = create_simple_batches(test_idx, BATCH_SIZE)\n    \n    print(f\"‚úÖ Train batches: {len(train_batches)}\")\n    print(f\"‚úÖ Val batches: {len(val_batches)}\")\n    print(f\"‚úÖ Test batches: {len(test_batches)}\")\n\n    # Create model\n    print(f\"\\nüèóÔ∏è  Creating {MODEL_NAME} model...\")\n    num_classes = len(torch.unique(data.y))\n\n    model_kwargs = {'num_layers': NUM_LAYERS, 'dropout': DROPOUT}\n    if MODEL_NAME == 'GAT':\n        model_kwargs['heads'] = HEADS\n\n    model = create_model(\n        MODEL_NAME,\n        in_channels=data.num_features,\n        hidden_channels=HIDDEN_CHANNELS,\n        num_classes=num_classes,\n        **model_kwargs\n    )\n\n    print(f\"‚úÖ Model: {count_parameters(model):,} parameters\")\n\n    # Train\n    trainer = MiniBatchGNNTrainer(model, DEVICE, task=TASK)\n    optimizer = Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n\n    history = trainer.train(train_batches, val_batches, data, optimizer, scheduler, NUM_EPOCHS, PATIENCE)\n\n    # Test\n    print(\"\\nüîÑ Loading best model...\")\n    trainer.load_model(os.path.join(MODEL_DIR, f'best_model_{TASK}.pt'))\n    results = trainer.test(test_batches, data)\n\n    # Save results\n    print(\"\\nüíæ Saving results...\")\n    plot_training_history(history, os.path.join(RESULTS_DIR, f'training_history_{TASK}.png'))\n    plot_confusion_matrix(results['confusion_matrix'], ['Benign', 'Attack'], \n                          os.path.join(RESULTS_DIR, f'confusion_matrix_{TASK}.png'))\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"üéâ TRAINING COMPLETED!\")\n    print(\"=\" * 80)\n    print(f\"üìä Test Accuracy: {results['test_acc']:.4f}\")\n    print(f\"üìä F1-Score: {results['f1']:.4f}\")\n    print(\"=\" * 80 + \"\\n\")\n\n\nif __name__ == \"__main__\":\n    main()",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!pip install -q torch-sparse",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}

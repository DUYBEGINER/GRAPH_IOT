{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14307565,"sourceType":"datasetVersion","datasetId":9133596},{"sourceId":672505,"sourceType":"modelInstanceVersion","modelInstanceId":509533,"modelId":524200}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\n======================================================================================\nBÆ¯á»šC 1: CLEAN VÃ€ TIá»€N Xá»¬ LÃ DATASET CICIDS2018 CHO MÃ” HÃŒNH CNN\n======================================================================================\n\nScript nÃ y thá»±c hiá»‡n cÃ¡c bÆ°á»›c tiá»n xá»­ lÃ½ dá»¯ liá»‡u:\n1. Äá»c tá»«ng file CSV theo chunks Ä‘á»ƒ tá»‘i Æ°u bá»™ nhá»›\n2. Loáº¡i bá» cÃ¡c cá»™t khÃ´ng cáº§n thiáº¿t (IP, Port, Timestamp, Flow ID)\n3. Loáº¡i bá» cÃ¡c cá»™t cÃ³ variance = 0 (zero-variance columns)\n4. Xá»­ lÃ½ Infinity vÃ  NaN báº±ng Mode cá»§a cá»™t\n5. Loáº¡i bá» cÃ¡c hÃ ng trÃ¹ng láº·p\n6. Chuyá»ƒn Ä‘á»•i nhÃ£n sang dáº¡ng binary (Benign=0, Attack=1)\n7. LÆ°u dá»¯ liá»‡u Ä‘Ã£ clean vÃ o folder Ä‘á»ƒ sá»­ dá»¥ng sau\n\nCÃ³ thá»ƒ cháº¡y trÃªn cáº£ Kaggle vÃ  Local\n\"\"\"\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport json\nimport gc\nfrom pathlib import Path\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Kiá»ƒm tra mÃ´i trÆ°á»ng cháº¡y (Kaggle hoáº·c Local)\nIS_KAGGLE = os.path.exists('/kaggle/input')\n\n# Progress bar\ntry:\n    from tqdm import tqdm\n    TQDM_AVAILABLE = True\nexcept ImportError:\n    TQDM_AVAILABLE = False\n    print(\"âš ï¸  tqdm khÃ´ng cÃ³ sáºµn. CÃ i Ä‘áº·t báº±ng: pip install tqdm\")\n    tqdm = lambda x, **kwargs: x\n\n# ============================================================================\n# Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN\n# ============================================================================\nif IS_KAGGLE:\n    # ÄÆ°á»ng dáº«n trÃªn Kaggle - thay Ä‘á»•i theo dataset cá»§a báº¡n\n    DATA_DIR = \"/kaggle/input/cicids2018\"  # Thay Ä‘á»•i náº¿u tÃªn dataset khÃ¡c\n    OUTPUT_DIR = \"/kaggle/working/cleaned_data\"\n    print(\"ğŸŒ Äang cháº¡y trÃªn KAGGLE\")\nelse:\n    # ÄÆ°á»ng dáº«n Local\n    DATA_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CICIDS2018-CSV\"\n    OUTPUT_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CNN\\cleaned_data\"\n    print(\"ğŸ’» Äang cháº¡y trÃªn LOCAL\")\n\n# ============================================================================\n# Cáº¤U HÃŒNH Xá»¬ LÃ Dá»® LIá»†U\n# ============================================================================\n\n# KÃ­ch thÆ°á»›c chunk khi Ä‘á»c CSV (Ä‘iá»u chá»‰nh theo RAM cá»§a mÃ¡y)\nCHUNK_SIZE = 300000  # 300k rows má»—i chunk\n\n# Random state Ä‘á»ƒ tÃ¡i táº¡o káº¿t quáº£\nRANDOM_STATE = 42\n\n# ============================================================================\n# DANH SÃCH CÃC Cá»˜T Cáº¦N LOáº I Bá» (Identification columns)\n# ============================================================================\n\nCOLUMNS_TO_DROP = [\n    'Flow ID',          # ID duy nháº¥t cho má»—i flow - khÃ´ng cÃ³ Ã½ nghÄ©a phÃ¢n loáº¡i\n    'Src IP',           # IP nguá»“n - khÃ´ng tá»•ng quÃ¡t\n    'Dst IP',           # IP Ä‘Ã­ch - khÃ´ng tá»•ng quÃ¡t\n    'Src Port',         # Port nguá»“n - cÃ³ thá»ƒ bá»‹ overfitting\n    'Dst Port',         # Port Ä‘Ã­ch - cÃ³ thá»ƒ bá»‹ overfitting\n    'Timestamp',        # Thá»i gian - khÃ´ng liÃªn quan Ä‘áº¿n pattern táº¥n cÃ´ng\n]\n\n# Cá»™t nhÃ£n\nLABEL_COLUMN = 'Label'\n\n# ============================================================================\n# CLASS Xá»¬ LÃ Dá»® LIá»†U\n# ============================================================================\n\nclass CICIDS2018_DataCleaner:\n    \"\"\"\n    Class clean dá»¯ liá»‡u CICIDS2018 cho mÃ´ hÃ¬nh CNN\n\n    CÃ¡c bÆ°á»›c xá»­ lÃ½:\n    1. Äá»c dá»¯ liá»‡u theo chunks\n    2. Loáº¡i bá» cá»™t identification\n    3. Loáº¡i bá» zero-variance columns\n    4. Xá»­ lÃ½ Infinity vÃ  NaN báº±ng Mode\n    5. Loáº¡i bá» duplicate\n    6. Chuyá»ƒn Ä‘á»•i nhÃ£n sang binary\n    7. LÆ°u dá»¯ liá»‡u Ä‘Ã£ clean\n    \"\"\"\n\n    def __init__(self, data_dir, output_dir, chunk_size=CHUNK_SIZE):\n        \"\"\"\n        Khá»Ÿi táº¡o data cleaner\n\n        Args:\n            data_dir: ÄÆ°á»ng dáº«n thÆ° má»¥c chá»©a file CSV\n            output_dir: ÄÆ°á»ng dáº«n thÆ° má»¥c lÆ°u káº¿t quáº£\n            chunk_size: Sá»‘ dÃ²ng má»—i chunk khi Ä‘á»c CSV\n        \"\"\"\n        self.data_dir = Path(data_dir)\n        self.output_dir = Path(output_dir)\n        self.chunk_size = chunk_size\n\n        # Táº¡o thÆ° má»¥c output náº¿u chÆ°a cÃ³\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n        # LÆ°u tÃªn cÃ¡c features vÃ  thÃ´ng tin xá»­ lÃ½\n        self.feature_names = None\n        self.zero_variance_cols = []\n        self.column_modes = {}  # LÆ°u mode cá»§a tá»«ng cá»™t Ä‘á»ƒ xá»­ lÃ½ NaN/Inf\n\n        # Thá»‘ng kÃª\n        self.stats = {\n            'total_rows_read': 0,\n            'rows_after_cleaning': 0,\n            'duplicates_removed': 0,\n            'nan_replaced': 0,\n            'inf_replaced': 0,\n            'zero_variance_cols_removed': 0,\n            'benign_count': 0,\n            'attack_count': 0,\n            'feature_count': 0,\n            'processing_time': 0.0  # Float Ä‘á»ƒ lÆ°u thá»i gian xá»­ lÃ½ (giÃ¢y)\n        }\n\n    def _get_csv_files(self):\n        \"\"\"Láº¥y danh sÃ¡ch cÃ¡c file CSV trong thÆ° má»¥c data\"\"\"\n        csv_files = list(self.data_dir.glob(\"*_TrafficForML_CICFlowMeter.csv\"))\n        if not csv_files:\n            # Thá»­ pattern khÃ¡c cho Kaggle\n            csv_files = list(self.data_dir.glob(\"*.csv\"))\n            # Loáº¡i bá» file zip náº¿u cÃ³\n            csv_files = [f for f in csv_files if not f.name.endswith('.zip')]\n\n        if not csv_files:\n            raise FileNotFoundError(f\"KhÃ´ng tÃ¬m tháº¥y file CSV trong {self.data_dir}\")\n\n        print(f\"\\nğŸ“‚ TÃ¬m tháº¥y {len(csv_files)} file CSV:\")\n        for f in sorted(csv_files):\n            print(f\"   - {f.name}\")\n        return sorted(csv_files)\n\n    def _clean_column_names(self, df):\n        \"\"\"Chuáº©n hÃ³a tÃªn cá»™t (loáº¡i bá» khoáº£ng tráº¯ng thá»«a)\"\"\"\n        df.columns = df.columns.str.strip()\n        return df\n\n    def _drop_identification_columns(self, df):\n        \"\"\"Loáº¡i bá» cÃ¡c cá»™t identification khÃ´ng cáº§n thiáº¿t cho huáº¥n luyá»‡n\"\"\"\n        columns_to_drop = [col for col in COLUMNS_TO_DROP if col in df.columns]\n        if columns_to_drop:\n            df = df.drop(columns=columns_to_drop)\n        return df\n\n    def _convert_to_numeric(self, df):\n        \"\"\"Chuyá»ƒn Ä‘á»•i cÃ¡c cá»™t vá» dáº¡ng sá»‘\"\"\"\n        feature_cols = [col for col in df.columns if col != LABEL_COLUMN]\n        for col in feature_cols:\n            if df[col].dtype == 'object':\n                df[col] = pd.to_numeric(df[col], errors='coerce')\n        return df\n\n    def _convert_to_binary_label(self, df):\n        \"\"\"\n        Chuyá»ƒn Ä‘á»•i nhÃ£n sang dáº¡ng binary:\n        - Benign -> 0 (lÆ°u lÆ°á»£ng bÃ¬nh thÆ°á»ng)\n        - Táº¥t cáº£ cÃ¡c loáº¡i táº¥n cÃ´ng khÃ¡c -> 1 (lÆ°u lÆ°á»£ng báº¥t thÆ°á»ng)\n        \"\"\"\n        if LABEL_COLUMN not in df.columns:\n            raise ValueError(f\"KhÃ´ng tÃ¬m tháº¥y cá»™t '{LABEL_COLUMN}' trong dá»¯ liá»‡u\")\n\n        # Chuáº©n hÃ³a nhÃ£n (loáº¡i bá» khoáº£ng tráº¯ng, lowercase)\n        df[LABEL_COLUMN] = df[LABEL_COLUMN].astype(str).str.strip().str.lower()\n\n        # Loáº¡i bá» cÃ¡c hÃ ng cÃ³ nhÃ£n lÃ  'label' (header bá»‹ láº«n vÃ o data)\n        df = df[df[LABEL_COLUMN] != 'label']\n\n        # Chuyá»ƒn Ä‘á»•i sang binary: Benign=0, Attack=1\n        df['binary_label'] = (df[LABEL_COLUMN] != 'benign').astype(int)\n\n        # XÃ³a cá»™t Label gá»‘c, giá»¯ láº¡i binary_label\n        df = df.drop(columns=[LABEL_COLUMN])\n\n        return df\n\n    def _first_pass_collect_info(self, csv_files):\n        \"\"\"\n        Láº§n Ä‘á»c Ä‘áº§u tiÃªn: Thu tháº­p thÃ´ng tin vá» columns vÃ  tÃ­nh mode\n\n        Má»¥c Ä‘Ã­ch:\n        - XÃ¡c Ä‘á»‹nh cÃ¡c cá»™t cÃ³ variance = 0\n        - TÃ­nh mode cá»§a tá»«ng cá»™t Ä‘á»ƒ thay tháº¿ NaN/Inf\n        \"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\"ğŸ“Š BÆ¯á»šC 1: THU THáº¬P THÃ”NG TIN Tá»ª Dá»® LIá»†U\")\n        print(\"=\"*80)\n\n        all_columns = None\n        column_value_counts = {}  # Äá»ƒ tÃ­nh mode\n        column_min_max = {}  # Äá»ƒ kiá»ƒm tra variance\n\n        for csv_file in csv_files:\n            print(f\"\\n   Äang scan: {csv_file.name}\")\n            chunk_iterator = pd.read_csv(csv_file, chunksize=self.chunk_size,\n                                        low_memory=False, encoding='utf-8')\n\n            for chunk in chunk_iterator:\n                chunk = self._clean_column_names(chunk)\n                chunk = self._drop_identification_columns(chunk)\n                chunk = self._convert_to_numeric(chunk)\n\n                if all_columns is None:\n                    all_columns = [col for col in chunk.columns if col != LABEL_COLUMN]\n                    for col in all_columns:\n                        column_value_counts[col] = {}\n                        column_min_max[col] = {'min': np.inf, 'max': -np.inf}\n\n                # Thu tháº­p thÃ´ng tin cho má»—i cá»™t\n                for col in all_columns:\n                    if col in chunk.columns:\n                        # Thay tháº¿ inf trÆ°á»›c khi tÃ­nh\n                        col_data = chunk[col].replace([np.inf, -np.inf], np.nan)\n                        valid_data = col_data.dropna()\n\n                        if len(valid_data) > 0:\n                            # Cáº­p nháº­t min/max\n                            col_min = valid_data.min()\n                            col_max = valid_data.max()\n                            column_min_max[col]['min'] = min(column_min_max[col]['min'], col_min)\n                            column_min_max[col]['max'] = max(column_min_max[col]['max'], col_max)\n\n                            # Thu tháº­p value counts cho mode (láº¥y top 10 Ä‘á»ƒ tiáº¿t kiá»‡m bá»™ nhá»›)\n                            vc = valid_data.value_counts().head(10).to_dict()\n                            for val, count in vc.items():\n                                if val not in column_value_counts[col]:\n                                    column_value_counts[col][val] = 0\n                                column_value_counts[col][val] += count\n\n                gc.collect()\n\n        # XÃ¡c Ä‘á»‹nh zero-variance columns\n        print(\"\\n   Äang phÃ¢n tÃ­ch variance cá»§a cÃ¡c cá»™t...\")\n        for col in all_columns:\n            if column_min_max[col]['min'] == column_min_max[col]['max']:\n                self.zero_variance_cols.append(col)\n\n        # TÃ­nh mode cho má»—i cá»™t\n        print(\"   Äang tÃ­nh mode cho má»—i cá»™t...\")\n        for col in all_columns:\n            if col not in self.zero_variance_cols:\n                if column_value_counts[col]:\n                    # Mode lÃ  giÃ¡ trá»‹ xuáº¥t hiá»‡n nhiá»u nháº¥t\n                    mode_val = max(column_value_counts[col], key=column_value_counts[col].get)\n                    self.column_modes[col] = mode_val\n                else:\n                    self.column_modes[col] = 0  # Fallback náº¿u khÃ´ng cÃ³ dá»¯ liá»‡u há»£p lá»‡\n\n        self.stats['zero_variance_cols_removed'] = len(self.zero_variance_cols)\n\n        print(f\"\\n   âœ… Sá»‘ cá»™t zero-variance sáº½ loáº¡i bá»: {len(self.zero_variance_cols)}\")\n        if self.zero_variance_cols:\n            print(f\"      CÃ¡c cá»™t: {self.zero_variance_cols}\")\n        print(f\"   âœ… Sá»‘ cá»™t sáº½ giá»¯ láº¡i: {len(all_columns) - len(self.zero_variance_cols)}\")\n\n        return all_columns\n\n    def _handle_nan_inf_with_mode(self, df):\n        \"\"\"\n        Xá»­ lÃ½ NaN vÃ  Infinity báº±ng Mode cá»§a cá»™t\n\n        Replace Infinity and NaN with the Mode of the column\n        \"\"\"\n        feature_cols = [col for col in df.columns if col != 'binary_label']\n\n        for col in feature_cols:\n            if col in self.column_modes:\n                mode_val = self.column_modes[col]\n\n                # Äáº¿m sá»‘ lÆ°á»£ng inf vÃ  nan\n                inf_mask = np.isinf(df[col])\n                nan_mask = df[col].isna()\n\n                self.stats['inf_replaced'] += inf_mask.sum()\n                self.stats['nan_replaced'] += nan_mask.sum()\n\n                # Thay tháº¿ inf báº±ng nan trÆ°á»›c\n                df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n\n                # Thay tháº¿ táº¥t cáº£ nan báº±ng mode\n                df[col] = df[col].fillna(mode_val)\n\n        return df\n\n    def _drop_zero_variance_columns(self, df):\n        \"\"\"Loáº¡i bá» cÃ¡c cá»™t cÃ³ variance = 0\"\"\"\n        cols_to_drop = [col for col in self.zero_variance_cols if col in df.columns]\n        if cols_to_drop:\n            df = df.drop(columns=cols_to_drop)\n        return df\n\n    def _process_single_file(self, csv_file):\n        \"\"\"\n        Xá»­ lÃ½ má»™t file CSV theo chunks\n\n        Args:\n            csv_file: ÄÆ°á»ng dáº«n file CSV\n\n        Returns:\n            DataFrame Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½\n        \"\"\"\n        print(f\"\\nğŸ“„ Äang xá»­ lÃ½: {csv_file.name}\")\n\n        processed_chunks = []\n        chunk_iterator = pd.read_csv(csv_file, chunksize=self.chunk_size,\n                                     low_memory=False, encoding='utf-8')\n\n        # Progress bar cho chunks\n        if TQDM_AVAILABLE:\n            file_size = csv_file.stat().st_size\n            estimated_chunks = max(1, file_size // (self.chunk_size * 500))\n            chunk_iterator = tqdm(chunk_iterator, desc=\"   Chunks\",\n                                  total=estimated_chunks, unit=\"chunk\")\n\n        for chunk in chunk_iterator:\n            self.stats['total_rows_read'] += len(chunk)\n\n            # BÆ°á»›c 1: Chuáº©n hÃ³a tÃªn cá»™t\n            chunk = self._clean_column_names(chunk)\n\n            # BÆ°á»›c 2: Loáº¡i bá» cá»™t identification\n            chunk = self._drop_identification_columns(chunk)\n\n            # BÆ°á»›c 3: Chuyá»ƒn Ä‘á»•i sang dáº¡ng sá»‘\n            chunk = self._convert_to_numeric(chunk)\n\n            # BÆ°á»›c 4: Chuyá»ƒn Ä‘á»•i nhÃ£n sang binary\n            chunk = self._convert_to_binary_label(chunk)\n\n            # BÆ°á»›c 5: Loáº¡i bá» zero-variance columns\n            chunk = self._drop_zero_variance_columns(chunk)\n\n            # BÆ°á»›c 6: Xá»­ lÃ½ NaN vÃ  Inf báº±ng Mode\n            chunk = self._handle_nan_inf_with_mode(chunk)\n\n            processed_chunks.append(chunk)\n            gc.collect()\n\n        # Gá»™p cÃ¡c chunks láº¡i\n        if processed_chunks:\n            df = pd.concat(processed_chunks, ignore_index=True)\n            del processed_chunks\n            gc.collect()\n            return df\n\n        return None\n\n    def clean_all_files(self):\n        \"\"\"\n        Clean táº¥t cáº£ cÃ¡c file CSV\n\n        Returns:\n            DataFrame Ä‘Ã£ clean hoÃ n chá»‰nh\n        \"\"\"\n        start_time = datetime.now()\n        print(\"\\n\" + \"=\"*80)\n        print(\" Báº®T Äáº¦U CLEAN Dá»® LIá»†U CICIDS2018\")\n        print(\"=\"*80)\n\n        csv_files = self._get_csv_files()\n\n        # BÆ°á»›c 1: Thu tháº­p thÃ´ng tin (mode, zero-variance)\n        all_columns = self._first_pass_collect_info(csv_files)\n\n        # BÆ°á»›c 2: Xá»­ lÃ½ tá»«ng file\n        print(\"\\n\" + \"=\"*80)\n        print(\" BÆ¯á»šC 2: CLEAN Dá»® LIá»†U\")\n        print(\"=\"*80)\n\n        all_dataframes = []\n        for csv_file in csv_files:\n            df = self._process_single_file(csv_file)\n            if df is not None:\n                all_dataframes.append(df)\n                print(f\"   âœ… ÄÃ£ xá»­ lÃ½: {len(df):,} máº«u\")\n\n        # Gá»™p táº¥t cáº£ láº¡i\n        print(\"\\n\" + \"-\"*80)\n        print(\" ÄANG Gá»˜P Dá»® LIá»†U...\")\n\n        df_combined = pd.concat(all_dataframes, ignore_index=True)\n        del all_dataframes\n        gc.collect()\n\n        print(f\"   Tá»•ng sá»‘ máº«u sau khi gá»™p: {len(df_combined):,}\")\n\n        # Loáº¡i bá» duplicate trÃªn toÃ n bá»™ dataset\n        print(\"   Äang loáº¡i bá» duplicate...\")\n        rows_before = len(df_combined)\n        df_combined = df_combined.drop_duplicates()\n        rows_after = len(df_combined)\n        self.stats['duplicates_removed'] = rows_before - rows_after\n        print(f\"   Sá»‘ máº«u sau khi loáº¡i duplicate: {len(df_combined):,}\")\n        print(f\"   Sá»‘ duplicate Ä‘Ã£ loáº¡i: {self.stats['duplicates_removed']:,}\")\n\n        # Äáº¿m sá»‘ lÆ°á»£ng má»—i class\n        self.stats['benign_count'] = int((df_combined['binary_label'] == 0).sum())\n        self.stats['attack_count'] = int((df_combined['binary_label'] == 1).sum())\n\n        # Cáº­p nháº­t thá»‘ng kÃª\n        self.stats['rows_after_cleaning'] = len(df_combined)\n        self.stats['feature_count'] = len(df_combined.columns) - 1  # Trá»« cá»™t label\n\n        # LÆ°u tÃªn features\n        self.feature_names = [col for col in df_combined.columns if col != 'binary_label']\n\n        end_time = datetime.now()\n        self.stats['processing_time'] = (end_time - start_time).total_seconds()\n\n        return df_combined\n\n    def save_cleaned_data(self, df):\n        \"\"\"\n        LÆ°u dá»¯ liá»‡u Ä‘Ã£ clean\n\n        LÆ°u thÃ nh cÃ¡c file:\n        - cleaned_data.parquet (dá»¯ liá»‡u Ä‘Ã£ clean, chÆ°a normalize)\n        - feature_names.txt\n        - cleaning_metadata.json\n        \"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\" ÄANG LÆ¯U Dá»® LIá»†U ÄÃƒ CLEAN...\")\n        print(\"=\"*80)\n\n        # LÆ°u dá»¯ liá»‡u dáº¡ng parquet (nhanh vÃ  nhá» gá»n)\n        parquet_path = self.output_dir / 'cleaned_data.parquet'\n        df.to_parquet(parquet_path, index=False)\n        print(f\"   âœ… ÄÃ£ lÆ°u: {parquet_path}\")\n        print(f\"      KÃ­ch thÆ°á»›c file: {parquet_path.stat().st_size / (1024*1024):.2f} MB\")\n\n        # CÅ©ng lÆ°u dáº¡ng CSV Ä‘á»ƒ dá»… kiá»ƒm tra (optional, cÃ³ thá»ƒ comment náº¿u file quÃ¡ lá»›n)\n        # csv_path = self.output_dir / 'cleaned_data.csv'\n        # df.to_csv(csv_path, index=False)\n        # print(f\"   âœ… ÄÃ£ lÆ°u: {csv_path}\")\n\n        # LÆ°u feature names\n        with open(self.output_dir / 'feature_names.txt', 'w') as f:\n            for name in self.feature_names:\n                f.write(name + '\\n')\n        print(f\"   âœ… ÄÃ£ lÆ°u: feature_names.txt\")\n\n        # LÆ°u column modes (Ä‘á»ƒ cÃ³ thá»ƒ sá»­ dá»¥ng cho dá»¯ liá»‡u má»›i)\n        with open(self.output_dir / 'column_modes.pkl', 'wb') as f:\n            pickle.dump(self.column_modes, f)\n        print(f\"   âœ… ÄÃ£ lÆ°u: column_modes.pkl\")\n\n        # LÆ°u zero-variance columns\n        with open(self.output_dir / 'zero_variance_cols.pkl', 'wb') as f:\n            pickle.dump(self.zero_variance_cols, f)\n        print(f\"   âœ… ÄÃ£ lÆ°u: zero_variance_cols.pkl\")\n\n        # Chuyá»ƒn Ä‘á»•i stats sang kiá»ƒu Python native\n        stats_native = {}\n        for key, value in self.stats.items():\n            if hasattr(value, 'item'):\n                stats_native[key] = value.item()\n            elif isinstance(value, (np.integer, np.floating)):\n                stats_native[key] = int(value) if isinstance(value, np.integer) else float(value)\n            else:\n                stats_native[key] = value\n\n        # LÆ°u metadata\n        metadata = {\n            'n_features': len(self.feature_names),\n            'feature_names': self.feature_names,\n            'total_samples': int(len(df)),\n            'benign_count': self.stats['benign_count'],\n            'attack_count': self.stats['attack_count'],\n            'benign_ratio': self.stats['benign_count'] / len(df),\n            'attack_ratio': self.stats['attack_count'] / len(df),\n            'zero_variance_cols': self.zero_variance_cols,\n            'stats': stats_native,\n            'created_at': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        }\n\n        with open(self.output_dir / 'cleaning_metadata.json', 'w', encoding='utf-8') as f:\n            json.dump(metadata, f, indent=4, ensure_ascii=False)\n        print(f\"   âœ… ÄÃ£ lÆ°u: cleaning_metadata.json\")\n\n        print(f\"\\nğŸ“ Táº¥t cáº£ file Ä‘Æ°á»£c lÆ°u táº¡i: {self.output_dir}\")\n\n    def print_summary(self):\n        \"\"\"In tÃ³m táº¯t quÃ¡ trÃ¬nh xá»­ lÃ½\"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\" TÃ“M Táº®T CLEAN Dá»® LIá»†U\")\n        print(\"=\"*80)\n        print(f\"   Tá»•ng sá»‘ dÃ²ng Ä‘á»c Ä‘Æ°á»£c:        {self.stats['total_rows_read']:,}\")\n        print(f\"   Sá»‘ dÃ²ng sau khi clean:        {self.stats['rows_after_cleaning']:,}\")\n        print(f\"   Sá»‘ duplicate Ä‘Ã£ loáº¡i:         {self.stats['duplicates_removed']:,}\")\n        print(f\"   Sá»‘ NaN Ä‘Ã£ thay báº±ng mode:     {self.stats['nan_replaced']:,}\")\n        print(f\"   Sá»‘ Inf Ä‘Ã£ thay báº±ng mode:     {self.stats['inf_replaced']:,}\")\n        print(f\"   Sá»‘ cá»™t zero-variance Ä‘Ã£ loáº¡i: {self.stats['zero_variance_cols_removed']}\")\n        print(f\"   Sá»‘ features cÃ²n láº¡i:          {self.stats['feature_count']}\")\n        print(f\"\\n   ğŸ“ˆ PHÃ‚N Bá» NHÃƒN:\")\n        print(f\"   Sá»‘ máº«u Benign (0):  {self.stats['benign_count']:,} ({self.stats['benign_count']/self.stats['rows_after_cleaning']*100:.1f}%)\")\n        print(f\"   Sá»‘ máº«u Attack (1):  {self.stats['attack_count']:,} ({self.stats['attack_count']/self.stats['rows_after_cleaning']*100:.1f}%)\")\n        print(f\"\\n   Thá»i gian xá»­ lÃ½: {self.stats['processing_time']:.2f} giÃ¢y\")\n        print(\"=\"*80)\n\n\ndef main():\n    \"\"\"HÃ m chÃ­nh Ä‘á»ƒ cháº¡y cleaning\"\"\"\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸ§¹ BÆ¯á»šC 1: CLEAN Dá»® LIá»†U CICIDS2018 CHO CNN\")\n    print(\"   PhÃ¡t hiá»‡n lÆ°u lÆ°á»£ng máº¡ng IoT báº¥t thÆ°á»ng\")\n    print(\"=\"*80)\n\n    # Khá»Ÿi táº¡o cleaner\n    cleaner = CICIDS2018_DataCleaner(\n        data_dir=DATA_DIR,\n        output_dir=OUTPUT_DIR,\n        chunk_size=CHUNK_SIZE\n    )\n\n    # Clean táº¥t cáº£ cÃ¡c file\n    df = cleaner.clean_all_files()\n\n    # LÆ°u dá»¯ liá»‡u Ä‘Ã£ clean\n    cleaner.save_cleaned_data(df)\n\n    # In tÃ³m táº¯t\n    cleaner.print_summary()\n\n    print(\"\\nâœ… HOÃ€N THÃ€NH BÆ¯á»šC 1!\")\n    print(\"   Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c clean vÃ  lÆ°u vÃ o folder.\")\n    print(\"   Cháº¡y step2_prepare_training_data.py Ä‘á»ƒ chia train/val/test vÃ  cÃ¢n báº±ng dá»¯ liá»‡u.\")\n\n    return cleaner\n\n\nif __name__ == \"__main__\":\n    cleaner = main()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T11:09:37.691763Z","iopub.execute_input":"2025-12-28T11:09:37.692006Z","iopub.status.idle":"2025-12-28T11:21:34.418223Z","shell.execute_reply.started":"2025-12-28T11:09:37.691986Z","shell.execute_reply":"2025-12-28T11:21:34.417554Z"}},"outputs":[{"name":"stdout","text":"ğŸŒ Äang cháº¡y trÃªn KAGGLE\n\n================================================================================\nğŸ§¹ BÆ¯á»šC 1: CLEAN Dá»® LIá»†U CICIDS2018 CHO CNN\n   PhÃ¡t hiá»‡n lÆ°u lÆ°á»£ng máº¡ng IoT báº¥t thÆ°á»ng\n================================================================================\n\n================================================================================\n Báº®T Äáº¦U CLEAN Dá»® LIá»†U CICIDS2018\n================================================================================\n\nğŸ“‚ TÃ¬m tháº¥y 10 file CSV:\n   - Friday-02-03-2018_TrafficForML_CICFlowMeter.csv\n   - Friday-16-02-2018_TrafficForML_CICFlowMeter.csv\n   - Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\n   - Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv\n   - Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv\n   - Thursday-15-02-2018_TrafficForML_CICFlowMeter.csv\n   - Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv\n   - Wednesday-14-02-2018_TrafficForML_CICFlowMeter.csv\n   - Wednesday-21-02-2018_TrafficForML_CICFlowMeter.csv\n   - Wednesday-28-02-2018_TrafficForML_CICFlowMeter.csv\n\n================================================================================\nğŸ“Š BÆ¯á»šC 1: THU THáº¬P THÃ”NG TIN Tá»ª Dá»® LIá»†U\n================================================================================\n\n   Äang scan: Friday-02-03-2018_TrafficForML_CICFlowMeter.csv\n\n   Äang scan: Friday-16-02-2018_TrafficForML_CICFlowMeter.csv\n\n   Äang scan: Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\n\n   Äang scan: Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv\n\n   Äang scan: Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv\n\n   Äang scan: Thursday-15-02-2018_TrafficForML_CICFlowMeter.csv\n\n   Äang scan: Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv\n\n   Äang scan: Wednesday-14-02-2018_TrafficForML_CICFlowMeter.csv\n\n   Äang scan: Wednesday-21-02-2018_TrafficForML_CICFlowMeter.csv\n\n   Äang scan: Wednesday-28-02-2018_TrafficForML_CICFlowMeter.csv\n\n   Äang phÃ¢n tÃ­ch variance cá»§a cÃ¡c cá»™t...\n   Äang tÃ­nh mode cho má»—i cá»™t...\n\n   âœ… Sá»‘ cá»™t zero-variance sáº½ loáº¡i bá»: 8\n      CÃ¡c cá»™t: ['Bwd PSH Flags', 'Bwd URG Flags', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg']\n   âœ… Sá»‘ cá»™t sáº½ giá»¯ láº¡i: 69\n\n================================================================================\n BÆ¯á»šC 2: CLEAN Dá»® LIá»†U\n================================================================================\n\nğŸ“„ Äang xá»­ lÃ½: Friday-02-03-2018_TrafficForML_CICFlowMeter.csv\n","output_type":"stream"},{"name":"stderr","text":"   Chunks: 4chunk [00:13,  3.30s/chunk]                    \n","output_type":"stream"},{"name":"stdout","text":"   âœ… ÄÃ£ xá»­ lÃ½: 1,048,575 máº«u\n\nğŸ“„ Äang xá»­ lÃ½: Friday-16-02-2018_TrafficForML_CICFlowMeter.csv\n","output_type":"stream"},{"name":"stderr","text":"   Chunks: 4chunk [00:17,  4.43s/chunk]                    \n","output_type":"stream"},{"name":"stdout","text":"   âœ… ÄÃ£ xá»­ lÃ½: 1,048,574 máº«u\n\nğŸ“„ Äang xá»­ lÃ½: Friday-23-02-2018_TrafficForML_CICFlowMeter.csv\n","output_type":"stream"},{"name":"stderr","text":"   Chunks: 4chunk [00:13,  3.32s/chunk]                    \n","output_type":"stream"},{"name":"stdout","text":"   âœ… ÄÃ£ xá»­ lÃ½: 1,048,575 máº«u\n\nğŸ“„ Äang xá»­ lÃ½: Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv\n","output_type":"stream"},{"name":"stderr","text":"   Chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [02:10<00:00,  4.84s/chunk]\n","output_type":"stream"},{"name":"stdout","text":"   âœ… ÄÃ£ xá»­ lÃ½: 7,948,748 máº«u\n\nğŸ“„ Äang xá»­ lÃ½: Thursday-01-03-2018_TrafficForML_CICFlowMeter.csv\n","output_type":"stream"},{"name":"stderr","text":"   Chunks: 2chunk [00:15,  7.85s/chunk]                    \n","output_type":"stream"},{"name":"stdout","text":"   âœ… ÄÃ£ xá»­ lÃ½: 331,100 máº«u\n\nğŸ“„ Äang xá»­ lÃ½: Thursday-15-02-2018_TrafficForML_CICFlowMeter.csv\n","output_type":"stream"},{"name":"stderr","text":"   Chunks: 4chunk [00:11,  2.97s/chunk]                    \n","output_type":"stream"},{"name":"stdout","text":"   âœ… ÄÃ£ xá»­ lÃ½: 1,048,575 máº«u\n\nğŸ“„ Äang xá»­ lÃ½: Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv\n","output_type":"stream"},{"name":"stderr","text":"   Chunks: 4chunk [00:11,  2.98s/chunk]                    \n","output_type":"stream"},{"name":"stdout","text":"   âœ… ÄÃ£ xá»­ lÃ½: 1,048,575 máº«u\n\nğŸ“„ Äang xá»­ lÃ½: Wednesday-14-02-2018_TrafficForML_CICFlowMeter.csv\n","output_type":"stream"},{"name":"stderr","text":"   Chunks: 4chunk [00:11,  2.80s/chunk]                    \n","output_type":"stream"},{"name":"stdout","text":"   âœ… ÄÃ£ xá»­ lÃ½: 1,048,575 máº«u\n\nğŸ“„ Äang xá»­ lÃ½: Wednesday-21-02-2018_TrafficForML_CICFlowMeter.csv\n","output_type":"stream"},{"name":"stderr","text":"   Chunks: 4chunk [00:10,  2.65s/chunk]                    \n","output_type":"stream"},{"name":"stdout","text":"   âœ… ÄÃ£ xá»­ lÃ½: 1,048,575 máº«u\n\nğŸ“„ Äang xá»­ lÃ½: Wednesday-28-02-2018_TrafficForML_CICFlowMeter.csv\n","output_type":"stream"},{"name":"stderr","text":"   Chunks: 3chunk [00:29,  9.83s/chunk]                    \n","output_type":"stream"},{"name":"stdout","text":"   âœ… ÄÃ£ xá»­ lÃ½: 613,071 máº«u\n\n--------------------------------------------------------------------------------\n ÄANG Gá»˜P Dá»® LIá»†U...\n   Tá»•ng sá»‘ máº«u sau khi gá»™p: 16,232,943\n   Äang loáº¡i bá» duplicate...\n   Sá»‘ máº«u sau khi loáº¡i duplicate: 10,822,059\n   Sá»‘ duplicate Ä‘Ã£ loáº¡i: 5,410,884\n\n================================================================================\n ÄANG LÆ¯U Dá»® LIá»†U ÄÃƒ CLEAN...\n================================================================================\n   âœ… ÄÃ£ lÆ°u: /kaggle/working/cleaned_data/cleaned_data.parquet\n      KÃ­ch thÆ°á»›c file: 1479.20 MB\n   âœ… ÄÃ£ lÆ°u: feature_names.txt\n   âœ… ÄÃ£ lÆ°u: column_modes.pkl\n   âœ… ÄÃ£ lÆ°u: zero_variance_cols.pkl\n   âœ… ÄÃ£ lÆ°u: cleaning_metadata.json\n\nğŸ“ Táº¥t cáº£ file Ä‘Æ°á»£c lÆ°u táº¡i: /kaggle/working/cleaned_data\n\n================================================================================\n TÃ“M Táº®T CLEAN Dá»® LIá»†U\n================================================================================\n   Tá»•ng sá»‘ dÃ²ng Ä‘á»c Ä‘Æ°á»£c:        16,233,002\n   Sá»‘ dÃ²ng sau khi clean:        10,822,059\n   Sá»‘ duplicate Ä‘Ã£ loáº¡i:         5,410,884\n   Sá»‘ NaN Ä‘Ã£ thay báº±ng mode:     59,721\n   Sá»‘ Inf Ä‘Ã£ thay báº±ng mode:     131,799\n   Sá»‘ cá»™t zero-variance Ä‘Ã£ loáº¡i: 8\n   Sá»‘ features cÃ²n láº¡i:          69\n\n   ğŸ“ˆ PHÃ‚N Bá» NHÃƒN:\n   Sá»‘ máº«u Benign (0):  9,496,113 (87.7%)\n   Sá»‘ máº«u Attack (1):  1,325,946 (12.3%)\n\n   Thá»i gian xá»­ lÃ½: 690.30 giÃ¢y\n================================================================================\n\nâœ… HOÃ€N THÃ€NH BÆ¯á»šC 1!\n   Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c clean vÃ  lÆ°u vÃ o folder.\n   Cháº¡y step2_prepare_training_data.py Ä‘á»ƒ chia train/val/test vÃ  cÃ¢n báº±ng dá»¯ liá»‡u.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\"\"\"\n======================================================================================\nBÆ¯á»šC 2: CHUáº¨N Bá»Š Dá»® LIá»†U TRAINING CHO CNN - CÃ‚N Báº°NG VÃ€ CHIA TRAIN/VAL/TEST\n======================================================================================\n\nScript nÃ y thá»±c hiá»‡n:\n1. Äá»c dá»¯ liá»‡u Ä‘Ã£ clean tá»« step1\n2. CÃ¢n báº±ng sá»‘ lÆ°á»£ng nhÃ£n (70% Benign, 30% Attack hoáº·c tá»· lá»‡ tÃ¹y chá»‰nh)\n3. Ãp dá»¥ng Log Transform: log_e(1+x)\n4. Chuáº©n hÃ³a báº±ng StandardScaler\n5. Reshape cho CNN 1D\n6. Chia train/val/test vá»›i stratify Ä‘á»ƒ giá»¯ tá»· lá»‡\n7. LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ Ä‘á»ƒ train\n\nCÃ³ thá»ƒ cháº¡y trÃªn cáº£ Kaggle vÃ  Local\n\"\"\"\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport json\nimport gc\nfrom pathlib import Path\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ============================================================================\n# THÆ¯ VIá»†N CHUáº¨N HÃ“A VÃ€ Xá»¬ LÃ Dá»® LIá»†U\n# ============================================================================\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Kiá»ƒm tra mÃ´i trÆ°á»ng cháº¡y (Kaggle hoáº·c Local)\nIS_KAGGLE = os.path.exists('/kaggle/input')\n\n# ============================================================================\n# Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN\n# ============================================================================\nif IS_KAGGLE:\n    CLEANED_DATA_DIR = \"/kaggle/working/cleaned_data\"\n    OUTPUT_DIR = \"/kaggle/working/training_data\"\n    print(\"ğŸŒ Äang cháº¡y trÃªn KAGGLE\")\nelse:\n    CLEANED_DATA_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CNN\\cleaned_data\"\n    OUTPUT_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CNN\\training_data\"\n    print(\"ğŸ’» Äang cháº¡y trÃªn LOCAL\")\n\n# ============================================================================\n# Cáº¤U HÃŒNH CÃ‚N Báº°NG Dá»® LIá»†U\n# ============================================================================\n\n# Random state Ä‘á»ƒ tÃ¡i táº¡o káº¿t quáº£\nRANDOM_STATE = 42\n\n# Tá»•ng sá»‘ máº«u mong muá»‘n (train + val + test)\nTOTAL_SAMPLES = 3000000  # 3 triá»‡u máº«u\n\n# Tá»· lá»‡ pháº§n trÄƒm cho má»—i class\nBENIGN_RATIO = 0.70  # 70% Benign\nATTACK_RATIO = 0.30  # 30% Attack\n\n# TÃ­nh sá»‘ lÆ°á»£ng máº«u cho má»—i class\nTARGET_BENIGN = int(TOTAL_SAMPLES * BENIGN_RATIO)  # 2,100,000\nTARGET_ATTACK = int(TOTAL_SAMPLES * ATTACK_RATIO)  # 900,000\n\n# Tá»· lá»‡ chia train/val/test\nTEST_SIZE = 0.20   # 20% cho test\nVAL_SIZE = 0.10    # 10% cho validation (tá»« tá»•ng)\n# Train sáº½ lÃ  70%\n\n# ============================================================================\n# CLASS CHUáº¨N Bá»Š Dá»® LIá»†U TRAINING\n# ============================================================================\n\nclass TrainingDataPreparer:\n    \"\"\"\n    Class chuáº©n bá»‹ dá»¯ liá»‡u training cho CNN\n\n    CÃ¡c bÆ°á»›c:\n    1. Äá»c dá»¯ liá»‡u Ä‘Ã£ clean\n    2. CÃ¢n báº±ng dá»¯ liá»‡u theo tá»· lá»‡ mong muá»‘n\n    3. Ãp dá»¥ng log transform: log_e(1+x)\n    4. Chuáº©n hÃ³a báº±ng StandardScaler\n    5. Reshape cho CNN\n    6. Chia train/val/test\n    7. LÆ°u dá»¯ liá»‡u\n    \"\"\"\n\n    def __init__(self, cleaned_data_dir, output_dir,\n                 total_samples=TOTAL_SAMPLES,\n                 benign_ratio=BENIGN_RATIO,\n                 attack_ratio=ATTACK_RATIO,\n                 test_size=TEST_SIZE,\n                 val_size=VAL_SIZE):\n        \"\"\"\n        Khá»Ÿi táº¡o preparer\n\n        Args:\n            cleaned_data_dir: ÄÆ°á»ng dáº«n thÆ° má»¥c chá»©a dá»¯ liá»‡u Ä‘Ã£ clean\n            output_dir: ÄÆ°á»ng dáº«n thÆ° má»¥c lÆ°u káº¿t quáº£\n            total_samples: Tá»•ng sá»‘ máº«u mong muá»‘n\n            benign_ratio: Tá»· lá»‡ Benign (0-1)\n            attack_ratio: Tá»· lá»‡ Attack (0-1)\n            test_size: Tá»· lá»‡ test set\n            val_size: Tá»· lá»‡ validation set\n        \"\"\"\n        self.cleaned_data_dir = Path(cleaned_data_dir)\n        self.output_dir = Path(output_dir)\n        self.total_samples = total_samples\n        self.benign_ratio = benign_ratio\n        self.attack_ratio = attack_ratio\n        self.test_size = test_size\n        self.val_size = val_size\n\n        # TÃ­nh target cho má»—i class\n        self.target_benign = int(total_samples * benign_ratio)\n        self.target_attack = int(total_samples * attack_ratio)\n\n        # Khá»Ÿi táº¡o scaler\n        self.scaler = StandardScaler()\n\n        # Táº¡o thÆ° má»¥c output\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n        # LÆ°u tÃªn features\n        self.feature_names = None\n\n        # Thá»‘ng kÃª\n        self.stats = {\n            'original_benign': 0,\n            'original_attack': 0,\n            'sampled_benign': 0,\n            'sampled_attack': 0,\n            'train_samples': 0,\n            'val_samples': 0,\n            'test_samples': 0,\n            'n_features': 0\n        }\n\n    def load_cleaned_data(self):\n        \"\"\"Äá»c dá»¯ liá»‡u Ä‘Ã£ clean tá»« step1\"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\"ğŸ“‚ ÄANG Äá»ŒC Dá»® LIá»†U ÄÃƒ CLEAN...\")\n        print(\"=\"*80)\n\n        parquet_path = self.cleaned_data_dir / 'cleaned_data.parquet'\n\n        if not parquet_path.exists():\n            raise FileNotFoundError(\n                f\"KhÃ´ng tÃ¬m tháº¥y file {parquet_path}\\n\"\n                f\"HÃ£y cháº¡y step1_clean_data.py trÆ°á»›c!\"\n            )\n\n        df = pd.read_parquet(parquet_path)\n\n        # Äá»c feature names\n        feature_names_path = self.cleaned_data_dir / 'feature_names.txt'\n        if feature_names_path.exists():\n            with open(feature_names_path, 'r') as f:\n                self.feature_names = [line.strip() for line in f.readlines()]\n        else:\n            self.feature_names = [col for col in df.columns if col != 'binary_label']\n\n        # Thá»‘ng kÃª\n        self.stats['original_benign'] = int((df['binary_label'] == 0).sum())\n        self.stats['original_attack'] = int((df['binary_label'] == 1).sum())\n        self.stats['n_features'] = len(self.feature_names)\n\n        print(f\"   âœ… ÄÃ£ Ä‘á»c: {len(df):,} máº«u\")\n        print(f\"   ğŸ“Š PhÃ¢n bá»‘ gá»‘c:\")\n        print(f\"      - Benign: {self.stats['original_benign']:,} ({self.stats['original_benign']/len(df)*100:.1f}%)\")\n        print(f\"      - Attack: {self.stats['original_attack']:,} ({self.stats['original_attack']/len(df)*100:.1f}%)\")\n        print(f\"   ğŸ“‹ Sá»‘ features: {self.stats['n_features']}\")\n\n        return df\n\n    def balanced_sample(self, df):\n        \"\"\"\n        Sample dá»¯ liá»‡u vá»›i tá»· lá»‡ cÃ¢n báº±ng mong muá»‘n\n\n        Chiáº¿n lÆ°á»£c:\n        - Náº¿u cÃ³ Ä‘á»§ máº«u: láº¥y Ä‘Ãºng sá»‘ lÆ°á»£ng target\n        - Náº¿u khÃ´ng Ä‘á»§ Attack: giáº£m Benign tÆ°Æ¡ng á»©ng Ä‘á»ƒ giá»¯ tá»· lá»‡\n        - Náº¿u khÃ´ng Ä‘á»§ cáº£ hai: láº¥y tá»‘i Ä‘a cÃ³ thá»ƒ vá»›i tá»· lá»‡ Ä‘Ãºng\n        \"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\"âš–ï¸ ÄANG CÃ‚N Báº°NG Dá»® LIá»†U...\")\n        print(\"=\"*80)\n\n        # TÃ¡ch theo class\n        df_benign = df[df['binary_label'] == 0]\n        df_attack = df[df['binary_label'] == 1]\n\n        n_benign = len(df_benign)\n        n_attack = len(df_attack)\n\n        print(f\"\\n   ğŸ¯ Target mong muá»‘n:\")\n        print(f\"      - Tá»•ng: {self.total_samples:,}\")\n        print(f\"      - Benign: {self.target_benign:,} ({self.benign_ratio*100:.0f}%)\")\n        print(f\"      - Attack: {self.target_attack:,} ({self.attack_ratio*100:.0f}%)\")\n\n        # XÃ¡c Ä‘á»‹nh sá»‘ lÆ°á»£ng thá»±c táº¿ cÃ³ thá»ƒ láº¥y\n        # Æ¯u tiÃªn giá»¯ Ä‘Ãºng tá»· lá»‡\n        actual_attack = min(self.target_attack, n_attack)\n        # TÃ­nh Benign dá»±a trÃªn Attack thá»±c táº¿ Ä‘á»ƒ giá»¯ tá»· lá»‡\n        actual_benign = int(actual_attack * (self.benign_ratio / self.attack_ratio))\n        actual_benign = min(actual_benign, n_benign)\n\n        # Náº¿u Benign bá»‹ giá»›i háº¡n, Ä‘iá»u chá»‰nh Attack\n        if actual_benign < int(actual_attack * (self.benign_ratio / self.attack_ratio)):\n            actual_attack = int(actual_benign * (self.attack_ratio / self.benign_ratio))\n\n        print(f\"\\n   ğŸ“Š Sá»‘ lÆ°á»£ng thá»±c táº¿ sáº½ láº¥y:\")\n        print(f\"      - Benign: {actual_benign:,}\")\n        print(f\"      - Attack: {actual_attack:,}\")\n        print(f\"      - Tá»•ng: {actual_benign + actual_attack:,}\")\n        print(f\"      - Tá»· lá»‡ thá»±c táº¿: {actual_benign/(actual_benign+actual_attack)*100:.1f}% - {actual_attack/(actual_benign+actual_attack)*100:.1f}%\")\n\n        if actual_benign < self.target_benign or actual_attack < self.target_attack:\n            print(f\"\\n   âš ï¸ KhÃ´ng Ä‘á»§ máº«u Ä‘á»ƒ Ä‘áº¡t target!\")\n            print(f\"      CÃ³ sáºµn: Benign={n_benign:,}, Attack={n_attack:,}\")\n\n        # Random sample tá»« má»—i class\n        print(f\"\\n   ğŸ”„ Äang sample...\")\n\n        # Sá»­ dá»¥ng random sampling\n        df_benign_sampled = df_benign.sample(n=actual_benign, random_state=RANDOM_STATE)\n        df_attack_sampled = df_attack.sample(n=actual_attack, random_state=RANDOM_STATE)\n\n        # Gá»™p láº¡i vÃ  shuffle\n        df_balanced = pd.concat([df_benign_sampled, df_attack_sampled], ignore_index=True)\n        df_balanced = df_balanced.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n\n        # Cáº­p nháº­t stats\n        self.stats['sampled_benign'] = actual_benign\n        self.stats['sampled_attack'] = actual_attack\n\n        print(f\"\\n   âœ… Káº¿t quáº£ sau khi cÃ¢n báº±ng:\")\n        print(f\"      - Benign: {actual_benign:,} ({actual_benign/(actual_benign+actual_attack)*100:.1f}%)\")\n        print(f\"      - Attack: {actual_attack:,} ({actual_attack/(actual_benign+actual_attack)*100:.1f}%)\")\n        print(f\"      - Tá»•ng: {len(df_balanced):,}\")\n\n        # Giáº£i phÃ³ng bá»™ nhá»›\n        del df_benign, df_attack, df_benign_sampled, df_attack_sampled\n        gc.collect()\n\n        return df_balanced\n\n    def apply_log_transform(self, X):\n        \"\"\"\n        Ãp dá»¥ng Log Transform: log_e(1+x)\n\n        LÆ°u Ã½: log(1+x) giÃºp:\n        - Giáº£m skewness cá»§a dá»¯ liá»‡u\n        - Xá»­ lÃ½ cÃ¡c giÃ¡ trá»‹ lá»›n\n        - Báº£o toÃ n giÃ¡ trá»‹ 0 (log(1+0) = 0)\n        \"\"\"\n        print(\"\\nğŸ”¢ ÄANG ÃP Dá»¤NG LOG TRANSFORM: log_e(1+x)...\")\n\n        # Äáº£m báº£o khÃ´ng cÃ³ giÃ¡ trá»‹ Ã¢m (log khÃ´ng xÃ¡c Ä‘á»‹nh cho sá»‘ Ã¢m)\n        # Vá»›i dá»¯ liá»‡u network flow, cÃ¡c giÃ¡ trá»‹ thÆ°á»ng >= 0\n        # Náº¿u cÃ³ giÃ¡ trá»‹ Ã¢m, ta shift Ä‘á»ƒ min = 0\n        min_val = X.min()\n        if min_val < 0:\n            print(f\"   âš ï¸ PhÃ¡t hiá»‡n giÃ¡ trá»‹ Ã¢m (min={min_val:.4f}), Ä‘ang shift...\")\n            X = X - min_val  # Shift Ä‘á»ƒ min = 0\n\n        # Ãp dá»¥ng log(1+x)\n        X_log = np.log1p(X)  # log1p(x) = log(1+x), á»•n Ä‘á»‹nh hÆ¡n vá»›i x nhá»\n\n        print(f\"   âœ… Log transform hoÃ n táº¥t\")\n        print(f\"      Range trÆ°á»›c: [{X.min():.4f}, {X.max():.4f}]\")\n        print(f\"      Range sau:   [{X_log.min():.4f}, {X_log.max():.4f}]\")\n\n        return X_log\n\n    def normalize_features(self, X):\n        \"\"\"\n        Chuáº©n hÃ³a features báº±ng StandardScaler\n        \"\"\"\n        print(\"\\nğŸ“ ÄANG CHUáº¨N HÃ“A Báº°NG STANDARDSCALER...\")\n\n        X_normalized = self.scaler.fit_transform(X)\n\n        print(f\"   âœ… StandardScaler hoÃ n táº¥t\")\n        print(f\"      Mean: {X_normalized.mean():.6f}\")\n        print(f\"      Std:  {X_normalized.std():.6f}\")\n\n        return X_normalized\n\n    def reshape_for_cnn(self, X):\n        \"\"\"\n        Reshape dá»¯ liá»‡u cho CNN 1D\n        CNN 1D yÃªu cáº§u input shape: (samples, features, channels)\n        \"\"\"\n        print(\"\\nğŸ”„ ÄANG RESHAPE CHO CNN 1D...\")\n\n        X_reshaped = X.reshape(X.shape[0], X.shape[1], 1)\n\n        print(f\"   âœ… Shape: {X.shape} -> {X_reshaped.shape}\")\n        print(f\"      (samples, features, channels)\")\n\n        return X_reshaped\n\n    def split_data(self, X, y):\n        \"\"\"\n        Chia dá»¯ liá»‡u thÃ nh train/val/test\n\n        ThÃªm validation: Train 70%, Val 10%, Test 20%\n\n        Sá»­ dá»¥ng stratify Ä‘á»ƒ giá»¯ tá»· lá»‡ class trong táº¥t cáº£ cÃ¡c táº­p\n        \"\"\"\n        print(\"\\nğŸ“Š ÄANG CHIA Dá»® LIá»†U TRAIN/VAL/TEST...\")\n\n        # BÆ°á»›c 1: Chia train+val / test (80/20)\n        X_temp, X_test, y_temp, y_test = train_test_split(\n            X, y,\n            test_size=self.test_size,\n            random_state=RANDOM_STATE,\n            stratify=y  # Giá»¯ tá»· lá»‡ class\n        )\n\n        # BÆ°á»›c 2: Chia train / val\n        val_ratio_from_temp = self.val_size / (1 - self.test_size)\n        X_train, X_val, y_train, y_val = train_test_split(\n            X_temp, y_temp,\n            test_size=val_ratio_from_temp,\n            random_state=RANDOM_STATE,\n            stratify=y_temp\n        )\n\n        # Cáº­p nháº­t stats\n        self.stats['train_samples'] = len(X_train)\n        self.stats['val_samples'] = len(X_val)\n        self.stats['test_samples'] = len(X_test)\n\n        print(f\"\\n   ğŸ“ˆ Káº¾T QUáº¢ CHIA Dá»® LIá»†U:\")\n        print(f\"   {'='*50}\")\n        print(f\"   {'Set':<10} {'Samples':>12} {'Benign':>12} {'Attack':>12}\")\n        print(f\"   {'-'*50}\")\n        print(f\"   {'Train':<10} {len(X_train):>12,} {(y_train==0).sum():>12,} {(y_train==1).sum():>12,}\")\n        print(f\"   {'Val':<10} {len(X_val):>12,} {(y_val==0).sum():>12,} {(y_val==1).sum():>12,}\")\n        print(f\"   {'Test':<10} {len(X_test):>12,} {(y_test==0).sum():>12,} {(y_test==1).sum():>12,}\")\n        print(f\"   {'-'*50}\")\n        print(f\"   {'Total':<10} {len(X_train)+len(X_val)+len(X_test):>12,}\")\n\n        # Kiá»ƒm tra tá»· lá»‡\n        print(f\"\\n   ğŸ“Š Tá»¶ Lá»† ATTACK TRONG Má»–I Táº¬P:\")\n        print(f\"      Train: {(y_train==1).sum()/len(y_train)*100:.1f}%\")\n        print(f\"      Val:   {(y_val==1).sum()/len(y_val)*100:.1f}%\")\n        print(f\"      Test:  {(y_test==1).sum()/len(y_test)*100:.1f}%\")\n\n        return X_train, X_val, X_test, y_train, y_val, y_test\n\n    def save_training_data(self, X_train, X_val, X_test, y_train, y_val, y_test):\n        \"\"\"\n        LÆ°u dá»¯ liá»‡u training\n\n        LÆ°u cÃ¡c file:\n        - X_train.npy, X_val.npy, X_test.npy\n        - y_train.npy, y_val.npy, y_test.npy\n        - scaler.pkl\n        - training_metadata.json\n        - feature_names.txt\n        \"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\"ğŸ’¾ ÄANG LÆ¯U Dá»® LIá»†U TRAINING...\")\n        print(\"=\"*80)\n\n        # LÆ°u numpy arrays\n        np.save(self.output_dir / 'X_train.npy', X_train)\n        np.save(self.output_dir / 'X_val.npy', X_val)\n        np.save(self.output_dir / 'X_test.npy', X_test)\n        np.save(self.output_dir / 'y_train.npy', y_train)\n        np.save(self.output_dir / 'y_val.npy', y_val)\n        np.save(self.output_dir / 'y_test.npy', y_test)\n\n        print(f\"   âœ… X_train.npy: {X_train.shape}\")\n        print(f\"   âœ… X_val.npy:   {X_val.shape}\")\n        print(f\"   âœ… X_test.npy:  {X_test.shape}\")\n        print(f\"   âœ… y_train.npy: {y_train.shape}\")\n        print(f\"   âœ… y_val.npy:   {y_val.shape}\")\n        print(f\"   âœ… y_test.npy:  {y_test.shape}\")\n\n        # LÆ°u scaler\n        with open(self.output_dir / 'scaler.pkl', 'wb') as f:\n            pickle.dump(self.scaler, f)\n        print(f\"   âœ… scaler.pkl\")\n\n        # LÆ°u feature names\n        with open(self.output_dir / 'feature_names.txt', 'w') as f:\n            for name in self.feature_names:\n                f.write(name + '\\n')\n        print(f\"   âœ… feature_names.txt\")\n\n        # Chuáº©n bá»‹ metadata\n        metadata = {\n            'n_features': len(self.feature_names),\n            'input_shape': [int(X_train.shape[1]), int(X_train.shape[2])],\n            'train_samples': int(X_train.shape[0]),\n            'val_samples': int(X_val.shape[0]),\n            'test_samples': int(X_test.shape[0]),\n            'total_samples': int(X_train.shape[0] + X_val.shape[0] + X_test.shape[0]),\n            'class_distribution': {\n                'train': {\n                    'benign': int((y_train == 0).sum()),\n                    'attack': int((y_train == 1).sum())\n                },\n                'val': {\n                    'benign': int((y_val == 0).sum()),\n                    'attack': int((y_val == 1).sum())\n                },\n                'test': {\n                    'benign': int((y_test == 0).sum()),\n                    'attack': int((y_test == 1).sum())\n                }\n            },\n            'benign_ratio': float(self.benign_ratio),\n            'attack_ratio': float(self.attack_ratio),\n            'preprocessing': {\n                'log_transform': 'log_e(1+x)',\n                'normalization': 'StandardScaler'\n            },\n            'created_at': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        }\n\n        with open(self.output_dir / 'training_metadata.json', 'w', encoding='utf-8') as f:\n            json.dump(metadata, f, indent=4, ensure_ascii=False)\n        print(f\"   âœ… training_metadata.json\")\n\n        print(f\"\\nğŸ“ Táº¥t cáº£ file Ä‘Æ°á»£c lÆ°u táº¡i: {self.output_dir}\")\n\n    def calculate_class_weights(self, y_train):\n        \"\"\"\n        TÃ­nh class weights cho training\n\n        Sá»­ dá»¥ng khi dá»¯ liá»‡u váº«n cÃ²n imbalanced\n        \"\"\"\n        from sklearn.utils.class_weight import compute_class_weight\n\n        classes = np.unique(y_train)\n        weights = compute_class_weight('balanced', classes=classes, y=y_train)\n        class_weights = dict(zip(classes, weights))\n\n        print(f\"\\nâš–ï¸ CLASS WEIGHTS (cho training):\")\n        print(f\"   Class 0 (Benign): {class_weights[0]:.4f}\")\n        print(f\"   Class 1 (Attack): {class_weights[1]:.4f}\")\n\n        # LÆ°u class weights\n        with open(self.output_dir / 'class_weights.pkl', 'wb') as f:\n            pickle.dump(class_weights, f)\n        print(f\"   âœ… ÄÃ£ lÆ°u class_weights.pkl\")\n\n        return class_weights\n\n\ndef main():\n    \"\"\"HÃ m chÃ­nh\"\"\"\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸ“Š BÆ¯á»šC 2: CHUáº¨N Bá»Š Dá»® LIá»†U TRAINING CHO CNN\")\n    print(\"   CÃ¢n báº±ng vÃ  chia train/val/test\")\n    print(\"=\"*80)\n\n    print(f\"\\nğŸ“‹ Cáº¤U HÃŒNH:\")\n    print(f\"   - Tá»•ng máº«u mong muá»‘n: {TOTAL_SAMPLES:,}\")\n    print(f\"   - Tá»· lá»‡ Benign: {BENIGN_RATIO*100:.0f}%\")\n    print(f\"   - Tá»· lá»‡ Attack: {ATTACK_RATIO*100:.0f}%\")\n    print(f\"   - Train/Val/Test: {(1-TEST_SIZE-VAL_SIZE)*100:.0f}%/{VAL_SIZE*100:.0f}%/{TEST_SIZE*100:.0f}%\")\n\n    # Khá»Ÿi táº¡o preparer\n    preparer = TrainingDataPreparer(\n        cleaned_data_dir=CLEANED_DATA_DIR,\n        output_dir=OUTPUT_DIR,\n        total_samples=TOTAL_SAMPLES,\n        benign_ratio=BENIGN_RATIO,\n        attack_ratio=ATTACK_RATIO,\n        test_size=TEST_SIZE,\n        val_size=VAL_SIZE\n    )\n\n    # BÆ°á»›c 1: Äá»c dá»¯ liá»‡u Ä‘Ã£ clean\n    df = preparer.load_cleaned_data()\n\n    # BÆ°á»›c 2: CÃ¢n báº±ng dá»¯ liá»‡u\n    df = preparer.balanced_sample(df)\n\n    # TÃ¡ch features vÃ  labels\n    X = df.drop(columns=['binary_label']).values\n    y = df['binary_label'].values\n\n    # Giáº£i phÃ³ng bá»™ nhá»› DataFrame\n    del df\n    gc.collect()\n\n    # BÆ°á»›c 3: Ãp dá»¥ng Log Transform\n    X = preparer.apply_log_transform(X)\n\n    # BÆ°á»›c 4: Chuáº©n hÃ³a\n    X = preparer.normalize_features(X)\n\n    # BÆ°á»›c 5: Reshape cho CNN\n    X = preparer.reshape_for_cnn(X)\n\n    # BÆ°á»›c 6: Chia train/val/test\n    X_train, X_val, X_test, y_train, y_val, y_test = preparer.split_data(X, y)\n\n    # Giáº£i phÃ³ng bá»™ nhá»›\n    del X, y\n    gc.collect()\n\n    # BÆ°á»›c 7: TÃ­nh class weights\n    class_weights = preparer.calculate_class_weights(y_train)\n\n    # BÆ°á»›c 8: LÆ°u dá»¯ liá»‡u\n    preparer.save_training_data(X_train, X_val, X_test, y_train, y_val, y_test)\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"âœ… HOÃ€N THÃ€NH BÆ¯á»šC 2!\")\n    print(\"   Dá»¯ liá»‡u Ä‘Ã£ sáºµn sÃ ng cho viá»‡c huáº¥n luyá»‡n CNN.\")\n    print(\"   Cháº¡y step3_train_cnn.py Ä‘á»ƒ train mÃ´ hÃ¬nh.\")\n    print(\"=\"*80)\n\n    return preparer\n\n\nif __name__ == \"__main__\":\n    preparer = main()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T11:26:15.058839Z","iopub.execute_input":"2025-12-28T11:26:15.059476Z","iopub.status.idle":"2025-12-28T11:26:48.765794Z","shell.execute_reply.started":"2025-12-28T11:26:15.059450Z","shell.execute_reply":"2025-12-28T11:26:48.765118Z"}},"outputs":[{"name":"stdout","text":"ğŸŒ Äang cháº¡y trÃªn KAGGLE\n\n================================================================================\nğŸ“Š BÆ¯á»šC 2: CHUáº¨N Bá»Š Dá»® LIá»†U TRAINING CHO CNN\n   CÃ¢n báº±ng vÃ  chia train/val/test\n================================================================================\n\nğŸ“‹ Cáº¤U HÃŒNH:\n   - Tá»•ng máº«u mong muá»‘n: 3,000,000\n   - Tá»· lá»‡ Benign: 70%\n   - Tá»· lá»‡ Attack: 30%\n   - Train/Val/Test: 70%/10%/20%\n\n================================================================================\nğŸ“‚ ÄANG Äá»ŒC Dá»® LIá»†U ÄÃƒ CLEAN...\n================================================================================\n   âœ… ÄÃ£ Ä‘á»c: 10,822,059 máº«u\n   ğŸ“Š PhÃ¢n bá»‘ gá»‘c:\n      - Benign: 9,496,113 (87.7%)\n      - Attack: 1,325,946 (12.3%)\n   ğŸ“‹ Sá»‘ features: 69\n\n================================================================================\nâš–ï¸ ÄANG CÃ‚N Báº°NG Dá»® LIá»†U...\n================================================================================\n\n   ğŸ¯ Target mong muá»‘n:\n      - Tá»•ng: 3,000,000\n      - Benign: 2,100,000 (70%)\n      - Attack: 900,000 (30%)\n\n   ğŸ“Š Sá»‘ lÆ°á»£ng thá»±c táº¿ sáº½ láº¥y:\n      - Benign: 2,100,000\n      - Attack: 900,000\n      - Tá»•ng: 3,000,000\n      - Tá»· lá»‡ thá»±c táº¿: 70.0% - 30.0%\n\n   ğŸ”„ Äang sample...\n\n   âœ… Káº¿t quáº£ sau khi cÃ¢n báº±ng:\n      - Benign: 2,100,000 (70.0%)\n      - Attack: 900,000 (30.0%)\n      - Tá»•ng: 3,000,000\n\nğŸ”¢ ÄANG ÃP Dá»¤NG LOG TRANSFORM: log_e(1+x)...\n   âš ï¸ PhÃ¡t hiá»‡n giÃ¡ trá»‹ Ã¢m (min=-947405000000.0000), Ä‘ang shift...\n   âœ… Log transform hoÃ n táº¥t\n      Range trÆ°á»›c: [0.0000, 1927186000000.0000]\n      Range sau:   [0.0000, 28.2871]\n\nğŸ“ ÄANG CHUáº¨N HÃ“A Báº°NG STANDARDSCALER...\n   âœ… StandardScaler hoÃ n táº¥t\n      Mean: -0.000000\n      Std:  0.701964\n\nğŸ”„ ÄANG RESHAPE CHO CNN 1D...\n   âœ… Shape: (3000000, 69) -> (3000000, 69, 1)\n      (samples, features, channels)\n\nğŸ“Š ÄANG CHIA Dá»® LIá»†U TRAIN/VAL/TEST...\n\n   ğŸ“ˆ Káº¾T QUáº¢ CHIA Dá»® LIá»†U:\n   ==================================================\n   Set             Samples       Benign       Attack\n   --------------------------------------------------\n   Train         2,100,000    1,470,000      630,000\n   Val             300,000      210,000       90,000\n   Test            600,000      420,000      180,000\n   --------------------------------------------------\n   Total         3,000,000\n\n   ğŸ“Š Tá»¶ Lá»† ATTACK TRONG Má»–I Táº¬P:\n      Train: 30.0%\n      Val:   30.0%\n      Test:  30.0%\n\nâš–ï¸ CLASS WEIGHTS (cho training):\n   Class 0 (Benign): 0.7143\n   Class 1 (Attack): 1.6667\n   âœ… ÄÃ£ lÆ°u class_weights.pkl\n\n================================================================================\nğŸ’¾ ÄANG LÆ¯U Dá»® LIá»†U TRAINING...\n================================================================================\n   âœ… X_train.npy: (2100000, 69, 1)\n   âœ… X_val.npy:   (300000, 69, 1)\n   âœ… X_test.npy:  (600000, 69, 1)\n   âœ… y_train.npy: (2100000,)\n   âœ… y_val.npy:   (300000,)\n   âœ… y_test.npy:  (600000,)\n   âœ… scaler.pkl\n   âœ… feature_names.txt\n   âœ… training_metadata.json\n\nğŸ“ Táº¥t cáº£ file Ä‘Æ°á»£c lÆ°u táº¡i: /kaggle/working/training_data\n\n================================================================================\nâœ… HOÃ€N THÃ€NH BÆ¯á»šC 2!\n   Dá»¯ liá»‡u Ä‘Ã£ sáºµn sÃ ng cho viá»‡c huáº¥n luyá»‡n CNN.\n   Cháº¡y step3_train_cnn.py Ä‘á»ƒ train mÃ´ hÃ¬nh.\n================================================================================\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\"\"\"\n======================================================================================\nBÆ¯á»šC 3: TRAIN MÃ” HÃŒNH CNN CHO PHÃT HIá»†N LÆ¯U LÆ¯á»¢NG Máº NG IOT Báº¤T THÆ¯á»œNG\n======================================================================================\n\nKiáº¿n trÃºc CNN theo yÃªu cáº§u:\n- Input Layer: Shape (num_features, 1)\n- Conv1D (32 filters, kernel 2) -> MaxPooling1D (2)\n- Conv1D (32 filters, kernel 2) -> MaxPooling1D (2)\n- Conv1D (64 filters, kernel 2) -> MaxPooling1D (2)\n- Conv1D (64 filters, kernel 2) -> MaxPooling1D (2)\n- Conv1D (64 filters, kernel 2) -> MaxPooling1D (2)\n- BatchNormalization + Dropout (0.5)\n- Flatten\n- Dense(1, activation='sigmoid')\n\nLoss: binary_crossentropy\nOptimizer: Adam\nMetrics: Accuracy, Precision, Recall\n\nCÃ³ thá»ƒ cháº¡y trÃªn cáº£ Kaggle vÃ  Local\n\"\"\"\n\nimport os\nimport numpy as np\nimport pickle\nimport json\nfrom pathlib import Path\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ============================================================================\n# TENSORFLOW/KERAS\n# ============================================================================\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import (\n    Conv1D, MaxPooling1D, Flatten, Dense,\n    Dropout, BatchNormalization, Input\n)\nfrom tensorflow.keras.callbacks import (\n    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n)\nfrom tensorflow.keras.metrics import Precision, Recall\n\n# Kiá»ƒm tra GPU\nprint(\"=\"*80)\nprint(\"ğŸ–¥ï¸ THÃ”NG TIN Há»† THá»NG\")\nprint(\"=\"*80)\nprint(f\"TensorFlow version: {tf.__version__}\")\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    print(f\"âœ… GPU available: {len(gpus)} GPU(s)\")\n    for gpu in gpus:\n        print(f\"   - {gpu}\")\n    # Cáº¥u hÃ¬nh GPU memory growth Ä‘á»ƒ trÃ¡nh chiáº¿m háº¿t bá»™ nhá»›\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu, True)\nelse:\n    print(\"âš ï¸ KhÃ´ng cÃ³ GPU, sáº½ sá»­ dá»¥ng CPU\")\n\n# Kiá»ƒm tra mÃ´i trÆ°á»ng cháº¡y\nIS_KAGGLE = os.path.exists('/kaggle/input')\n\n# ============================================================================\n# Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN\n# ============================================================================\nif IS_KAGGLE:\n    TRAINING_DATA_DIR = \"/kaggle/working/training_data\"\n    MODEL_DIR = \"/kaggle/working/models\"\n    LOG_DIR = \"/kaggle/working/logs\"\n    print(\"ğŸŒ Äang cháº¡y trÃªn KAGGLE\")\nelse:\n    TRAINING_DATA_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CNN\\training_data\"\n    MODEL_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CNN\\models\"\n    LOG_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CNN\\logs\"\n    print(\"ğŸ’» Äang cháº¡y trÃªn LOCAL\")\n\n# ============================================================================\n# Cáº¤U HÃŒNH HUáº¤N LUYá»†N\n# ============================================================================\n\n# Hyperparameters\nBATCH_SIZE = 256        # Batch size cho training\nEPOCHS = 50             # Sá»‘ epochs tá»‘i Ä‘a\nLEARNING_RATE = 0.001   # Learning rate ban Ä‘áº§u\n\n# Regularization\nDROPOUT_RATE = 0.5      # Dropout rate trÆ°á»›c Flatten\n\n# Early stopping\nPATIENCE = 10           # Sá»‘ epochs chá» trÆ°á»›c khi dá»«ng\n\n# Random seed\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)\n\n\n# ============================================================================\n# HÃ€M XÃ‚Y Dá»°NG MÃ” HÃŒNH CNN\n# ============================================================================\n\ndef build_cnn_model(input_shape):\n    \"\"\"\n    XÃ¢y dá»±ng mÃ´ hÃ¬nh CNN cho phÃ¢n loáº¡i binary\n\n    Kiáº¿n trÃºc theo yÃªu cáº§u:\n    - 5 lá»›p Conv1D vá»›i MaxPooling\n    - BatchNormalization vÃ  Dropout trÆ°á»›c Flatten\n    - Output layer vá»›i sigmoid activation\n\n    Args:\n        input_shape: Shape cá»§a input (n_features, 1)\n\n    Returns:\n        model: Keras Sequential model\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸ—ï¸ ÄANG XÃ‚Y Dá»°NG MÃ” HÃŒNH CNN\")\n    print(\"=\"*80)\n    print(f\"   Input shape: {input_shape}\")\n\n    model = Sequential(name='CNN_Binary_Classification')\n\n    # Input layer\n    model.add(Input(shape=input_shape))\n\n    # ========== KHá»I CONV 1 ==========\n    # Conv1D (32 filters, kernel 2x1) -> MaxPooling1D (2)\n    model.add(Conv1D(\n        filters=32,\n        kernel_size=2,\n        activation='relu',\n        padding='same',  # Giá»¯ nguyÃªn kÃ­ch thÆ°á»›c\n        name='conv1d_1'\n    ))\n    model.add(MaxPooling1D(pool_size=2, name='maxpool_1'))\n\n    # ========== KHá»I CONV 2 ==========\n    # Conv1D (32 filters, kernel 2x1) -> MaxPooling1D (2)\n    model.add(Conv1D(\n        filters=32,\n        kernel_size=2,\n        activation='relu',\n        padding='same',\n        name='conv1d_2'\n    ))\n    model.add(MaxPooling1D(pool_size=2, name='maxpool_2'))\n\n    # ========== KHá»I CONV 3 ==========\n    # Conv1D (64 filters, kernel 2x1) -> MaxPooling1D (2)\n    model.add(Conv1D(\n        filters=64,\n        kernel_size=2,\n        activation='relu',\n        padding='same',\n        name='conv1d_3'\n    ))\n    model.add(MaxPooling1D(pool_size=2, name='maxpool_3'))\n\n    # ========== KHá»I CONV 4 ==========\n    # Conv1D (64 filters, kernel 2x1) -> MaxPooling1D (2)\n    model.add(Conv1D(\n        filters=64,\n        kernel_size=2,\n        activation='relu',\n        padding='same',\n        name='conv1d_4'\n    ))\n    model.add(MaxPooling1D(pool_size=2, name='maxpool_4'))\n\n    # ========== KHá»I CONV 5 ==========\n    # Conv1D (64 filters, kernel 2x1) -> MaxPooling1D (2)\n    model.add(Conv1D(\n        filters=64,\n        kernel_size=2,\n        activation='relu',\n        padding='same',\n        name='conv1d_5'\n    ))\n    model.add(MaxPooling1D(pool_size=2, name='maxpool_5'))\n\n    # ========== REGULARIZATION ==========\n    # BatchNormalization vÃ  Dropout trÆ°á»›c Flatten\n    model.add(BatchNormalization(name='batch_norm'))\n    model.add(Dropout(DROPOUT_RATE, name='dropout'))\n\n    # ========== FLATTEN ==========\n    model.add(Flatten(name='flatten'))\n\n    # ========== OUTPUT LAYER ==========\n    # Dense(1, activation='sigmoid') cho binary classification\n    model.add(Dense(1, activation='sigmoid', name='output'))\n\n    # ========== COMPILE ==========\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n        loss='binary_crossentropy',\n        metrics=[\n            'accuracy',\n            Precision(name='precision'),\n            Recall(name='recall')\n        ]\n    )\n\n    # In tÃ³m táº¯t mÃ´ hÃ¬nh\n    print(\"\\n   ğŸ“‹ KIáº¾N TRÃšC MÃ” HÃŒNH:\")\n    model.summary()\n\n    return model\n\n\ndef load_training_data(data_dir):\n    \"\"\"\n    Load dá»¯ liá»‡u training Ä‘Ã£ Ä‘Æ°á»£c chuáº©n bá»‹ tá»« step 2\n\n    Args:\n        data_dir: ÄÆ°á»ng dáº«n thÆ° má»¥c chá»©a dá»¯ liá»‡u\n\n    Returns:\n        X_train, X_val, X_test, y_train, y_val, y_test, class_weights\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸ“‚ ÄANG LOAD Dá»® LIá»†U TRAINING...\")\n    print(\"=\"*80)\n\n    data_dir = Path(data_dir)\n\n    # Load numpy arrays\n    X_train = np.load(data_dir / 'X_train.npy')\n    X_val = np.load(data_dir / 'X_val.npy')\n    X_test = np.load(data_dir / 'X_test.npy')\n    y_train = np.load(data_dir / 'y_train.npy')\n    y_val = np.load(data_dir / 'y_val.npy')\n    y_test = np.load(data_dir / 'y_test.npy')\n\n    print(f\"   âœ… X_train: {X_train.shape}\")\n    print(f\"   âœ… X_val:   {X_val.shape}\")\n    print(f\"   âœ… X_test:  {X_test.shape}\")\n    print(f\"   âœ… y_train: {y_train.shape}\")\n    print(f\"   âœ… y_val:   {y_val.shape}\")\n    print(f\"   âœ… y_test:  {y_test.shape}\")\n\n    # Load class weights náº¿u cÃ³\n    class_weights = None\n    class_weights_path = data_dir / 'class_weights.pkl'\n    if class_weights_path.exists():\n        with open(class_weights_path, 'rb') as f:\n            class_weights = pickle.load(f)\n        print(f\"\\n   âš–ï¸ Class weights loaded:\")\n        print(f\"      Class 0 (Benign): {class_weights[0]:.4f}\")\n        print(f\"      Class 1 (Attack): {class_weights[1]:.4f}\")\n\n    # Thá»‘ng kÃª phÃ¢n bá»‘\n    print(f\"\\n   ğŸ“Š PHÃ‚N Bá» Dá»® LIá»†U:\")\n    for name, y in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:\n        benign = (y == 0).sum()\n        attack = (y == 1).sum()\n        total = len(y)\n        print(f\"      {name}: Benign={benign:,} ({benign/total*100:.1f}%), Attack={attack:,} ({attack/total*100:.1f}%)\")\n\n    return X_train, X_val, X_test, y_train, y_val, y_test, class_weights\n\n\ndef create_callbacks(model_dir, log_dir):\n    \"\"\"\n    Táº¡o cÃ¡c callbacks cho training\n\n    Callbacks:\n    - EarlyStopping: Dá»«ng sá»›m khi val_loss khÃ´ng giáº£m\n    - ModelCheckpoint: LÆ°u model tá»‘t nháº¥t\n    - ReduceLROnPlateau: Giáº£m learning rate khi plateau\n    - TensorBoard: Logging cho visualization\n    \"\"\"\n    print(\"\\nğŸ“Œ ÄANG Cáº¤U HÃŒNH CALLBACKS...\")\n\n    model_dir = Path(model_dir)\n    log_dir = Path(log_dir)\n    model_dir.mkdir(parents=True, exist_ok=True)\n    log_dir.mkdir(parents=True, exist_ok=True)\n\n    callbacks = []\n\n    # 1. Early Stopping\n    # Dá»«ng training khi val_loss khÃ´ng cáº£i thiá»‡n sau PATIENCE epochs\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=PATIENCE,\n        verbose=1,\n        mode='min',\n        restore_best_weights=True  # KhÃ´i phá»¥c weights tá»‘t nháº¥t\n    )\n    callbacks.append(early_stopping)\n    print(f\"   âœ… EarlyStopping: patience={PATIENCE}\")\n\n    # 2. Model Checkpoint\n    # LÆ°u model cÃ³ val_loss tháº¥p nháº¥t\n    checkpoint_path = model_dir / 'best_model.keras'\n    model_checkpoint = ModelCheckpoint(\n        filepath=str(checkpoint_path),\n        monitor='val_loss',\n        verbose=1,\n        save_best_only=True,\n        mode='min'\n    )\n    callbacks.append(model_checkpoint)\n    print(f\"   âœ… ModelCheckpoint: {checkpoint_path}\")\n\n    # 3. Reduce Learning Rate on Plateau\n    # Giáº£m LR khi val_loss khÃ´ng giáº£m\n    reduce_lr = ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,        # Giáº£m LR cÃ²n 1/2\n        patience=5,        # Chá» 5 epochs\n        min_lr=1e-7,       # LR tá»‘i thiá»ƒu\n        verbose=1\n    )\n    callbacks.append(reduce_lr)\n    print(f\"   âœ… ReduceLROnPlateau: factor=0.5, patience=5\")\n\n    # 4. TensorBoard (optional)\n    tensorboard_log = log_dir / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    tensorboard = TensorBoard(\n        log_dir=str(tensorboard_log),\n        histogram_freq=1\n    )\n    callbacks.append(tensorboard)\n    print(f\"   âœ… TensorBoard: {tensorboard_log}\")\n\n    return callbacks\n\n\ndef train_model(model, X_train, y_train, X_val, y_val, class_weights, callbacks):\n    \"\"\"\n    Huáº¥n luyá»‡n mÃ´ hÃ¬nh\n\n    Args:\n        model: Keras model\n        X_train, y_train: Dá»¯ liá»‡u training\n        X_val, y_val: Dá»¯ liá»‡u validation\n        class_weights: Dictionary class weights\n        callbacks: List cÃ¡c callbacks\n\n    Returns:\n        history: Training history\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸš€ Báº®T Äáº¦U HUáº¤N LUYá»†N MÃ” HÃŒNH\")\n    print(\"=\"*80)\n    print(f\"   Epochs: {EPOCHS}\")\n    print(f\"   Batch size: {BATCH_SIZE}\")\n    print(f\"   Learning rate: {LEARNING_RATE}\")\n    print(f\"   Class weights: {'CÃ³' if class_weights else 'KhÃ´ng'}\")\n\n    start_time = datetime.now()\n\n    history = model.fit(\n        X_train, y_train,\n        batch_size=BATCH_SIZE,\n        epochs=EPOCHS,\n        validation_data=(X_val, y_val),\n        class_weight=class_weights,  # Sá»­ dá»¥ng class weights Ä‘á»ƒ xá»­ lÃ½ imbalance\n        callbacks=callbacks,\n        verbose=1\n    )\n\n    end_time = datetime.now()\n    training_time = (end_time - start_time).total_seconds()\n\n    print(f\"\\n   â±ï¸ Thá»i gian training: {training_time/60:.2f} phÃºt\")\n    print(f\"   ğŸ“ˆ Best val_loss: {min(history.history['val_loss']):.4f}\")\n    print(f\"   ğŸ“ˆ Best val_accuracy: {max(history.history['val_accuracy']):.4f}\")\n\n    return history, training_time\n\n\ndef evaluate_model(model, X_test, y_test):\n    \"\"\"\n    ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn test set\n\n    Args:\n        model: Trained model\n        X_test, y_test: Dá»¯ liá»‡u test\n\n    Returns:\n        results: Dictionary káº¿t quáº£ Ä‘Ã¡nh giÃ¡\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸ“Š ÄÃNH GIÃ MÃ” HÃŒNH TRÃŠN TEST SET\")\n    print(\"=\"*80)\n\n    # Evaluate\n    loss, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=1)\n\n    # TÃ­nh F1-score\n    f1_score = 2 * (precision * recall) / (precision + recall + 1e-7)\n\n    results = {\n        'test_loss': float(loss),\n        'test_accuracy': float(accuracy),\n        'test_precision': float(precision),\n        'test_recall': float(recall),\n        'test_f1_score': float(f1_score)\n    }\n\n    print(f\"\\n   ğŸ“Š Káº¾T QUáº¢:\")\n    print(f\"   {'='*40}\")\n    print(f\"   Loss:      {loss:.4f}\")\n    print(f\"   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n    print(f\"   Precision: {precision:.4f}\")\n    print(f\"   Recall:    {recall:.4f}\")\n    print(f\"   F1-Score:  {f1_score:.4f}\")\n    print(f\"   {'='*40}\")\n\n    # Predictions cho confusion matrix\n    y_pred_prob = model.predict(X_test, verbose=0)\n    y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n\n    # Confusion matrix\n    from sklearn.metrics import confusion_matrix, classification_report\n\n    cm = confusion_matrix(y_test, y_pred)\n    print(f\"\\n   ğŸ“‹ CONFUSION MATRIX:\")\n    print(f\"                 Predicted\")\n    print(f\"                 Benign  Attack\")\n    print(f\"   Actual Benign  {cm[0,0]:>6}  {cm[0,1]:>6}\")\n    print(f\"   Actual Attack  {cm[1,0]:>6}  {cm[1,1]:>6}\")\n\n    print(f\"\\n   ğŸ“‹ CLASSIFICATION REPORT:\")\n    report = classification_report(y_test, y_pred, target_names=['Benign', 'Attack'])\n    print(report)\n\n    # LÆ°u classification report dáº¡ng dictionary\n    report_dict = classification_report(y_test, y_pred, target_names=['Benign', 'Attack'], output_dict=True)\n\n    # ThÃªm confusion matrix vÃ  cÃ¡c metrics khÃ¡c vÃ o results\n    results['confusion_matrix'] = cm.tolist()\n    results['classification_report'] = report_dict\n\n    # ThÃªm cÃ¡c metrics chi tiáº¿t cho tá»«ng class\n    results['benign_precision'] = float(report_dict['Benign']['precision'])\n    results['benign_recall'] = float(report_dict['Benign']['recall'])\n    results['benign_f1'] = float(report_dict['Benign']['f1-score'])\n    results['attack_precision'] = float(report_dict['Attack']['precision'])\n    results['attack_recall'] = float(report_dict['Attack']['recall'])\n    results['attack_f1'] = float(report_dict['Attack']['f1-score'])\n\n    # TÃ­nh thÃªm má»™t sá»‘ metrics bá»• sung\n    tn, fp, fn, tp = cm.ravel()\n    results['true_negative'] = int(tn)\n    results['false_positive'] = int(fp)\n    results['false_negative'] = int(fn)\n    results['true_positive'] = int(tp)\n    results['specificity'] = float(tn / (tn + fp + 1e-7))  # True Negative Rate\n    results['false_positive_rate'] = float(fp / (fp + tn + 1e-7))\n    results['false_negative_rate'] = float(fn / (fn + tp + 1e-7))\n\n    return results, y_pred, y_pred_prob\n\n\ndef save_model_and_results(model, history, results, training_time, model_dir, y_pred=None, y_pred_prob=None):\n    \"\"\"\n    LÆ°u model vÃ  káº¿t quáº£ training\n\n    Args:\n        model: Trained model\n        history: Training history\n        results: Evaluation results\n        training_time: Thá»i gian training (seconds)\n        model_dir: ÄÆ°á»ng dáº«n lÆ°u\n        y_pred: Predictions (optional)\n        y_pred_prob: Prediction probabilities (optional)\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸ’¾ ÄANG LÆ¯U MODEL VÃ€ Káº¾T QUáº¢...\")\n    print(\"=\"*80)\n\n    model_dir = Path(model_dir)\n    model_dir.mkdir(parents=True, exist_ok=True)\n\n    # LÆ°u model cuá»‘i cÃ¹ng\n    final_model_path = model_dir / 'final_model.keras'\n    model.save(final_model_path)\n    print(f\"   âœ… Final model: {final_model_path}\")\n\n    # LÆ°u model weights\n    weights_path = model_dir / 'model_weights.weights.h5'\n    model.save_weights(weights_path)\n    print(f\"   âœ… Model weights: {weights_path}\")\n\n    # LÆ°u training history\n    history_dict = {key: [float(v) for v in values] for key, values in history.history.items()}\n    with open(model_dir / 'training_history.json', 'w') as f:\n        json.dump(history_dict, f, indent=4)\n    print(f\"   âœ… Training history: training_history.json\")\n\n    # LÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡ vá»›i thÃ´ng tin bá»• sung\n    results['training_time_seconds'] = float(training_time)\n    results['training_time_minutes'] = float(training_time / 60)\n    results['epochs_trained'] = int(len(history.history['loss']))\n\n    # ThÃªm best validation metrics\n    results['best_val_loss'] = float(min(history.history['val_loss']))\n    results['best_val_accuracy'] = float(max(history.history['val_accuracy']))\n    results['best_val_precision'] = float(max(history.history['val_precision']))\n    results['best_val_recall'] = float(max(history.history['val_recall']))\n\n    # TÃ­nh best val F1-score\n    val_precisions = history.history['val_precision']\n    val_recalls = history.history['val_recall']\n    val_f1_scores = [2 * (p * r) / (p + r + 1e-7) for p, r in zip(val_precisions, val_recalls)]\n    results['best_val_f1_score'] = float(max(val_f1_scores))\n\n    # Epoch nÃ o Ä‘áº¡t best val_loss\n    results['best_val_loss_epoch'] = int(np.argmin(history.history['val_loss']) + 1)\n    results['best_val_accuracy_epoch'] = int(np.argmax(history.history['val_accuracy']) + 1)\n\n    with open(model_dir / 'evaluation_results.json', 'w') as f:\n        json.dump(results, f, indent=4)\n    print(f\"   âœ… Evaluation results: evaluation_results.json\")\n\n    # LÆ°u predictions náº¿u cÃ³\n    if y_pred is not None:\n        np.save(model_dir / 'y_pred.npy', y_pred)\n        print(f\"   âœ… Predictions: y_pred.npy\")\n\n    if y_pred_prob is not None:\n        np.save(model_dir / 'y_pred_prob.npy', y_pred_prob)\n        print(f\"   âœ… Prediction probabilities: y_pred_prob.npy\")\n\n    # LÆ°u cáº¥u hÃ¬nh training\n    config = {\n        'batch_size': BATCH_SIZE,\n        'epochs': EPOCHS,\n        'learning_rate': LEARNING_RATE,\n        'dropout_rate': DROPOUT_RATE,\n        'patience': PATIENCE,\n        'random_seed': RANDOM_SEED,\n        'tensorflow_version': tf.__version__,\n        'created_at': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    }\n    with open(model_dir / 'training_config.json', 'w') as f:\n        json.dump(config, f, indent=4)\n    print(f\"   âœ… Training config: training_config.json\")\n\n    print(f\"\\nğŸ“ Táº¥t cáº£ file Ä‘Æ°á»£c lÆ°u táº¡i: {model_dir}\")\n\n\ndef plot_training_history(history, model_dir):\n    \"\"\"\n    Váº½ biá»ƒu Ä‘á»“ training history\n\n    Args:\n        history: Training history\n        model_dir: ÄÆ°á»ng dáº«n lÆ°u hÃ¬nh\n    \"\"\"\n    try:\n        import matplotlib.pyplot as plt\n\n        model_dir = Path(model_dir)\n        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n        # 1. Loss\n        axes[0, 0].plot(history.history['loss'], label='Train Loss')\n        axes[0, 0].plot(history.history['val_loss'], label='Val Loss')\n        axes[0, 0].set_title('Model Loss')\n        axes[0, 0].set_xlabel('Epoch')\n        axes[0, 0].set_ylabel('Loss')\n        axes[0, 0].legend()\n        axes[0, 0].grid(True)\n\n        # 2. Accuracy\n        axes[0, 1].plot(history.history['accuracy'], label='Train Accuracy')\n        axes[0, 1].plot(history.history['val_accuracy'], label='Val Accuracy')\n        axes[0, 1].set_title('Model Accuracy')\n        axes[0, 1].set_xlabel('Epoch')\n        axes[0, 1].set_ylabel('Accuracy')\n        axes[0, 1].legend()\n        axes[0, 1].grid(True)\n\n        # 3. Precision\n        axes[1, 0].plot(history.history['precision'], label='Train Precision')\n        axes[1, 0].plot(history.history['val_precision'], label='Val Precision')\n        axes[1, 0].set_title('Model Precision')\n        axes[1, 0].set_xlabel('Epoch')\n        axes[1, 0].set_ylabel('Precision')\n        axes[1, 0].legend()\n        axes[1, 0].grid(True)\n\n        # 4. Recall\n        axes[1, 1].plot(history.history['recall'], label='Train Recall')\n        axes[1, 1].plot(history.history['val_recall'], label='Val Recall')\n        axes[1, 1].set_title('Model Recall')\n        axes[1, 1].set_xlabel('Epoch')\n        axes[1, 1].set_ylabel('Recall')\n        axes[1, 1].legend()\n        axes[1, 1].grid(True)\n\n        plt.tight_layout()\n        plt.savefig(model_dir / 'training_history.png', dpi=150)\n        plt.close()\n        print(f\"   âœ… Training history plot: training_history.png\")\n\n    except ImportError:\n        print(\"   âš ï¸ matplotlib khÃ´ng cÃ³ sáºµn, bá» qua viá»‡c váº½ biá»ƒu Ä‘á»“\")\n\n\ndef plot_confusion_matrix(cm, model_dir):\n    \"\"\"\n    Váº½ confusion matrix dÆ°á»›i dáº¡ng heatmap\n\n    Args:\n        cm: Confusion matrix (numpy array hoáº·c list)\n        model_dir: ÄÆ°á»ng dáº«n lÆ°u hÃ¬nh\n    \"\"\"\n    try:\n        import matplotlib.pyplot as plt\n        import seaborn as sns\n\n        model_dir = Path(model_dir)\n\n        # Convert to numpy array if needed\n        if isinstance(cm, list):\n            cm = np.array(cm)\n\n        # Váº½ heatmap\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                   xticklabels=['Benign', 'Attack'],\n                   yticklabels=['Benign', 'Attack'],\n                   cbar_kws={'label': 'Count'})\n        plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n        plt.ylabel('Actual', fontsize=12)\n        plt.xlabel('Predicted', fontsize=12)\n        plt.tight_layout()\n        plt.savefig(model_dir / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n        plt.close()\n        print(f\"   âœ… Confusion matrix plot: confusion_matrix.png\")\n\n        # Váº½ normalized confusion matrix\n        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n                   xticklabels=['Benign', 'Attack'],\n                   yticklabels=['Benign', 'Attack'],\n                   cbar_kws={'label': 'Percentage'})\n        plt.title('Normalized Confusion Matrix', fontsize=16, fontweight='bold')\n        plt.ylabel('Actual', fontsize=12)\n        plt.xlabel('Predicted', fontsize=12)\n        plt.tight_layout()\n        plt.savefig(model_dir / 'confusion_matrix_normalized.png', dpi=150, bbox_inches='tight')\n        plt.close()\n        print(f\"   âœ… Normalized confusion matrix plot: confusion_matrix_normalized.png\")\n\n    except ImportError as e:\n        print(f\"   âš ï¸ matplotlib/seaborn khÃ´ng cÃ³ sáºµn, bá» qua viá»‡c váº½ confusion matrix: {e}\")\n\n\ndef main():\n    \"\"\"HÃ m chÃ­nh Ä‘á»ƒ train model\"\"\"\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸ§  HUáº¤N LUYá»†N MÃ” HÃŒNH CNN - PHÃT HIá»†N LÆ¯U LÆ¯á»¢NG Máº NG Báº¤T THÆ¯á»œNG\")\n    print(\"   Binary Classification: Benign vs Attack\")\n    print(\"=\"*80)\n\n    # BÆ°á»›c 1: Load dá»¯ liá»‡u\n    X_train, X_val, X_test, y_train, y_val, y_test, class_weights = load_training_data(TRAINING_DATA_DIR)\n\n    # BÆ°á»›c 2: XÃ¢y dá»±ng mÃ´ hÃ¬nh\n    input_shape = (X_train.shape[1], X_train.shape[2])  # (n_features, 1)\n    model = build_cnn_model(input_shape)\n\n    # BÆ°á»›c 3: Táº¡o callbacks\n    callbacks = create_callbacks(MODEL_DIR, LOG_DIR)\n\n    # BÆ°á»›c 4: Huáº¥n luyá»‡n\n    history, training_time = train_model(\n        model, X_train, y_train, X_val, y_val, class_weights, callbacks\n    )\n\n    # BÆ°á»›c 5: ÄÃ¡nh giÃ¡\n    results, y_pred, y_pred_prob = evaluate_model(model, X_test, y_test)\n\n    # BÆ°á»›c 6: LÆ°u model vÃ  káº¿t quáº£\n    save_model_and_results(model, history, results, training_time, MODEL_DIR, y_pred, y_pred_prob)\n\n    # BÆ°á»›c 7: Váº½ biá»ƒu Ä‘á»“\n    plot_training_history(history, MODEL_DIR)\n\n    # BÆ°á»›c 8: Váº½ confusion matrix\n    if 'confusion_matrix' in results:\n        plot_confusion_matrix(results['confusion_matrix'], MODEL_DIR)\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"âœ… HOÃ€N THÃ€NH HUáº¤N LUYá»†N!\")\n    print(f\"   Test Accuracy:  {results['test_accuracy']*100:.2f}%\")\n    print(f\"   Test Precision: {results['test_precision']*100:.2f}%\")\n    print(f\"   Test Recall:    {results['test_recall']*100:.2f}%\")\n    print(f\"   Test F1-Score:  {results['test_f1_score']*100:.2f}%\")\n    print(\"=\"*80)\n\n    return model, history, results\n\n\nif __name__ == \"__main__\":\n    model, history, results = main()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T12:12:43.396114Z","iopub.execute_input":"2025-12-28T12:12:43.396494Z","iopub.status.idle":"2025-12-28T12:48:30.250915Z","shell.execute_reply.started":"2025-12-28T12:12:43.396462Z","shell.execute_reply":"2025-12-28T12:48:30.250208Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nğŸ–¥ï¸ THÃ”NG TIN Há»† THá»NG\n================================================================================\nTensorFlow version: 2.18.0\nâœ… GPU available: 2 GPU(s)\n   - PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n   - PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\nğŸŒ Äang cháº¡y trÃªn KAGGLE\n\n================================================================================\nğŸ§  HUáº¤N LUYá»†N MÃ” HÃŒNH CNN - PHÃT HIá»†N LÆ¯U LÆ¯á»¢NG Máº NG Báº¤T THÆ¯á»œNG\n   Binary Classification: Benign vs Attack\n================================================================================\n\n================================================================================\nğŸ“‚ ÄANG LOAD Dá»® LIá»†U TRAINING...\n================================================================================\n   âœ… X_train: (2100000, 69, 1)\n   âœ… X_val:   (300000, 69, 1)\n   âœ… X_test:  (600000, 69, 1)\n   âœ… y_train: (2100000,)\n   âœ… y_val:   (300000,)\n   âœ… y_test:  (600000,)\n\n   âš–ï¸ Class weights loaded:\n      Class 0 (Benign): 0.7143\n      Class 1 (Attack): 1.6667\n\n   ğŸ“Š PHÃ‚N Bá» Dá»® LIá»†U:\n      Train: Benign=1,470,000 (70.0%), Attack=630,000 (30.0%)\n      Val: Benign=210,000 (70.0%), Attack=90,000 (30.0%)\n      Test: Benign=420,000 (70.0%), Attack=180,000 (30.0%)\n\n================================================================================\nğŸ—ï¸ ÄANG XÃ‚Y Dá»°NG MÃ” HÃŒNH CNN\n================================================================================\n   Input shape: (69, 1)\n\n   ğŸ“‹ KIáº¾N TRÃšC MÃ” HÃŒNH:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"CNN_Binary_Classification\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"CNN_Binary_Classification\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m32\u001b[0m)         â”‚            \u001b[38;5;34m96\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ maxpool_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m32\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m32\u001b[0m)         â”‚         \u001b[38;5;34m2,080\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ maxpool_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m32\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚         \u001b[38;5;34m4,160\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ maxpool_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)          â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)          â”‚         \u001b[38;5;34m8,256\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ maxpool_4 (\u001b[38;5;33mMaxPooling1D\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)          â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)          â”‚         \u001b[38;5;34m8,256\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ maxpool_5 (\u001b[38;5;33mMaxPooling1D\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_norm (\u001b[38;5;33mBatchNormalization\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          â”‚           \u001b[38;5;34m256\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ flatten (\u001b[38;5;33mFlatten\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ output (\u001b[38;5;33mDense\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚           \u001b[38;5;34m129\u001b[0m â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ maxpool_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ maxpool_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ maxpool_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ maxpool_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ maxpool_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_norm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,233\u001b[0m (90.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,233</span> (90.75 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m23,105\u001b[0m (90.25 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,105</span> (90.25 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m128\u001b[0m (512.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\nğŸ“Œ ÄANG Cáº¤U HÃŒNH CALLBACKS...\n   âœ… EarlyStopping: patience=10\n   âœ… ModelCheckpoint: /kaggle/working/models/best_model.keras\n   âœ… ReduceLROnPlateau: factor=0.5, patience=5\n   âœ… TensorBoard: /kaggle/working/logs/20251228-121244\n\n================================================================================\nğŸš€ Báº®T Äáº¦U HUáº¤N LUYá»†N MÃ” HÃŒNH\n================================================================================\n   Epochs: 50\n   Batch size: 256\n   Learning rate: 0.001\n   Class weights: CÃ³\nEpoch 1/50\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9208 - loss: 0.2457 - precision: 0.8701 - recall: 0.8715\nEpoch 1: val_loss improved from inf to 0.14378, saving model to /kaggle/working/models/best_model.keras\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 6ms/step - accuracy: 0.9208 - loss: 0.2457 - precision: 0.8701 - recall: 0.8715 - val_accuracy: 0.9604 - val_loss: 0.1438 - val_precision: 0.9612 - val_recall: 0.9046 - learning_rate: 0.0010\nEpoch 2/50\n\u001b[1m8203/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9619 - loss: 0.1650 - precision: 0.9625 - recall: 0.9085\nEpoch 2: val_loss improved from 0.14378 to 0.12872, saving model to /kaggle/working/models/best_model.keras\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 6ms/step - accuracy: 0.9619 - loss: 0.1650 - precision: 0.9625 - recall: 0.9085 - val_accuracy: 0.9652 - val_loss: 0.1287 - val_precision: 0.9747 - val_recall: 0.9076 - learning_rate: 0.0010\nEpoch 3/50\n\u001b[1m8200/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9660 - loss: 0.1524 - precision: 0.9739 - recall: 0.9111\nEpoch 3: val_loss improved from 0.12872 to 0.12308, saving model to /kaggle/working/models/best_model.keras\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 6ms/step - accuracy: 0.9660 - loss: 0.1524 - precision: 0.9739 - recall: 0.9111 - val_accuracy: 0.9667 - val_loss: 0.1231 - val_precision: 0.9853 - val_recall: 0.9026 - learning_rate: 0.0010\nEpoch 4/50\n\u001b[1m8200/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9678 - loss: 0.1468 - precision: 0.9785 - recall: 0.9126\nEpoch 4: val_loss did not improve from 0.12308\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 5ms/step - accuracy: 0.9678 - loss: 0.1468 - precision: 0.9785 - recall: 0.9126 - val_accuracy: 0.9652 - val_loss: 0.1257 - val_precision: 0.9895 - val_recall: 0.8935 - learning_rate: 0.0010\nEpoch 5/50\n\u001b[1m8203/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9687 - loss: 0.1440 - precision: 0.9805 - recall: 0.9138\nEpoch 5: val_loss did not improve from 0.12308\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 5ms/step - accuracy: 0.9687 - loss: 0.1440 - precision: 0.9805 - recall: 0.9138 - val_accuracy: 0.9686 - val_loss: 0.1238 - val_precision: 0.9907 - val_recall: 0.9037 - learning_rate: 0.0010\nEpoch 6/50\n\u001b[1m8201/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9691 - loss: 0.1421 - precision: 0.9814 - recall: 0.9142\nEpoch 6: val_loss did not improve from 0.12308\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 5ms/step - accuracy: 0.9691 - loss: 0.1421 - precision: 0.9814 - recall: 0.9142 - val_accuracy: 0.9661 - val_loss: 0.1389 - val_precision: 0.9678 - val_recall: 0.9174 - learning_rate: 0.0010\nEpoch 7/50\n\u001b[1m8199/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9697 - loss: 0.1401 - precision: 0.9829 - recall: 0.9150\nEpoch 7: val_loss improved from 0.12308 to 0.11348, saving model to /kaggle/working/models/best_model.keras\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 5ms/step - accuracy: 0.9697 - loss: 0.1401 - precision: 0.9829 - recall: 0.9150 - val_accuracy: 0.9715 - val_loss: 0.1135 - val_precision: 0.9892 - val_recall: 0.9150 - learning_rate: 0.0010\nEpoch 8/50\n\u001b[1m8194/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9700 - loss: 0.1391 - precision: 0.9837 - recall: 0.9153\nEpoch 8: val_loss did not improve from 0.11348\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 5ms/step - accuracy: 0.9700 - loss: 0.1391 - precision: 0.9837 - recall: 0.9153 - val_accuracy: 0.9719 - val_loss: 0.1142 - val_precision: 0.9890 - val_recall: 0.9165 - learning_rate: 0.0010\nEpoch 9/50\n\u001b[1m8203/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9702 - loss: 0.1388 - precision: 0.9841 - recall: 0.9154\nEpoch 9: val_loss did not improve from 0.11348\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5ms/step - accuracy: 0.9702 - loss: 0.1388 - precision: 0.9841 - recall: 0.9154 - val_accuracy: 0.9659 - val_loss: 0.1213 - val_precision: 0.9911 - val_recall: 0.8943 - learning_rate: 0.0010\nEpoch 10/50\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9704 - loss: 0.1378 - precision: 0.9846 - recall: 0.9157\nEpoch 10: val_loss improved from 0.11348 to 0.10998, saving model to /kaggle/working/models/best_model.keras\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5ms/step - accuracy: 0.9704 - loss: 0.1378 - precision: 0.9846 - recall: 0.9157 - val_accuracy: 0.9725 - val_loss: 0.1100 - val_precision: 0.9931 - val_recall: 0.9145 - learning_rate: 0.0010\nEpoch 11/50\n\u001b[1m8195/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9709 - loss: 0.1363 - precision: 0.9856 - recall: 0.9163\nEpoch 11: val_loss improved from 0.10998 to 0.10765, saving model to /kaggle/working/models/best_model.keras\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9709 - loss: 0.1363 - precision: 0.9856 - recall: 0.9163 - val_accuracy: 0.9729 - val_loss: 0.1077 - val_precision: 0.9943 - val_recall: 0.9147 - learning_rate: 0.0010\nEpoch 12/50\n\u001b[1m8194/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9709 - loss: 0.1359 - precision: 0.9856 - recall: 0.9164\nEpoch 12: val_loss did not improve from 0.10765\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9709 - loss: 0.1359 - precision: 0.9856 - recall: 0.9164 - val_accuracy: 0.9727 - val_loss: 0.1094 - val_precision: 0.9950 - val_recall: 0.9135 - learning_rate: 0.0010\nEpoch 13/50\n\u001b[1m8195/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9711 - loss: 0.1353 - precision: 0.9859 - recall: 0.9166\nEpoch 13: val_loss improved from 0.10765 to 0.10705, saving model to /kaggle/working/models/best_model.keras\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9711 - loss: 0.1353 - precision: 0.9859 - recall: 0.9166 - val_accuracy: 0.9725 - val_loss: 0.1071 - val_precision: 0.9923 - val_recall: 0.9155 - learning_rate: 0.0010\nEpoch 14/50\n\u001b[1m8203/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9712 - loss: 0.1349 - precision: 0.9864 - recall: 0.9167\nEpoch 14: val_loss did not improve from 0.10705\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9712 - loss: 0.1349 - precision: 0.9864 - recall: 0.9167 - val_accuracy: 0.9722 - val_loss: 0.1099 - val_precision: 0.9946 - val_recall: 0.9123 - learning_rate: 0.0010\nEpoch 15/50\n\u001b[1m8195/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9713 - loss: 0.1343 - precision: 0.9864 - recall: 0.9169\nEpoch 15: val_loss did not improve from 0.10705\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 5ms/step - accuracy: 0.9713 - loss: 0.1343 - precision: 0.9864 - recall: 0.9169 - val_accuracy: 0.9724 - val_loss: 0.1094 - val_precision: 0.9911 - val_recall: 0.9161 - learning_rate: 0.0010\nEpoch 16/50\n\u001b[1m8197/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9714 - loss: 0.1340 - precision: 0.9864 - recall: 0.9172\nEpoch 16: val_loss did not improve from 0.10705\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9714 - loss: 0.1340 - precision: 0.9864 - recall: 0.9172 - val_accuracy: 0.9725 - val_loss: 0.1077 - val_precision: 0.9925 - val_recall: 0.9155 - learning_rate: 0.0010\nEpoch 17/50\n\u001b[1m8195/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9714 - loss: 0.1339 - precision: 0.9866 - recall: 0.9171\nEpoch 17: val_loss did not improve from 0.10705\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9714 - loss: 0.1339 - precision: 0.9866 - recall: 0.9171 - val_accuracy: 0.9727 - val_loss: 0.1087 - val_precision: 0.9934 - val_recall: 0.9152 - learning_rate: 0.0010\nEpoch 18/50\n\u001b[1m8200/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9713 - loss: 0.1336 - precision: 0.9865 - recall: 0.9171\nEpoch 18: val_loss did not improve from 0.10705\n\nEpoch 18: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9713 - loss: 0.1336 - precision: 0.9865 - recall: 0.9171 - val_accuracy: 0.9729 - val_loss: 0.1072 - val_precision: 0.9956 - val_recall: 0.9136 - learning_rate: 0.0010\nEpoch 19/50\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9721 - loss: 0.1310 - precision: 0.9881 - recall: 0.9180\nEpoch 19: val_loss improved from 0.10705 to 0.10415, saving model to /kaggle/working/models/best_model.keras\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 5ms/step - accuracy: 0.9721 - loss: 0.1310 - precision: 0.9881 - recall: 0.9180 - val_accuracy: 0.9735 - val_loss: 0.1041 - val_precision: 0.9953 - val_recall: 0.9161 - learning_rate: 5.0000e-04\nEpoch 20/50\n\u001b[1m8195/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9722 - loss: 0.1305 - precision: 0.9880 - recall: 0.9183\nEpoch 20: val_loss improved from 0.10415 to 0.10358, saving model to /kaggle/working/models/best_model.keras\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 5ms/step - accuracy: 0.9722 - loss: 0.1305 - precision: 0.9880 - recall: 0.9183 - val_accuracy: 0.9734 - val_loss: 0.1036 - val_precision: 0.9948 - val_recall: 0.9162 - learning_rate: 5.0000e-04\nEpoch 21/50\n\u001b[1m8194/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9723 - loss: 0.1301 - precision: 0.9880 - recall: 0.9187\nEpoch 21: val_loss improved from 0.10358 to 0.10318, saving model to /kaggle/working/models/best_model.keras\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9723 - loss: 0.1301 - precision: 0.9880 - recall: 0.9187 - val_accuracy: 0.9735 - val_loss: 0.1032 - val_precision: 0.9948 - val_recall: 0.9165 - learning_rate: 5.0000e-04\nEpoch 22/50\n\u001b[1m8199/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9722 - loss: 0.1300 - precision: 0.9878 - recall: 0.9187\nEpoch 22: val_loss improved from 0.10318 to 0.10256, saving model to /kaggle/working/models/best_model.keras\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5ms/step - accuracy: 0.9722 - loss: 0.1300 - precision: 0.9878 - recall: 0.9187 - val_accuracy: 0.9736 - val_loss: 0.1026 - val_precision: 0.9959 - val_recall: 0.9157 - learning_rate: 5.0000e-04\nEpoch 23/50\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9722 - loss: 0.1300 - precision: 0.9879 - recall: 0.9186\nEpoch 23: val_loss did not improve from 0.10256\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9722 - loss: 0.1300 - precision: 0.9879 - recall: 0.9186 - val_accuracy: 0.9734 - val_loss: 0.1039 - val_precision: 0.9953 - val_recall: 0.9157 - learning_rate: 5.0000e-04\nEpoch 24/50\n\u001b[1m8195/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9723 - loss: 0.1296 - precision: 0.9880 - recall: 0.9189\nEpoch 24: val_loss did not improve from 0.10256\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5ms/step - accuracy: 0.9723 - loss: 0.1296 - precision: 0.9880 - recall: 0.9189 - val_accuracy: 0.9732 - val_loss: 0.1044 - val_precision: 0.9960 - val_recall: 0.9142 - learning_rate: 5.0000e-04\nEpoch 25/50\n\u001b[1m8200/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9722 - loss: 0.1297 - precision: 0.9875 - recall: 0.9189\nEpoch 25: val_loss did not improve from 0.10256\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9722 - loss: 0.1297 - precision: 0.9875 - recall: 0.9189 - val_accuracy: 0.9735 - val_loss: 0.1027 - val_precision: 0.9960 - val_recall: 0.9155 - learning_rate: 5.0000e-04\nEpoch 26/50\n\u001b[1m8203/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9723 - loss: 0.1292 - precision: 0.9876 - recall: 0.9191\nEpoch 26: val_loss improved from 0.10256 to 0.10242, saving model to /kaggle/working/models/best_model.keras\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5ms/step - accuracy: 0.9723 - loss: 0.1292 - precision: 0.9876 - recall: 0.9191 - val_accuracy: 0.9734 - val_loss: 0.1024 - val_precision: 0.9962 - val_recall: 0.9150 - learning_rate: 5.0000e-04\nEpoch 27/50\n\u001b[1m8203/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9723 - loss: 0.1294 - precision: 0.9876 - recall: 0.9191\nEpoch 27: val_loss did not improve from 0.10242\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9723 - loss: 0.1294 - precision: 0.9876 - recall: 0.9191 - val_accuracy: 0.9735 - val_loss: 0.1035 - val_precision: 0.9961 - val_recall: 0.9153 - learning_rate: 5.0000e-04\nEpoch 28/50\n\u001b[1m8193/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9722 - loss: 0.1291 - precision: 0.9876 - recall: 0.9189\nEpoch 28: val_loss did not improve from 0.10242\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9722 - loss: 0.1291 - precision: 0.9876 - recall: 0.9189 - val_accuracy: 0.9736 - val_loss: 0.1027 - val_precision: 0.9959 - val_recall: 0.9157 - learning_rate: 5.0000e-04\nEpoch 29/50\n\u001b[1m8198/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9723 - loss: 0.1290 - precision: 0.9876 - recall: 0.9192\nEpoch 29: val_loss did not improve from 0.10242\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9723 - loss: 0.1290 - precision: 0.9876 - recall: 0.9192 - val_accuracy: 0.9735 - val_loss: 0.1025 - val_precision: 0.9959 - val_recall: 0.9155 - learning_rate: 5.0000e-04\nEpoch 30/50\n\u001b[1m8197/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9723 - loss: 0.1289 - precision: 0.9878 - recall: 0.9191\nEpoch 30: val_loss did not improve from 0.10242\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9723 - loss: 0.1289 - precision: 0.9878 - recall: 0.9191 - val_accuracy: 0.9736 - val_loss: 0.1038 - val_precision: 0.9957 - val_recall: 0.9158 - learning_rate: 5.0000e-04\nEpoch 31/50\n\u001b[1m8196/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9723 - loss: 0.1290 - precision: 0.9877 - recall: 0.9192\nEpoch 31: val_loss did not improve from 0.10242\n\nEpoch 31: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 5ms/step - accuracy: 0.9723 - loss: 0.1290 - precision: 0.9877 - recall: 0.9192 - val_accuracy: 0.9733 - val_loss: 0.1049 - val_precision: 0.9966 - val_recall: 0.9139 - learning_rate: 5.0000e-04\nEpoch 32/50\n\u001b[1m8194/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9725 - loss: 0.1277 - precision: 0.9878 - recall: 0.9197\nEpoch 32: val_loss improved from 0.10242 to 0.10139, saving model to /kaggle/working/models/best_model.keras\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 5ms/step - accuracy: 0.9725 - loss: 0.1277 - precision: 0.9878 - recall: 0.9197 - val_accuracy: 0.9737 - val_loss: 0.1014 - val_precision: 0.9964 - val_recall: 0.9156 - learning_rate: 2.5000e-04\nEpoch 33/50\n\u001b[1m8194/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9725 - loss: 0.1273 - precision: 0.9876 - recall: 0.9200\nEpoch 33: val_loss did not improve from 0.10139\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 5ms/step - accuracy: 0.9725 - loss: 0.1273 - precision: 0.9876 - recall: 0.9200 - val_accuracy: 0.9739 - val_loss: 0.1017 - val_precision: 0.9964 - val_recall: 0.9161 - learning_rate: 2.5000e-04\nEpoch 34/50\n\u001b[1m8199/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9725 - loss: 0.1273 - precision: 0.9876 - recall: 0.9199\nEpoch 34: val_loss improved from 0.10139 to 0.10067, saving model to /kaggle/working/models/best_model.keras\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5ms/step - accuracy: 0.9725 - loss: 0.1273 - precision: 0.9876 - recall: 0.9199 - val_accuracy: 0.9738 - val_loss: 0.1007 - val_precision: 0.9964 - val_recall: 0.9158 - learning_rate: 2.5000e-04\nEpoch 35/50\n\u001b[1m8193/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9725 - loss: 0.1271 - precision: 0.9873 - recall: 0.9201\nEpoch 35: val_loss did not improve from 0.10067\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 5ms/step - accuracy: 0.9725 - loss: 0.1271 - precision: 0.9873 - recall: 0.9201 - val_accuracy: 0.9739 - val_loss: 0.1012 - val_precision: 0.9963 - val_recall: 0.9163 - learning_rate: 2.5000e-04\nEpoch 36/50\n\u001b[1m8203/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9725 - loss: 0.1268 - precision: 0.9872 - recall: 0.9201\nEpoch 36: val_loss did not improve from 0.10067\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5ms/step - accuracy: 0.9725 - loss: 0.1268 - precision: 0.9872 - recall: 0.9201 - val_accuracy: 0.9737 - val_loss: 0.1015 - val_precision: 0.9967 - val_recall: 0.9153 - learning_rate: 2.5000e-04\nEpoch 37/50\n\u001b[1m8196/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9725 - loss: 0.1270 - precision: 0.9871 - recall: 0.9202\nEpoch 37: val_loss did not improve from 0.10067\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9725 - loss: 0.1270 - precision: 0.9871 - recall: 0.9202 - val_accuracy: 0.9737 - val_loss: 0.1013 - val_precision: 0.9965 - val_recall: 0.9157 - learning_rate: 2.5000e-04\nEpoch 38/50\n\u001b[1m8193/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9725 - loss: 0.1270 - precision: 0.9875 - recall: 0.9200\nEpoch 38: val_loss did not improve from 0.10067\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9725 - loss: 0.1270 - precision: 0.9875 - recall: 0.9200 - val_accuracy: 0.9738 - val_loss: 0.1020 - val_precision: 0.9965 - val_recall: 0.9159 - learning_rate: 2.5000e-04\nEpoch 39/50\n\u001b[1m8194/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9726 - loss: 0.1269 - precision: 0.9876 - recall: 0.9203\nEpoch 39: val_loss did not improve from 0.10067\n\nEpoch 39: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9726 - loss: 0.1269 - precision: 0.9876 - recall: 0.9203 - val_accuracy: 0.9735 - val_loss: 0.1022 - val_precision: 0.9970 - val_recall: 0.9146 - learning_rate: 2.5000e-04\nEpoch 40/50\n\u001b[1m8203/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9727 - loss: 0.1262 - precision: 0.9878 - recall: 0.9204\nEpoch 40: val_loss improved from 0.10067 to 0.10063, saving model to /kaggle/working/models/best_model.keras\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5ms/step - accuracy: 0.9727 - loss: 0.1262 - precision: 0.9878 - recall: 0.9204 - val_accuracy: 0.9739 - val_loss: 0.1006 - val_precision: 0.9967 - val_recall: 0.9160 - learning_rate: 1.2500e-04\nEpoch 41/50\n\u001b[1m8199/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9727 - loss: 0.1258 - precision: 0.9873 - recall: 0.9207\nEpoch 41: val_loss did not improve from 0.10063\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5ms/step - accuracy: 0.9727 - loss: 0.1258 - precision: 0.9873 - recall: 0.9207 - val_accuracy: 0.9737 - val_loss: 0.1014 - val_precision: 0.9966 - val_recall: 0.9156 - learning_rate: 1.2500e-04\nEpoch 42/50\n\u001b[1m8197/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9726 - loss: 0.1258 - precision: 0.9875 - recall: 0.9204\nEpoch 42: val_loss did not improve from 0.10063\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9726 - loss: 0.1258 - precision: 0.9875 - recall: 0.9204 - val_accuracy: 0.9737 - val_loss: 0.1014 - val_precision: 0.9968 - val_recall: 0.9151 - learning_rate: 1.2500e-04\nEpoch 43/50\n\u001b[1m8200/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9727 - loss: 0.1256 - precision: 0.9873 - recall: 0.9209\nEpoch 43: val_loss did not improve from 0.10063\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5ms/step - accuracy: 0.9727 - loss: 0.1256 - precision: 0.9873 - recall: 0.9209 - val_accuracy: 0.9737 - val_loss: 0.1010 - val_precision: 0.9969 - val_recall: 0.9151 - learning_rate: 1.2500e-04\nEpoch 44/50\n\u001b[1m8203/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9726 - loss: 0.1256 - precision: 0.9871 - recall: 0.9206\nEpoch 44: val_loss did not improve from 0.10063\n\nEpoch 44: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9726 - loss: 0.1256 - precision: 0.9871 - recall: 0.9206 - val_accuracy: 0.9737 - val_loss: 0.1009 - val_precision: 0.9970 - val_recall: 0.9152 - learning_rate: 1.2500e-04\nEpoch 45/50\n\u001b[1m8194/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9727 - loss: 0.1253 - precision: 0.9873 - recall: 0.9208\nEpoch 45: val_loss improved from 0.10063 to 0.10020, saving model to /kaggle/working/models/best_model.keras\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9727 - loss: 0.1253 - precision: 0.9873 - recall: 0.9208 - val_accuracy: 0.9739 - val_loss: 0.1002 - val_precision: 0.9967 - val_recall: 0.9159 - learning_rate: 6.2500e-05\nEpoch 46/50\n\u001b[1m8203/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9727 - loss: 0.1250 - precision: 0.9874 - recall: 0.9209\nEpoch 46: val_loss improved from 0.10020 to 0.10013, saving model to /kaggle/working/models/best_model.keras\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9727 - loss: 0.1250 - precision: 0.9874 - recall: 0.9209 - val_accuracy: 0.9740 - val_loss: 0.1001 - val_precision: 0.9975 - val_recall: 0.9157 - learning_rate: 6.2500e-05\nEpoch 47/50\n\u001b[1m8198/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9727 - loss: 0.1251 - precision: 0.9872 - recall: 0.9211\nEpoch 47: val_loss improved from 0.10013 to 0.10002, saving model to /kaggle/working/models/best_model.keras\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9727 - loss: 0.1251 - precision: 0.9872 - recall: 0.9211 - val_accuracy: 0.9741 - val_loss: 0.1000 - val_precision: 0.9975 - val_recall: 0.9158 - learning_rate: 6.2500e-05\nEpoch 48/50\n\u001b[1m8201/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9728 - loss: 0.1250 - precision: 0.9874 - recall: 0.9211\nEpoch 48: val_loss improved from 0.10002 to 0.09981, saving model to /kaggle/working/models/best_model.keras\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 5ms/step - accuracy: 0.9728 - loss: 0.1250 - precision: 0.9874 - recall: 0.9211 - val_accuracy: 0.9739 - val_loss: 0.0998 - val_precision: 0.9966 - val_recall: 0.9160 - learning_rate: 6.2500e-05\nEpoch 49/50\n\u001b[1m8193/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9728 - loss: 0.1250 - precision: 0.9873 - recall: 0.9210\nEpoch 49: val_loss improved from 0.09981 to 0.09955, saving model to /kaggle/working/models/best_model.keras\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 5ms/step - accuracy: 0.9728 - loss: 0.1250 - precision: 0.9873 - recall: 0.9210 - val_accuracy: 0.9739 - val_loss: 0.0995 - val_precision: 0.9969 - val_recall: 0.9157 - learning_rate: 6.2500e-05\nEpoch 50/50\n\u001b[1m8198/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9728 - loss: 0.1250 - precision: 0.9875 - recall: 0.9211\nEpoch 50: val_loss did not improve from 0.09955\n\u001b[1m8204/8204\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 5ms/step - accuracy: 0.9728 - loss: 0.1250 - precision: 0.9875 - recall: 0.9211 - val_accuracy: 0.9737 - val_loss: 0.1002 - val_precision: 0.9970 - val_recall: 0.9152 - learning_rate: 6.2500e-05\nRestoring model weights from the end of the best epoch: 49.\n\n   â±ï¸ Thá»i gian training: 34.39 phÃºt\n   ğŸ“ˆ Best val_loss: 0.0995\n   ğŸ“ˆ Best val_accuracy: 0.9741\n\n================================================================================\nğŸ“Š ÄÃNH GIÃ MÃ” HÃŒNH TRÃŠN TEST SET\n================================================================================\n\u001b[1m18750/18750\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2ms/step - accuracy: 0.9732 - loss: 0.1009 - precision: 0.9968 - recall: 0.9136\n\n   ğŸ“Š Káº¾T QUáº¢:\n   ========================================\n   Loss:      0.1005\n   Accuracy:  0.9734 (97.34%)\n   Precision: 0.9967\n   Recall:    0.9143\n   F1-Score:  0.9537\n   ========================================\n\n   ğŸ“‹ CONFUSION MATRIX:\n                 Predicted\n                 Benign  Attack\n   Actual Benign  419453     547\n   Actual Attack   15428  164572\n\n   ğŸ“‹ CLASSIFICATION REPORT:\n              precision    recall  f1-score   support\n\n      Benign       0.96      1.00      0.98    420000\n      Attack       1.00      0.91      0.95    180000\n\n    accuracy                           0.97    600000\n   macro avg       0.98      0.96      0.97    600000\nweighted avg       0.97      0.97      0.97    600000\n\n\n================================================================================\nğŸ’¾ ÄANG LÆ¯U MODEL VÃ€ Káº¾T QUáº¢...\n================================================================================\n   âœ… Final model: /kaggle/working/models/final_model.keras\n   âœ… Model weights: /kaggle/working/models/model_weights.weights.h5\n   âœ… Training history: training_history.json\n   âœ… Evaluation results: evaluation_results.json\n   âœ… Predictions: y_pred.npy\n   âœ… Prediction probabilities: y_pred_prob.npy\n   âœ… Training config: training_config.json\n\nğŸ“ Táº¥t cáº£ file Ä‘Æ°á»£c lÆ°u táº¡i: /kaggle/working/models\n   âœ… Training history plot: training_history.png\n   âœ… Confusion matrix plot: confusion_matrix.png\n   âœ… Normalized confusion matrix plot: confusion_matrix_normalized.png\n\n================================================================================\nâœ… HOÃ€N THÃ€NH HUáº¤N LUYá»†N!\n   Test Accuracy:  97.34%\n   Test Precision: 99.67%\n   Test Recall:    91.43%\n   Test F1-Score:  95.37%\n================================================================================\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!rm -rf /kaggle/working/models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T12:08:38.588168Z","iopub.execute_input":"2025-12-28T12:08:38.588875Z","iopub.status.idle":"2025-12-28T12:08:38.878153Z","shell.execute_reply.started":"2025-12-28T12:08:38.588842Z","shell.execute_reply":"2025-12-28T12:08:38.877081Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\"\"\"\n======================================================================================\nTIá»€N Xá»¬ LÃ DATASET CICIDS2018 CHO MÃ” HÃŒNH CNN - PHÃT HIá»†N LÆ¯U LÆ¯á»¢NG Máº NG IOT Báº¤T THÆ¯á»œNG\n======================================================================================\n\nScript nÃ y thá»±c hiá»‡n cÃ¡c bÆ°á»›c tiá»n xá»­ lÃ½ dá»¯ liá»‡u:\n1. Äá»c tá»«ng file CSV theo chunks Ä‘á»ƒ tá»‘i Æ°u bá»™ nhá»›\n2. Loáº¡i bá» cÃ¡c cá»™t khÃ´ng cáº§n thiáº¿t (IP, Port, Timestamp, Flow ID)\n3. Xá»­ lÃ½ missing values, NaN, Inf\n4. Loáº¡i bá» cÃ¡c hÃ ng trÃ¹ng láº·p\n5. Chuyá»ƒn Ä‘á»•i nhÃ£n sang dáº¡ng binary (Benign=0, Attack=1)\n6. Chuáº©n hÃ³a dá»¯ liá»‡u báº±ng StandardScaler\n7. LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ sang Ä‘á»‹nh dáº¡ng nhanh (parquet/npy)\n\nCÃ³ thá»ƒ cháº¡y trÃªn cáº£ Kaggle vÃ  Local\n\"\"\"\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport json\nimport gc\nfrom pathlib import Path\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ============================================================================\n# THÆ¯ VIá»†N CHUáº¨N HÃ“A VÃ€ Xá»¬ LÃ Dá»® LIá»†U\n# ============================================================================\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\n# Kiá»ƒm tra mÃ´i trÆ°á»ng cháº¡y (Kaggle hoáº·c Local)\nIS_KAGGLE = os.path.exists('/kaggle/input')\n\n# Progress bar\ntry:\n    from tqdm import tqdm\n    TQDM_AVAILABLE = True\nexcept ImportError:\n    TQDM_AVAILABLE = False\n    print(\"âš ï¸  tqdm khÃ´ng cÃ³ sáºµn. CÃ i Ä‘áº·t báº±ng: pip install tqdm\")\n    tqdm = lambda x, **kwargs: x\n\n# ============================================================================\n# Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN\n# ============================================================================\nif IS_KAGGLE:\n    # ÄÆ°á»ng dáº«n trÃªn Kaggle - thay Ä‘á»•i theo dataset cá»§a báº¡n\n    DATA_DIR = \"/kaggle/input/cicids2018\"  # Thay Ä‘á»•i náº¿u tÃªn dataset khÃ¡c\n    OUTPUT_DIR = \"/kaggle/working/processed_data_cnn\"\n    print(\" Äang cháº¡y trÃªn KAGGLE\")\nelse:\n    # ÄÆ°á»ng dáº«n Local\n    DATA_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CICIDS2018-CSV\"\n    OUTPUT_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CNN\\processed_data_cnn\"\n    print(\" Äang cháº¡y trÃªn LOCAL\")\n\n# ============================================================================\n# Cáº¤U HÃŒNH Xá»¬ LÃ Dá»® LIá»†U\n# ============================================================================\n\n# KÃ­ch thÆ°á»›c chunk khi Ä‘á»c CSV (Ä‘iá»u chá»‰nh theo RAM cá»§a mÃ¡y)\nCHUNK_SIZE = 300000  # 300k rows má»—i chunk\n\n# Random state Ä‘á»ƒ tÃ¡i táº¡o káº¿t quáº£\nRANDOM_STATE = 42\n\n# Loáº¡i scaler: 'standard' (StandardScaler) hoáº·c 'minmax' (MinMaxScaler)\nSCALER_TYPE = 'standard'\n\n# ============================================================================\n# Cáº¤U HÃŒNH SAMPLE CÃ‚N Báº°NG\n# ============================================================================\n# Tá»•ng sá»‘ máº«u mong muá»‘n (train + val + test)\nTOTAL_SAMPLES = 3500000  # 3 triá»‡u máº«u\n\n# Tá»· lá»‡ pháº§n trÄƒm cho má»—i class\nBENIGN_RATIO = 0.70  # 70% Benign = 2,100,000 máº«u\nATTACK_RATIO = 0.30  # 30% Attack = 900,000 máº«u\n\n# TÃ­nh sá»‘ lÆ°á»£ng máº«u cho má»—i class\nTARGET_BENIGN = int(TOTAL_SAMPLES * BENIGN_RATIO)  # 2,100,000\nTARGET_ATTACK = int(TOTAL_SAMPLES * ATTACK_RATIO)  # 900,000\n\n# ============================================================================\n# DANH SÃCH CÃC Cá»˜T Cáº¦N LOáº I Bá»\n# ============================================================================\n\n# CÃ¡c cá»™t khÃ´ng cáº§n thiáº¿t cho viá»‡c huáº¥n luyá»‡n CNN\nCOLUMNS_TO_DROP = [\n    # ThÃ´ng tin Ä‘á»‹nh danh - khÃ´ng mang tÃ­nh tá»•ng quÃ¡t\n    'Flow ID',          # ID duy nháº¥t cho má»—i flow\n    'Src IP',           # IP nguá»“n\n    'Dst IP',           # IP Ä‘Ã­ch\n    'Src Port',         # Port nguá»“n\n    'Timestamp',        # Thá»i gian - khÃ´ng liÃªn quan Ä‘áº¿n pattern\n\n    # CÃ¡c cá»™t flag khÃ´ng mang nhiá»u thÃ´ng tin\n    # 'Bwd PSH Flags',    # Hiáº¿m khi cÃ³ giÃ¡ trá»‹ khÃ¡c 0\n    # 'Bwd URG Flags',    # Hiáº¿m khi cÃ³ giÃ¡ trá»‹ khÃ¡c 0\n    # 'Fwd URG Flags',    # Hiáº¿m khi cÃ³ giÃ¡ trá»‹ khÃ¡c 0\n]\n\n# Cá»™t nhÃ£n\nLABEL_COLUMN = 'Label'\n\n# ============================================================================\n# CLASS Xá»¬ LÃ Dá»® LIá»†U CHO CNN\n# ============================================================================\n\nclass CICIDS2018_CNN_Preprocessor:\n    \"\"\"\n    Class xá»­ lÃ½ dá»¯ liá»‡u CICIDS2018 cho mÃ´ hÃ¬nh CNN phÃ¡t hiá»‡n báº¥t thÆ°á»ng\n\n    CÃ¡c bÆ°á»›c xá»­ lÃ½:\n    1. Äá»c dá»¯ liá»‡u theo chunks\n    2. Loáº¡i bá» cá»™t khÃ´ng cáº§n thiáº¿t\n    3. Xá»­ lÃ½ giÃ¡ trá»‹ thiáº¿u, NaN, Inf\n    4. Loáº¡i bá» duplicate\n    5. Chuyá»ƒn Ä‘á»•i nhÃ£n sang binary\n    6. Chuáº©n hÃ³a features\n    7. LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½\n    \"\"\"\n\n    def __init__(self, data_dir, output_dir, chunk_size=CHUNK_SIZE,\n                 scaler_type=SCALER_TYPE, target_benign=TARGET_BENIGN,\n                 target_attack=TARGET_ATTACK):\n        \"\"\"\n        Khá»Ÿi táº¡o preprocessor\n\n        Args:\n            data_dir: ÄÆ°á»ng dáº«n thÆ° má»¥c chá»©a file CSV\n            output_dir: ÄÆ°á»ng dáº«n thÆ° má»¥c lÆ°u káº¿t quáº£\n            chunk_size: Sá»‘ dÃ²ng má»—i chunk khi Ä‘á»c CSV\n            scaler_type: Loáº¡i scaler ('standard' hoáº·c 'minmax')\n            target_benign: Sá»‘ lÆ°á»£ng máº«u Benign mong muá»‘n\n            target_attack: Sá»‘ lÆ°á»£ng máº«u Attack mong muá»‘n\n        \"\"\"\n        self.data_dir = Path(data_dir)\n        self.output_dir = Path(output_dir)\n        self.chunk_size = chunk_size\n        self.scaler_type = scaler_type\n        self.target_benign = target_benign\n        self.target_attack = target_attack\n\n        # Khá»Ÿi táº¡o scaler\n        if scaler_type == 'minmax':\n            self.scaler = MinMaxScaler()\n        else:\n            self.scaler = StandardScaler()\n\n        # Táº¡o thÆ° má»¥c output náº¿u chÆ°a cÃ³\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n        # Thá»‘ng kÃª\n        self.stats = {\n            'total_rows_read': 0,\n            'rows_after_cleaning': 0,\n            'duplicates_removed': 0,\n            'nan_inf_replaced': 0,\n            'benign_count': 0,\n            'attack_count': 0,\n            'feature_count': 0,\n            'processing_time': 0\n        }\n\n        # LÆ°u tÃªn cÃ¡c features\n        self.feature_names = None\n\n    def _get_csv_files(self):\n        \"\"\"Láº¥y danh sÃ¡ch cÃ¡c file CSV trong thÆ° má»¥c data\"\"\"\n        csv_files = list(self.data_dir.glob(\"*_TrafficForML_CICFlowMeter.csv\"))\n        if not csv_files:\n            # Thá»­ pattern khÃ¡c cho Kaggle\n            csv_files = list(self.data_dir.glob(\"*.csv\"))\n\n        if not csv_files:\n            raise FileNotFoundError(f\"KhÃ´ng tÃ¬m tháº¥y file CSV trong {self.data_dir}\")\n\n        print(f\"\\nğŸ“‚ TÃ¬m tháº¥y {len(csv_files)} file CSV:\")\n        for f in sorted(csv_files):\n            print(f\"   - {f.name}\")\n        return sorted(csv_files)\n\n    def _clean_column_names(self, df):\n        \"\"\"Chuáº©n hÃ³a tÃªn cá»™t (loáº¡i bá» khoáº£ng tráº¯ng thá»«a)\"\"\"\n        df.columns = df.columns.str.strip()\n        return df\n\n    def _drop_unnecessary_columns(self, df):\n        \"\"\"Loáº¡i bá» cÃ¡c cá»™t khÃ´ng cáº§n thiáº¿t cho huáº¥n luyá»‡n\"\"\"\n        columns_to_drop = [col for col in COLUMNS_TO_DROP if col in df.columns]\n\n        if columns_to_drop:\n            df = df.drop(columns=columns_to_drop)\n\n        return df\n\n    def _convert_to_numeric(self, df):\n        \"\"\"Chuyá»ƒn Ä‘á»•i cÃ¡c cá»™t vá» dáº¡ng sá»‘\"\"\"\n        # Láº¥y táº¥t cáº£ cá»™t trá»« Label\n        feature_cols = [col for col in df.columns if col != LABEL_COLUMN]\n\n        for col in feature_cols:\n            if df[col].dtype == 'object':\n                df[col] = pd.to_numeric(df[col], errors='coerce')\n\n        return df\n\n    def _handle_nan_inf(self, df):\n        \"\"\"Xá»­ lÃ½ giÃ¡ trá»‹ NaN vÃ  Infinity\"\"\"\n        feature_cols = [col for col in df.columns if col != LABEL_COLUMN]\n\n        # Äáº¿m sá»‘ lÆ°á»£ng NaN vÃ  Inf trÆ°á»›c khi xá»­ lÃ½\n        nan_count = df[feature_cols].isna().sum().sum()\n        inf_count = np.isinf(df[feature_cols].select_dtypes(include=[np.number])).sum().sum()\n\n        self.stats['nan_inf_replaced'] += nan_count + inf_count\n\n        # Thay tháº¿ Infinity báº±ng NaN trÆ°á»›c\n        df[feature_cols] = df[feature_cols].replace([np.inf, -np.inf], np.nan)\n\n        # Thay tháº¿ NaN báº±ng 0 (hoáº·c cÃ³ thá»ƒ dÃ¹ng median/mean)\n        df[feature_cols] = df[feature_cols].fillna(0)\n\n        return df\n\n    def _remove_duplicates(self, df):\n        \"\"\"Loáº¡i bá» cÃ¡c hÃ ng trÃ¹ng láº·p\"\"\"\n        rows_before = len(df)\n        df = df.drop_duplicates()\n        rows_after = len(df)\n\n        self.stats['duplicates_removed'] += (rows_before - rows_after)\n\n        return df\n\n    def _convert_to_binary_label(self, df):\n        \"\"\"\n        Chuyá»ƒn Ä‘á»•i nhÃ£n sang dáº¡ng binary:\n        - Benign -> 0 (lÆ°u lÆ°á»£ng bÃ¬nh thÆ°á»ng)\n        - Táº¥t cáº£ cÃ¡c loáº¡i táº¥n cÃ´ng khÃ¡c -> 1 (lÆ°u lÆ°á»£ng báº¥t thÆ°á»ng)\n        \"\"\"\n        if LABEL_COLUMN not in df.columns:\n            raise ValueError(f\"KhÃ´ng tÃ¬m tháº¥y cá»™t '{LABEL_COLUMN}' trong dá»¯ liá»‡u\")\n\n        # Chuáº©n hÃ³a nhÃ£n (loáº¡i bá» khoáº£ng tráº¯ng, lowercase)\n        df[LABEL_COLUMN] = df[LABEL_COLUMN].astype(str).str.strip().str.lower()\n\n        # Loáº¡i bá» cÃ¡c hÃ ng cÃ³ nhÃ£n lÃ  'label' (header bá»‹ láº«n vÃ o data)\n        df = df[df[LABEL_COLUMN] != 'label']\n\n        # Chuyá»ƒn Ä‘á»•i sang binary: Benign=0, Attack=1\n        df['binary_label'] = (df[LABEL_COLUMN] != 'benign').astype(int)\n\n        # Äáº¿m sá»‘ lÆ°á»£ng má»—i class\n        benign_count = (df['binary_label'] == 0).sum()\n        attack_count = (df['binary_label'] == 1).sum()\n\n        self.stats['benign_count'] += benign_count\n        self.stats['attack_count'] += attack_count\n\n        # XÃ³a cá»™t Label gá»‘c, giá»¯ láº¡i binary_label\n        df = df.drop(columns=[LABEL_COLUMN])\n\n        return df\n\n    def _process_single_file(self, csv_file):\n        \"\"\"\n        Xá»­ lÃ½ má»™t file CSV theo chunks\n\n        Args:\n            csv_file: ÄÆ°á»ng dáº«n file CSV\n\n        Returns:\n            DataFrame Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½\n        \"\"\"\n        print(f\"\\nğŸ“„ Äang xá»­ lÃ½: {csv_file.name}\")\n\n        processed_chunks = []\n        chunk_iterator = pd.read_csv(csv_file, chunksize=self.chunk_size,\n                                     low_memory=False, encoding='utf-8')\n\n        # Progress bar cho chunks\n        if TQDM_AVAILABLE:\n            # Æ¯á»›c tÃ­nh sá»‘ chunks dá»±a trÃªn file size\n            file_size = csv_file.stat().st_size\n            estimated_chunks = max(1, file_size // (self.chunk_size * 500))  # Æ¯á»›c tÃ­nh\n            chunk_iterator = tqdm(chunk_iterator, desc=\"   Chunks\",\n                                  total=estimated_chunks, unit=\"chunk\")\n\n        for chunk in chunk_iterator:\n            self.stats['total_rows_read'] += len(chunk)\n\n            # BÆ°á»›c 1: Chuáº©n hÃ³a tÃªn cá»™t\n            chunk = self._clean_column_names(chunk)\n\n            # BÆ°á»›c 2: Loáº¡i bá» cá»™t khÃ´ng cáº§n thiáº¿t\n            chunk = self._drop_unnecessary_columns(chunk)\n\n            # BÆ°á»›c 3: Chuyá»ƒn Ä‘á»•i sang dáº¡ng sá»‘\n            chunk = self._convert_to_numeric(chunk)\n\n            # BÆ°á»›c 4: Xá»­ lÃ½ NaN vÃ  Inf\n            chunk = self._handle_nan_inf(chunk)\n\n            # BÆ°á»›c 5: Chuyá»ƒn Ä‘á»•i nhÃ£n sang binary\n            chunk = self._convert_to_binary_label(chunk)\n\n            processed_chunks.append(chunk)\n\n            # Giáº£i phÃ³ng bá»™ nhá»›\n            gc.collect()\n\n        # Gá»™p cÃ¡c chunks láº¡i\n        if processed_chunks:\n            df = pd.concat(processed_chunks, ignore_index=True)\n            del processed_chunks\n            gc.collect()\n            return df\n\n        return None\n\n    def process_all_files(self):\n        \"\"\"\n        Xá»­ lÃ½ táº¥t cáº£ cÃ¡c file CSV vÃ  gá»™p láº¡i\n\n        Returns:\n            DataFrame Ä‘Ã£ xá»­ lÃ½ hoÃ n chá»‰nh\n        \"\"\"\n        start_time = datetime.now()\n        print(\"\\n\" + \"=\"*80)\n        print(\"ğŸš€ Báº®T Äáº¦U Xá»¬ LÃ Dá»® LIá»†U CICIDS2018 CHO CNN\")\n        print(\"=\"*80)\n\n        csv_files = self._get_csv_files()\n\n        all_dataframes = []\n\n        # Xá»­ lÃ½ tá»«ng file\n        for csv_file in csv_files:\n            df = self._process_single_file(csv_file)\n            if df is not None:\n                all_dataframes.append(df)\n                print(f\"   âœ… ÄÃ£ xá»­ lÃ½: {len(df):,} máº«u\")\n\n        # Gá»™p táº¥t cáº£ láº¡i\n        print(\"\\n\" + \"-\"*80)\n        print(\"ğŸ“Š ÄANG Gá»˜P VÃ€ Xá»¬ LÃ CUá»I CÃ™NG...\")\n\n        df_combined = pd.concat(all_dataframes, ignore_index=True)\n        del all_dataframes\n        gc.collect()\n\n        print(f\"   Tá»•ng sá»‘ máº«u sau khi gá»™p: {len(df_combined):,}\")\n\n        # Loáº¡i bá» duplicate trÃªn toÃ n bá»™ dataset\n        print(\"   Äang loáº¡i bá» duplicate...\")\n        df_combined = self._remove_duplicates(df_combined)\n        print(f\"   Sá»‘ máº«u sau khi loáº¡i duplicate: {len(df_combined):,}\")\n\n        # Cáº­p nháº­t thá»‘ng kÃª\n        self.stats['rows_after_cleaning'] = len(df_combined)\n        self.stats['feature_count'] = len(df_combined.columns) - 1  # Trá»« cá»™t label\n\n        # LÆ°u tÃªn features\n        self.feature_names = [col for col in df_combined.columns if col != 'binary_label']\n\n        end_time = datetime.now()\n        self.stats['processing_time'] = (end_time - start_time).total_seconds()\n\n        return df_combined\n\n    def balanced_sample(self, df):\n        \"\"\"\n        Sample dá»¯ liá»‡u vá»›i sá»‘ lÆ°á»£ng cÃ¢n báº±ng theo target Ä‘Ã£ Ä‘á»‹nh\n\n        Láº¥y chÃ­nh xÃ¡c:\n        - TARGET_BENIGN máº«u Benign (2,100,000)\n        - TARGET_ATTACK máº«u Attack (900,000)\n\n        Args:\n            df: DataFrame Ä‘Ã£ clean\n\n        Returns:\n            DataFrame Ä‘Ã£ Ä‘Æ°á»£c sample cÃ¢n báº±ng\n        \"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\"âš–ï¸ ÄANG SAMPLE CÃ‚N Báº°NG Dá»® LIá»†U\")\n        print(\"=\"*80)\n\n        # TÃ¡ch theo class\n        df_benign = df[df['binary_label'] == 0]\n        df_attack = df[df['binary_label'] == 1]\n\n        n_benign = len(df_benign)\n        n_attack = len(df_attack)\n\n        print(f\"\\n   Dá»¯ liá»‡u gá»‘c (sau khi clean):\")\n        print(f\"   - Benign: {n_benign:,}\")\n        print(f\"   - Attack: {n_attack:,}\")\n        print(f\"   - Tá»•ng: {n_benign + n_attack:,}\")\n\n        print(f\"\\n   Target mong muá»‘n:\")\n        print(f\"   - Benign: {self.target_benign:,} ({BENIGN_RATIO*100:.0f}%)\")\n        print(f\"   - Attack: {self.target_attack:,} ({ATTACK_RATIO*100:.0f}%)\")\n        print(f\"   - Tá»•ng: {self.target_benign + self.target_attack:,}\")\n\n        # Kiá»ƒm tra vÃ  Ä‘iá»u chá»‰nh náº¿u khÃ´ng Ä‘á»§ máº«u\n        actual_benign = min(self.target_benign, n_benign)\n        actual_attack = min(self.target_attack, n_attack)\n\n        if actual_benign < self.target_benign:\n            print(f\"\\n   âš ï¸ KhÃ´ng Ä‘á»§ Benign! Chá»‰ cÃ³ {n_benign:,}, cáº§n {self.target_benign:,}\")\n        if actual_attack < self.target_attack:\n            print(f\"\\n   âš ï¸ KhÃ´ng Ä‘á»§ Attack! Chá»‰ cÃ³ {n_attack:,}, cáº§n {self.target_attack:,}\")\n\n        # Random sample tá»« má»—i class\n        print(f\"\\n   Äang sample...\")\n        df_benign_sampled = df_benign.sample(n=actual_benign, random_state=RANDOM_STATE)\n        df_attack_sampled = df_attack.sample(n=actual_attack, random_state=RANDOM_STATE)\n\n        # Gá»™p láº¡i vÃ  shuffle\n        df_balanced = pd.concat([df_benign_sampled, df_attack_sampled], ignore_index=True)\n        df_balanced = df_balanced.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n\n        # Thá»‘ng kÃª káº¿t quáº£\n        final_benign = (df_balanced['binary_label'] == 0).sum()\n        final_attack = (df_balanced['binary_label'] == 1).sum()\n        total = len(df_balanced)\n\n        print(f\"\\n   âœ… Káº¿t quáº£ sau khi sample:\")\n        print(f\"   - Benign: {final_benign:,} ({final_benign/total*100:.1f}%)\")\n        print(f\"   - Attack: {final_attack:,} ({final_attack/total*100:.1f}%)\")\n        print(f\"   - Tá»•ng: {total:,}\")\n        print(f\"   - Tá»· lá»‡ Benign:Attack = {final_benign/final_attack:.2f}:1\")\n\n        # Cáº­p nháº­t stats\n        self.stats['benign_count'] = final_benign\n        self.stats['attack_count'] = final_attack\n        self.stats['rows_after_cleaning'] = total\n\n        return df_balanced\n\n    def normalize_features(self, df):\n        \"\"\"\n        Chuáº©n hÃ³a cÃ¡c features báº±ng scaler\n\n        Args:\n            df: DataFrame chá»©a features vÃ  label\n\n        Returns:\n            X_normalized: Features Ä‘Ã£ chuáº©n hÃ³a\n            y: Labels\n        \"\"\"\n        print(\"\\nğŸ”„ ÄANG CHUáº¨N HÃ“A Dá»® LIá»†U...\")\n\n        # TÃ¡ch features vÃ  label\n        X = df.drop(columns=['binary_label']).values\n        y = df['binary_label'].values\n\n        # Chuáº©n hÃ³a features\n        X_normalized = self.scaler.fit_transform(X)\n\n        print(f\"   Scaler type: {self.scaler_type}\")\n        print(f\"   Shape X: {X_normalized.shape}\")\n        print(f\"   Shape y: {y.shape}\")\n\n        return X_normalized, y\n\n    def reshape_for_cnn(self, X):\n        \"\"\"\n        Reshape dá»¯ liá»‡u cho CNN 1D\n\n        CNN 1D yÃªu cáº§u input shape: (samples, features, channels)\n        Trong trÆ°á»ng há»£p nÃ y: (samples, n_features, 1)\n\n        Args:\n            X: Features Ä‘Ã£ chuáº©n hÃ³a, shape (samples, features)\n\n        Returns:\n            X_reshaped: Shape (samples, features, 1)\n        \"\"\"\n        print(\"\\nğŸ”„ ÄANG RESHAPE Dá»® LIá»†U CHO CNN...\")\n\n        X_reshaped = X.reshape(X.shape[0], X.shape[1], 1)\n\n        print(f\"   Shape sau reshape: {X_reshaped.shape}\")\n\n        return X_reshaped\n\n    def split_data(self, X, y, test_size=0.2, val_size=0.1):\n        \"\"\"\n        Chia dá»¯ liá»‡u thÃ nh train/val/test sets\n\n        Args:\n            X: Features\n            y: Labels\n            test_size: Tá»· lá»‡ test set\n            val_size: Tá»· lá»‡ validation set (tá»« train)\n\n        Returns:\n            X_train, X_val, X_test, y_train, y_val, y_test\n        \"\"\"\n        print(\"\\nğŸ“Š ÄANG CHIA Dá»® LIá»†U TRAIN/VAL/TEST...\")\n\n        # Chia train+val / test\n        X_temp, X_test, y_temp, y_test = train_test_split(\n            X, y, test_size=test_size, random_state=RANDOM_STATE, stratify=y\n        )\n\n        # Chia train / val\n        val_ratio = val_size / (1 - test_size)\n        X_train, X_val, y_train, y_val = train_test_split(\n            X_temp, y_temp, test_size=val_ratio, random_state=RANDOM_STATE, stratify=y_temp\n        )\n\n        print(f\"   Train set: {X_train.shape[0]:,} máº«u\")\n        print(f\"   Val set:   {X_val.shape[0]:,} máº«u\")\n        print(f\"   Test set:  {X_test.shape[0]:,} máº«u\")\n\n        # Thá»‘ng kÃª phÃ¢n bá»‘ class\n        print(f\"\\n   PhÃ¢n bá»‘ Train - Benign: {(y_train==0).sum():,}, Attack: {(y_train==1).sum():,}\")\n        print(f\"   PhÃ¢n bá»‘ Val   - Benign: {(y_val==0).sum():,}, Attack: {(y_val==1).sum():,}\")\n        print(f\"   PhÃ¢n bá»‘ Test  - Benign: {(y_test==0).sum():,}, Attack: {(y_test==1).sum():,}\")\n\n        return X_train, X_val, X_test, y_train, y_val, y_test\n\n\n    def save_processed_data(self, X_train, X_val, X_test, y_train, y_val, y_test):\n        \"\"\"\n        LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ sang Ä‘á»‹nh dáº¡ng nhanh\n\n        LÆ°u thÃ nh cÃ¡c file:\n        - X_train.npy, X_val.npy, X_test.npy\n        - y_train.npy, y_val.npy, y_test.npy\n        - scaler.pkl\n        - metadata.json\n        \"\"\"\n        print(\"\\n ÄANG LÆ¯U Dá»® LIá»†U ÄÃƒ Xá»¬ LÃ...\")\n\n        # LÆ°u numpy arrays\n        np.save(self.output_dir / 'X_train.npy', X_train)\n        np.save(self.output_dir / 'X_val.npy', X_val)\n        np.save(self.output_dir / 'X_test.npy', X_test)\n        np.save(self.output_dir / 'y_train.npy', y_train)\n        np.save(self.output_dir / 'y_val.npy', y_val)\n        np.save(self.output_dir / 'y_test.npy', y_test)\n\n        print(f\"   âœ… ÄÃ£ lÆ°u X_train.npy: {X_train.shape}\")\n        print(f\"   âœ… ÄÃ£ lÆ°u X_val.npy: {X_val.shape}\")\n        print(f\"   âœ… ÄÃ£ lÆ°u X_test.npy: {X_test.shape}\")\n        print(f\"   âœ… ÄÃ£ lÆ°u y_train.npy: {y_train.shape}\")\n        print(f\"   âœ… ÄÃ£ lÆ°u y_val.npy: {y_val.shape}\")\n        print(f\"   âœ… ÄÃ£ lÆ°u y_test.npy: {y_test.shape}\")\n\n        # LÆ°u scaler\n        with open(self.output_dir / 'scaler.pkl', 'wb') as f:\n            pickle.dump(self.scaler, f)\n        print(f\"   âœ… ÄÃ£ lÆ°u scaler.pkl\")\n\n        # LÆ°u feature names\n        with open(self.output_dir / 'feature_names.txt', 'w') as f:\n            for name in self.feature_names:\n                f.write(name + '\\n')\n        print(f\"   âœ… ÄÃ£ lÆ°u feature_names.txt\")\n\n        # Chuyá»ƒn Ä‘á»•i stats sang kiá»ƒu Python native (Ä‘á»ƒ trÃ¡nh lá»—i JSON vá»›i numpy.int64)\n        stats_native = {}\n        for key, value in self.stats.items():\n            if hasattr(value, 'item'):  # Kiá»ƒm tra náº¿u lÃ  numpy type\n                stats_native[key] = value.item()\n            elif isinstance(value, (np.integer, np.floating)):\n                stats_native[key] = int(value) if isinstance(value, np.integer) else float(value)\n            else:\n                stats_native[key] = value\n\n        # LÆ°u metadata\n        metadata = {\n            'n_features': len(self.feature_names),\n            'feature_names': self.feature_names,\n            'train_samples': int(X_train.shape[0]),\n            'val_samples': int(X_val.shape[0]),\n            'test_samples': int(X_test.shape[0]),\n            'input_shape': [int(x) for x in X_train.shape[1:]],\n            'scaler_type': self.scaler_type,\n            'stats': stats_native,\n            'created_at': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        }\n\n        with open(self.output_dir / 'metadata.json', 'w') as f:\n            json.dump(metadata, f, indent=4)\n        print(f\"   âœ… ÄÃ£ lÆ°u metadata.json\")\n\n        print(f\"\\nğŸ“ Táº¥t cáº£ file Ä‘Æ°á»£c lÆ°u táº¡i: {self.output_dir}\")\n\n    def print_summary(self):\n        \"\"\"In tÃ³m táº¯t quÃ¡ trÃ¬nh xá»­ lÃ½\"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\"ğŸ“Š TÃ“M Táº®T Xá»¬ LÃ Dá»® LIá»†U\")\n        print(\"=\"*80)\n        print(f\"   Tá»•ng sá»‘ dÃ²ng Ä‘á»c Ä‘Æ°á»£c:     {self.stats['total_rows_read']:,}\")\n        print(f\"   Sá»‘ dÃ²ng sau khi xá»­ lÃ½:     {self.stats['rows_after_cleaning']:,}\")\n        print(f\"   Sá»‘ duplicate Ä‘Ã£ loáº¡i:      {self.stats['duplicates_removed']:,}\")\n        print(f\"   Sá»‘ NaN/Inf Ä‘Ã£ thay tháº¿:    {self.stats['nan_inf_replaced']:,}\")\n        print(f\"   Sá»‘ features:               {self.stats['feature_count']}\")\n        print(f\"   Sá»‘ máº«u Benign:             {self.stats['benign_count']:,}\")\n        print(f\"   Sá»‘ máº«u Attack:             {self.stats['attack_count']:,}\")\n        print(f\"   Thá»i gian xá»­ lÃ½:           {self.stats['processing_time']:.2f} giÃ¢y\")\n        print(\"=\"*80)\n\n\ndef main():\n    \"\"\"HÃ m chÃ­nh Ä‘á»ƒ cháº¡y preprocessing\"\"\"\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸ”§ TIá»€N Xá»¬ LÃ Dá»® LIá»†U CICIDS2018 CHO MÃ” HÃŒNH CNN\")\n    print(\"   PhÃ¡t hiá»‡n lÆ°u lÆ°á»£ng máº¡ng IoT báº¥t thÆ°á»ng\")\n    print(\"=\"*80)\n\n    print(f\"\\nğŸ“‹ Cáº¤U HÃŒNH:\")\n    print(f\"   - Tá»•ng máº«u mong muá»‘n: {TOTAL_SAMPLES:,}\")\n    print(f\"   - Benign: {TARGET_BENIGN:,} ({BENIGN_RATIO*100:.0f}%)\")\n    print(f\"   - Attack: {TARGET_ATTACK:,} ({ATTACK_RATIO*100:.0f}%)\")\n\n    # Khá»Ÿi táº¡o preprocessor\n    preprocessor = CICIDS2018_CNN_Preprocessor(\n        data_dir=DATA_DIR,\n        output_dir=OUTPUT_DIR,\n        chunk_size=CHUNK_SIZE,\n        scaler_type=SCALER_TYPE,\n        target_benign=TARGET_BENIGN,\n        target_attack=TARGET_ATTACK\n    )\n\n    # BÆ°á»›c 1: Xá»­ lÃ½ táº¥t cáº£ cÃ¡c file CSV (clean data)\n    df = preprocessor.process_all_files()\n\n    # BÆ°á»›c 2: SAMPLE CÃ‚N Báº°NG TRÆ¯á»šC KHI CHIA\n    # Äiá»u nÃ y Ä‘áº£m báº£o train/val/test Ä‘á»u cÃ³ tá»· lá»‡ 70-30\n    df = preprocessor.balanced_sample(df)\n\n    # BÆ°á»›c 3: Chuáº©n hÃ³a features\n    X, y = preprocessor.normalize_features(df)\n\n    # Giáº£i phÃ³ng bá»™ nhá»› cá»§a DataFrame\n    del df\n    gc.collect()\n\n    # BÆ°á»›c 4: Reshape cho CNN\n    X = preprocessor.reshape_for_cnn(X)\n\n    # BÆ°á»›c 5: Chia dá»¯ liá»‡u (stratify Ä‘á»ƒ giá»¯ tá»· lá»‡ 70-30 trong táº¥t cáº£ cÃ¡c táº­p)\n    X_train, X_val, X_test, y_train, y_val, y_test = preprocessor.split_data(X, y)\n\n    # Giáº£i phÃ³ng bá»™ nhá»›\n    del X, y\n    gc.collect()\n\n    # BÆ°á»›c 6: LÆ°u dá»¯ liá»‡u\n    preprocessor.save_processed_data(X_train, X_val, X_test, y_train, y_val, y_test)\n\n    # In tÃ³m táº¯t\n    preprocessor.print_summary()\n\n    print(\"\\nâœ… HOÃ€N THÃ€NH! Dá»¯ liá»‡u Ä‘Ã£ sáºµn sÃ ng cho viá»‡c huáº¥n luyá»‡n CNN.\")\n\n    return preprocessor\n\n\nif __name__ == \"__main__\":\n    preprocessor = main()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nimport os\n\n# ÄÆ°á»ng dáº«n chuáº©n trong Kaggle\ndirectory_to_zip = '/kaggle/working/models'\noutput_filename = 'model_cnn_5layer' # TÃªn file káº¿t quáº£ (khÃ´ng cáº§n Ä‘uÃ´i .zip)\n\n# Kiá»ƒm tra xem cÃ³ file nÃ o trong Ä‘Ã³ khÃ´ng\nif os.path.exists(directory_to_zip):\n    print(\"Äang nÃ©n dá»¯ liá»‡u...\")\n    # NÃ©n thÆ° má»¥c\n    shutil.make_archive(output_filename, 'zip', directory_to_zip)\n    print(f\"âœ… Xong! File náº±m táº¡i: {os.path.join(directory_to_zip, output_filename + '.zip')}\")\nelse:\n    print(\"KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c working.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-28T12:53:34.253334Z","iopub.execute_input":"2025-12-28T12:53:34.253903Z","iopub.status.idle":"2025-12-28T12:53:34.611601Z","shell.execute_reply.started":"2025-12-28T12:53:34.253876Z","shell.execute_reply":"2025-12-28T12:53:34.611030Z"}},"outputs":[{"name":"stdout","text":"Äang nÃ©n dá»¯ liá»‡u...\nâœ… Xong! File náº±m táº¡i: /kaggle/working/models/model_cnn_5layer.zip\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"\"\"\"\n======================================================================================\nHUáº¤N LUYá»†N MÃ” HÃŒNH CNN - PHÃT HIá»†N LÆ¯U LÆ¯á»¢NG Máº NG IOT Báº¤T THÆ¯á»œNG\n======================================================================================\n\nScript nÃ y huáº¥n luyá»‡n mÃ´ hÃ¬nh CNN 1D cho bÃ i toÃ¡n phÃ¢n loáº¡i binary:\n- Benign (0): LÆ°u lÆ°á»£ng máº¡ng bÃ¬nh thÆ°á»ng\n- Attack (1): LÆ°u lÆ°á»£ng máº¡ng báº¥t thÆ°á»ng/táº¥n cÃ´ng\n\nMÃ´ hÃ¬nh CNN 1D Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ há»c cÃ¡c patterns tá»« network flow features.\n\nCÃ³ thá»ƒ cháº¡y trÃªn cáº£ Kaggle vÃ  Local.\n\"\"\"\n\nimport os\nimport sys\nimport numpy as np\nimport json\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ============================================================================\n# KIá»‚M TRA MÃ”I TRÆ¯á»œNG VÃ€ IMPORT THÆ¯ VIá»†N\n# ============================================================================\n\n# Kiá»ƒm tra mÃ´i trÆ°á»ng cháº¡y (Kaggle hoáº·c Local)\nIS_KAGGLE = os.path.exists('/kaggle/input')\n\nif IS_KAGGLE:\n    print(\"ğŸŒ Äang cháº¡y trÃªn KAGGLE\")\nelse:\n    print(\"ğŸ’» Äang cháº¡y trÃªn LOCAL\")\n\n# Import TensorFlow/Keras\ntry:\n    import tensorflow as tf\n    from tensorflow import keras\n    from tensorflow.keras import layers, models, callbacks\n    from tensorflow.keras.optimizers import Adam\n    print(f\"âœ… TensorFlow version: {tf.__version__}\")\nexcept ImportError:\n    print(\"âŒ Lá»—i: TensorFlow chÆ°a Ä‘Æ°á»£c cÃ i Ä‘áº·t!\")\n    print(\"   CÃ i Ä‘áº·t báº±ng: pip install tensorflow\")\n    sys.exit(1)\n\n# Import sklearn cho metrics\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix,\n    accuracy_score, precision_score, recall_score,\n    f1_score, roc_auc_score, roc_curve\n)\n\n# ============================================================================\n# Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN\n# ============================================================================\n\nif IS_KAGGLE:\n    # ÄÆ°á»ng dáº«n trÃªn Kaggle\n    PROCESSED_DATA_DIR = \"/kaggle/working/processed_data_cnn\"\n    OUTPUT_DIR = \"/kaggle/working/cnn_results\"\nelse:\n    # ÄÆ°á»ng dáº«n Local\n    PROCESSED_DATA_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CNN\\processed_data_cnn\"\n    OUTPUT_DIR = r\"D:\\PROJECT\\Machine Learning\\IOT\\CNN\\results\"\n\n# ============================================================================\n# Cáº¤U HÃŒNH MÃ” HÃŒNH VÃ€ HUáº¤N LUYá»†N\n# ============================================================================\n\n# Hyperparameters cho CNN\nCNN_CONFIG = {\n    # Kiáº¿n trÃºc máº¡ng\n    'conv_filters': [64, 128, 256],      # Sá»‘ filters cho má»—i Conv layer\n    'kernel_size': 3,                     # KÃ­ch thÆ°á»›c kernel\n    'pool_size': 2,                       # KÃ­ch thÆ°á»›c pooling\n    'dense_units': [128, 64],             # Sá»‘ units cho Dense layers\n    'dropout_rate': 0.3,                  # Tá»· lá»‡ dropout\n\n    # Huáº¥n luyá»‡n\n    'batch_size': 256,                    # Batch size\n    'epochs': 30,                         # Sá»‘ epochs tá»‘i Ä‘a\n    'learning_rate': 0.001,               # Learning rate\n    'early_stopping_patience': 10,        # Patience cho early stopping\n    'reduce_lr_patience': 5,              # Patience cho reduce LR\n    'reduce_lr_factor': 0.5,              # Factor giáº£m LR\n    'min_lr': 1e-7,                       # LR tá»‘i thiá»ƒu\n\n    # Class weights Ä‘á»ƒ xá»­ lÃ½ imbalanced data\n    'use_class_weight': True,             # Sá»­ dá»¥ng class weight\n}\n\n# ============================================================================\n# LOAD Dá»® LIá»†U ÄÃƒ Xá»¬ LÃ\n# ============================================================================\n\ndef load_processed_data(data_dir):\n    \"\"\"\n    Load dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c tiá»n xá»­ lÃ½\n\n    Args:\n        data_dir: ÄÆ°á»ng dáº«n thÆ° má»¥c chá»©a dá»¯ liá»‡u\n\n    Returns:\n        X_train, X_val, X_test, y_train, y_val, y_test, metadata\n    \"\"\"\n    data_dir = Path(data_dir)\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸ“‚ ÄANG LOAD Dá»® LIá»†U ÄÃƒ Xá»¬ LÃ\")\n    print(\"=\"*80)\n\n    # Load numpy arrays\n    X_train = np.load(data_dir / 'X_train.npy')\n    X_val = np.load(data_dir / 'X_val.npy')\n    X_test = np.load(data_dir / 'X_test.npy')\n    y_train = np.load(data_dir / 'y_train.npy')\n    y_val = np.load(data_dir / 'y_val.npy')\n    y_test = np.load(data_dir / 'y_test.npy')\n\n    print(f\"   X_train: {X_train.shape}\")\n    print(f\"   X_val:   {X_val.shape}\")\n    print(f\"   X_test:  {X_test.shape}\")\n    print(f\"   y_train: {y_train.shape}\")\n    print(f\"   y_val:   {y_val.shape}\")\n    print(f\"   y_test:  {y_test.shape}\")\n\n    # Load metadata\n    with open(data_dir / 'metadata.json', 'r') as f:\n        metadata = json.load(f)\n\n    print(f\"\\n   Sá»‘ features: {metadata['n_features']}\")\n    print(f\"   Train samples: {metadata['train_samples']:,}\")\n    print(f\"   Val samples: {metadata['val_samples']:,}\")\n    print(f\"   Test samples: {metadata['test_samples']:,}\")\n\n    return X_train, X_val, X_test, y_train, y_val, y_test, metadata\n\n\n# ============================================================================\n# XÃ‚Y Dá»°NG MÃ” HÃŒNH CNN\n# ============================================================================\n\ndef build_cnn_model(input_shape, config=CNN_CONFIG):\n    \"\"\"\n    XÃ¢y dá»±ng mÃ´ hÃ¬nh CNN 1D cho phÃ¢n loáº¡i binary\n\n    Kiáº¿n trÃºc:\n    - Nhiá»u block Conv1D + BatchNorm + ReLU + MaxPooling + Dropout\n    - Flatten\n    - Dense layers vá»›i Dropout\n    - Output layer vá»›i Sigmoid\n\n    Args:\n        input_shape: Shape cá»§a input (n_features, 1)\n        config: Dictionary chá»©a cÃ¡c hyperparameters\n\n    Returns:\n        model: MÃ´ hÃ¬nh Keras Ä‘Ã£ compile\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸ—ï¸ ÄANG XÃ‚Y Dá»°NG MÃ” HÃŒNH CNN\")\n    print(\"=\"*80)\n\n    model = models.Sequential(name=\"CNN_IDS\")\n\n    # Input layer\n    model.add(layers.InputLayer(input_shape=input_shape))\n\n    # Convolutional blocks\n    for i, filters in enumerate(config['conv_filters']):\n        # Conv1D layer\n        model.add(layers.Conv1D(\n            filters=filters,\n            kernel_size=config['kernel_size'],\n            padding='same',\n            name=f'conv1d_{i+1}'\n        ))\n\n        # Batch Normalization\n        model.add(layers.BatchNormalization(name=f'bn_{i+1}'))\n\n        # Activation\n        model.add(layers.Activation('relu', name=f'relu_{i+1}'))\n\n        # MaxPooling (chá»‰ Ã¡p dá»¥ng náº¿u kÃ­ch thÆ°á»›c Ä‘á»§ lá»›n)\n        model.add(layers.MaxPooling1D(\n            pool_size=config['pool_size'],\n            padding='same',\n            name=f'maxpool_{i+1}'\n        ))\n\n        # Dropout\n        model.add(layers.Dropout(\n            config['dropout_rate'],\n            name=f'dropout_conv_{i+1}'\n        ))\n\n    # Flatten\n    model.add(layers.Flatten(name='flatten'))\n\n    # Dense layers\n    for i, units in enumerate(config['dense_units']):\n        model.add(layers.Dense(units, name=f'dense_{i+1}'))\n        model.add(layers.BatchNormalization(name=f'bn_dense_{i+1}'))\n        model.add(layers.Activation('relu', name=f'relu_dense_{i+1}'))\n        model.add(layers.Dropout(config['dropout_rate'], name=f'dropout_dense_{i+1}'))\n\n    # Output layer (binary classification)\n    model.add(layers.Dense(1, activation='sigmoid', name='output'))\n\n    # Compile model\n    optimizer = Adam(learning_rate=config['learning_rate'])\n\n    model.compile(\n        optimizer=optimizer,\n        loss='binary_crossentropy',\n        metrics=[\n            'accuracy',\n            keras.metrics.Precision(name='precision'),\n            keras.metrics.Recall(name='recall'),\n            keras.metrics.AUC(name='auc')\n        ]\n    )\n\n    # In summary\n    print(f\"\\n   Input shape: {input_shape}\")\n    print(f\"   Conv filters: {config['conv_filters']}\")\n    print(f\"   Dense units: {config['dense_units']}\")\n    print(f\"   Dropout rate: {config['dropout_rate']}\")\n    print(f\"   Learning rate: {config['learning_rate']}\")\n\n    model.summary()\n\n    return model\n\n\n# ============================================================================\n# TÃNH CLASS WEIGHTS\n# ============================================================================\n\ndef compute_class_weights(y_train):\n    \"\"\"\n    TÃ­nh class weights Ä‘á»ƒ xá»­ lÃ½ imbalanced data\n\n    Args:\n        y_train: Labels cá»§a training set\n\n    Returns:\n        class_weight: Dictionary {0: weight_0, 1: weight_1}\n    \"\"\"\n    # Äáº¿m sá»‘ lÆ°á»£ng má»—i class\n    n_benign = (y_train == 0).sum()\n    n_attack = (y_train == 1).sum()\n    total = len(y_train)\n\n    # TÃ­nh weights\n    weight_benign = total / (2 * n_benign)\n    weight_attack = total / (2 * n_attack)\n\n    class_weight = {\n        0: weight_benign,\n        1: weight_attack\n    }\n\n    print(f\"\\nğŸ“Š Class weights:\")\n    print(f\"   Benign (0): {n_benign:,} máº«u, weight = {weight_benign:.4f}\")\n    print(f\"   Attack (1): {n_attack:,} máº«u, weight = {weight_attack:.4f}\")\n\n    return class_weight\n\n\n# ============================================================================\n# CALLBACKS\n# ============================================================================\n\ndef get_callbacks(output_dir, config=CNN_CONFIG):\n    \"\"\"\n    Táº¡o cÃ¡c callbacks cho quÃ¡ trÃ¬nh huáº¥n luyá»‡n\n\n    Args:\n        output_dir: ÄÆ°á»ng dáº«n lÆ°u model\n        config: Dictionary chá»©a cÃ¡c hyperparameters\n\n    Returns:\n        List cÃ¡c callbacks\n    \"\"\"\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    callback_list = [\n        # Early stopping - dá»«ng náº¿u val_loss khÃ´ng cáº£i thiá»‡n\n        callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=config['early_stopping_patience'],\n            restore_best_weights=True,\n            verbose=1\n        ),\n\n        # Reduce learning rate khi val_loss khÃ´ng cáº£i thiá»‡n\n        callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=config['reduce_lr_factor'],\n            patience=config['reduce_lr_patience'],\n            min_lr=config['min_lr'],\n            verbose=1\n        ),\n\n        # Model checkpoint - lÆ°u model tá»‘t nháº¥t\n        callbacks.ModelCheckpoint(\n            filepath=str(output_dir / 'best_model.keras'),\n            monitor='val_auc',\n            mode='max',\n            save_best_only=True,\n            verbose=1\n        ),\n\n        # TensorBoard logs (optional)\n        callbacks.TensorBoard(\n            log_dir=str(output_dir / 'logs'),\n            histogram_freq=1\n        ),\n\n        # CSV logger - lÆ°u history ra file\n        callbacks.CSVLogger(\n            filename=str(output_dir / 'training_history.csv'),\n            separator=',',\n            append=False\n        )\n    ]\n\n    return callback_list\n\n\n# ============================================================================\n# HUáº¤N LUYá»†N MÃ” HÃŒNH\n# ============================================================================\n\ndef train_model(model, X_train, y_train, X_val, y_val,\n                output_dir, config=None):\n    \"\"\"\n    Huáº¥n luyá»‡n mÃ´ hÃ¬nh CNN\n\n    Args:\n        model: MÃ´ hÃ¬nh Keras\n        X_train, y_train: Dá»¯ liá»‡u training\n        X_val, y_val: Dá»¯ liá»‡u validation\n        output_dir: ÄÆ°á»ng dáº«n lÆ°u káº¿t quáº£\n        config: Dictionary chá»©a cÃ¡c hyperparameters\n\n    Returns:\n        history: History object cá»§a quÃ¡ trÃ¬nh training\n    \"\"\"\n    if config is None:\n        config = CNN_CONFIG\n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸš€ Báº®T Äáº¦U HUáº¤N LUYá»†N MÃ” HÃŒNH\")\n    print(\"=\"*80)\n\n    # TÃ­nh class weights náº¿u cáº§n\n    class_weight = None\n    if config['use_class_weight']:\n        class_weight = compute_class_weights(y_train)\n\n    # Láº¥y callbacks\n    callback_list = get_callbacks(output_dir, config)\n\n    print(f\"\\n   Batch size: {config['batch_size']}\")\n    print(f\"   Max epochs: {config['epochs']}\")\n    print(f\"   Early stopping patience: {config['early_stopping_patience']}\")\n\n    # Huáº¥n luyá»‡n\n    start_time = datetime.now()\n\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        epochs=config['epochs'],\n        batch_size=config['batch_size'],\n        class_weight=class_weight,\n        callbacks=callback_list,\n        verbose=1\n    )\n\n    end_time = datetime.now()\n    training_time = (end_time - start_time).total_seconds()\n\n    print(f\"\\nâ±ï¸ Thá»i gian huáº¥n luyá»‡n: {training_time/60:.2f} phÃºt\")\n\n    return history, training_time\n\n\n# ============================================================================\n# ÄÃNH GIÃ MÃ” HÃŒNH\n# ============================================================================\n\ndef evaluate_model(model, X_test, y_test):\n    \"\"\"\n    ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn test set\n\n    Args:\n        model: MÃ´ hÃ¬nh Ä‘Ã£ train\n        X_test, y_test: Dá»¯ liá»‡u test\n\n    Returns:\n        results: Dictionary chá»©a cÃ¡c metrics\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸ“Š ÄÃNH GIÃ MÃ” HÃŒNH TRÃŠN TEST SET\")\n    print(\"=\"*80)\n\n    # Dá»± Ä‘oÃ¡n\n    y_pred_prob = model.predict(X_test, verbose=0)\n    y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n\n    # TÃ­nh cÃ¡c metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    auc = roc_auc_score(y_test, y_pred_prob)\n\n    # In káº¿t quáº£\n    print(f\"\\n   Accuracy:  {accuracy:.4f}\")\n    print(f\"   Precision: {precision:.4f}\")\n    print(f\"   Recall:    {recall:.4f}\")\n    print(f\"   F1-Score:  {f1:.4f}\")\n    print(f\"   AUC-ROC:   {auc:.4f}\")\n\n    # Classification report\n    print(\"\\n\" + \"-\"*60)\n    print(\"CLASSIFICATION REPORT:\")\n    print(\"-\"*60)\n    target_names = ['Benign', 'Attack']\n    print(classification_report(y_test, y_pred, target_names=target_names))\n\n    # Confusion matrix\n    print(\"-\"*60)\n    print(\"CONFUSION MATRIX:\")\n    print(\"-\"*60)\n    cm = confusion_matrix(y_test, y_pred)\n    print(f\"                Predicted\")\n    print(f\"              Benign  Attack\")\n    print(f\"Actual Benign   {cm[0,0]:6d}  {cm[0,1]:6d}\")\n    print(f\"       Attack   {cm[1,0]:6d}  {cm[1,1]:6d}\")\n\n    # TÃ­nh detection rate vÃ  false alarm rate\n    tn, fp, fn, tp = cm.ravel()\n    detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 0\n    false_alarm_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n\n    print(f\"\\n   Detection Rate (True Positive Rate): {detection_rate:.4f}\")\n    print(f\"   False Alarm Rate (False Positive Rate): {false_alarm_rate:.4f}\")\n\n    results = {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1,\n        'auc_roc': auc,\n        'detection_rate': detection_rate,\n        'false_alarm_rate': false_alarm_rate,\n        'confusion_matrix': cm.tolist(),\n        'y_pred_prob': y_pred_prob,\n        'y_pred': y_pred\n    }\n\n    return results\n\n\n# ============================================================================\n# Váº¼ BIá»‚U Äá»’\n# ============================================================================\n\ndef plot_training_history(history, output_dir):\n    \"\"\"\n    Váº½ biá»ƒu Ä‘á»“ training history\n\n    Args:\n        history: History object tá»« model.fit()\n        output_dir: ÄÆ°á»ng dáº«n lÆ°u biá»ƒu Ä‘á»“\n    \"\"\"\n    output_dir = Path(output_dir)\n\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n    # Plot Loss\n    axes[0, 0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n    axes[0, 0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n    axes[0, 0].set_title('Loss', fontsize=14, fontweight='bold')\n    axes[0, 0].set_xlabel('Epoch')\n    axes[0, 0].set_ylabel('Loss')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True, alpha=0.3)\n\n    # Plot Accuracy\n    axes[0, 1].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n    axes[0, 1].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n    axes[0, 1].set_title('Accuracy', fontsize=14, fontweight='bold')\n    axes[0, 1].set_xlabel('Epoch')\n    axes[0, 1].set_ylabel('Accuracy')\n    axes[0, 1].legend()\n    axes[0, 1].grid(True, alpha=0.3)\n\n    # Plot Precision & Recall\n    axes[1, 0].plot(history.history['precision'], label='Train Precision', linewidth=2)\n    axes[1, 0].plot(history.history['val_precision'], label='Val Precision', linewidth=2)\n    axes[1, 0].plot(history.history['recall'], label='Train Recall', linewidth=2)\n    axes[1, 0].plot(history.history['val_recall'], label='Val Recall', linewidth=2)\n    axes[1, 0].set_title('Precision & Recall', fontsize=14, fontweight='bold')\n    axes[1, 0].set_xlabel('Epoch')\n    axes[1, 0].set_ylabel('Score')\n    axes[1, 0].legend()\n    axes[1, 0].grid(True, alpha=0.3)\n\n    # Plot AUC\n    axes[1, 1].plot(history.history['auc'], label='Train AUC', linewidth=2)\n    axes[1, 1].plot(history.history['val_auc'], label='Val AUC', linewidth=2)\n    axes[1, 1].set_title('AUC-ROC', fontsize=14, fontweight='bold')\n    axes[1, 1].set_xlabel('Epoch')\n    axes[1, 1].set_ylabel('AUC')\n    axes[1, 1].legend()\n    axes[1, 1].grid(True, alpha=0.3)\n\n    plt.suptitle('Training History - CNN IoT Anomaly Detection',\n                 fontsize=16, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig(output_dir / 'training_history.png', dpi=150, bbox_inches='tight')\n    plt.close()\n\n    print(f\"\\nğŸ“Š ÄÃ£ lÆ°u biá»ƒu Ä‘á»“ training history: {output_dir / 'training_history.png'}\")\n\n\ndef plot_confusion_matrix(cm, output_dir):\n    \"\"\"\n    Váº½ confusion matrix\n\n    Args:\n        cm: Confusion matrix\n        output_dir: ÄÆ°á»ng dáº«n lÆ°u biá»ƒu Ä‘á»“\n    \"\"\"\n    output_dir = Path(output_dir)\n\n    fig, ax = plt.subplots(figsize=(8, 6))\n\n    # Váº½ heatmap\n    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    ax.figure.colorbar(im, ax=ax)\n\n    # Labels\n    classes = ['Benign', 'Attack']\n    ax.set(xticks=[0, 1], yticks=[0, 1],\n           xticklabels=classes, yticklabels=classes,\n           title='Confusion Matrix',\n           ylabel='Actual',\n           xlabel='Predicted')\n\n    # ThÃªm text vÃ o cÃ¡c Ã´\n    thresh = cm.max() / 2.\n    for i in range(2):\n        for j in range(2):\n            ax.text(j, i, format(cm[i, j], 'd'),\n                   ha=\"center\", va=\"center\",\n                   color=\"white\" if cm[i, j] > thresh else \"black\",\n                   fontsize=14)\n\n    plt.tight_layout()\n    plt.savefig(output_dir / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n    plt.close()\n\n    print(f\"ğŸ“Š ÄÃ£ lÆ°u confusion matrix: {output_dir / 'confusion_matrix.png'}\")\n\n\ndef plot_roc_curve(y_test, y_pred_prob, output_dir):\n    \"\"\"\n    Váº½ ROC curve\n\n    Args:\n        y_test: Labels thá»±c táº¿\n        y_pred_prob: XÃ¡c suáº¥t dá»± Ä‘oÃ¡n\n        output_dir: ÄÆ°á»ng dáº«n lÆ°u biá»ƒu Ä‘á»“\n    \"\"\"\n    output_dir = Path(output_dir)\n\n    # TÃ­nh ROC curve\n    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n    auc = roc_auc_score(y_test, y_pred_prob)\n\n    fig, ax = plt.subplots(figsize=(8, 6))\n\n    ax.plot(fpr, tpr, color='blue', linewidth=2,\n            label=f'ROC curve (AUC = {auc:.4f})')\n    ax.plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=1)\n\n    ax.set_xlim([0.0, 1.0])\n    ax.set_ylim([0.0, 1.05])\n    ax.set_xlabel('False Positive Rate', fontsize=12)\n    ax.set_ylabel('True Positive Rate', fontsize=12)\n    ax.set_title('Receiver Operating Characteristic (ROC) Curve',\n                fontsize=14, fontweight='bold')\n    ax.legend(loc=\"lower right\")\n    ax.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.savefig(output_dir / 'roc_curve.png', dpi=150, bbox_inches='tight')\n    plt.close()\n\n    print(f\"ğŸ“Š ÄÃ£ lÆ°u ROC curve: {output_dir / 'roc_curve.png'}\")\n\n\n# ============================================================================\n# LÆ¯U Káº¾T QUáº¢\n# ============================================================================\n\ndef save_results(results, history, training_time, output_dir, config):\n    \"\"\"\n    LÆ°u káº¿t quáº£ vÃ  thÃ´ng tin mÃ´ hÃ¬nh\n\n    Args:\n        results: Dictionary chá»©a metrics\n        history: Training history\n        training_time: Thá»i gian training (giÃ¢y)\n        output_dir: ÄÆ°á»ng dáº«n lÆ°u\n        config: Config cá»§a mÃ´ hÃ¬nh\n    \"\"\"\n    output_dir = Path(output_dir)\n\n    # Táº¡o summary\n    summary = {\n        'model_name': 'CNN_1D_IDS',\n        'task': 'Binary Classification - IoT Anomaly Detection',\n        'dataset': 'CICIDS2018',\n        'training_time_seconds': training_time,\n        'training_time_minutes': training_time / 60,\n        'config': config,\n        'results': {\n            'accuracy': float(results['accuracy']),\n            'precision': float(results['precision']),\n            'recall': float(results['recall']),\n            'f1_score': float(results['f1_score']),\n            'auc_roc': float(results['auc_roc']),\n            'detection_rate': float(results['detection_rate']),\n            'false_alarm_rate': float(results['false_alarm_rate']),\n            'confusion_matrix': results['confusion_matrix']\n        },\n        'final_epoch': len(history.history['loss']),\n        'best_val_loss': float(min(history.history['val_loss'])),\n        'best_val_auc': float(max(history.history['val_auc'])),\n        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    }\n\n    # LÆ°u summary\n    with open(output_dir / 'results_summary.json', 'w') as f:\n        json.dump(summary, f, indent=4)\n\n    print(f\"\\nğŸ’¾ ÄÃ£ lÆ°u káº¿t quáº£: {output_dir / 'results_summary.json'}\")\n\n\n# ============================================================================\n# HÃ€M CHÃNH\n# ============================================================================\n\ndef main():\n    \"\"\"HÃ m chÃ­nh Ä‘á»ƒ cháº¡y toÃ n bá»™ pipeline\"\"\"\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸ”§ HUáº¤N LUYá»†N MÃ” HÃŒNH CNN - PHÃT HIá»†N LÆ¯U LÆ¯á»¢NG Máº NG IOT Báº¤T THÆ¯á»œNG\")\n    print(\"=\"*80)\n\n    # Táº¡o thÆ° má»¥c output\n    output_dir = Path(OUTPUT_DIR)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # BÆ°á»›c 1: Load dá»¯ liá»‡u\n    X_train, X_val, X_test, y_train, y_val, y_test, metadata = load_processed_data(PROCESSED_DATA_DIR)\n\n    # BÆ°á»›c 2: XÃ¢y dá»±ng mÃ´ hÃ¬nh\n    input_shape = (X_train.shape[1], X_train.shape[2])  # (n_features, 1)\n    model = build_cnn_model(input_shape)\n\n    # BÆ°á»›c 3: Huáº¥n luyá»‡n mÃ´ hÃ¬nh\n    history, training_time = train_model(\n        model, X_train, y_train, X_val, y_val,\n        output_dir, CNN_CONFIG\n    )\n\n    # BÆ°á»›c 4: ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh\n    results = evaluate_model(model, X_test, y_test)\n\n    # BÆ°á»›c 5: Váº½ biá»ƒu Ä‘á»“\n    plot_training_history(history, output_dir)\n    plot_confusion_matrix(np.array(results['confusion_matrix']), output_dir)\n    plot_roc_curve(y_test, results['y_pred_prob'], output_dir)\n\n    # BÆ°á»›c 6: LÆ°u káº¿t quáº£\n    save_results(results, history, training_time, output_dir, CNN_CONFIG)\n\n    # LÆ°u model cuá»‘i cÃ¹ng\n    model.save(output_dir / 'final_model.keras')\n    print(f\"ğŸ’¾ ÄÃ£ lÆ°u model cuá»‘i cÃ¹ng: {output_dir / 'final_model.keras'}\")\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"âœ… HOÃ€N THÃ€NH HUáº¤N LUYá»†N MÃ” HÃŒNH CNN!\")\n    print(\"=\"*80)\n    print(f\"\\nğŸ“ Táº¥t cáº£ káº¿t quáº£ Ä‘Æ°á»£c lÆ°u táº¡i: {output_dir}\")\n    print(f\"   - best_model.keras: Model tá»‘t nháº¥t (theo val_auc)\")\n    print(f\"   - final_model.keras: Model cuá»‘i cÃ¹ng\")\n    print(f\"   - training_history.png: Biá»ƒu Ä‘á»“ training\")\n    print(f\"   - confusion_matrix.png: Confusion matrix\")\n    print(f\"   - roc_curve.png: ROC curve\")\n    print(f\"   - results_summary.json: TÃ³m táº¯t káº¿t quáº£\")\n\n    return model, history, results\n\n\n# ============================================================================\n# CHáº Y SCRIPT\n# ============================================================================\n\nif __name__ == \"__main__\":\n    model, history, results = main()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nGraph Construction for Network Traffic Data\nXÃ¢y dá»±ng Ä‘á»“ thá»‹ tá»« dá»¯ liá»‡u network traffic Ä‘á»ƒ sá»­ dá»¥ng vá»›i GNN\nOptimized for Kaggle with Garbage Collection\n\"\"\"\n\nimport numpy as np\nimport torch\nfrom torch_geometric.data import Data, InMemoryDataset\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport pickle\nimport os\nfrom tqdm import tqdm\nimport gc  # ThÃªm garbage collection\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\nPROCESSED_DATA_DIR = \"/kaggle/input/preprocess-data/processed_data\"  # Dataset Ä‘Ã£ upload\nGRAPH_DATA_DIR = \"/kaggle/working\"  # Output directory\n\n# Graph construction parameters\nK_NEIGHBORS = 8  # Sá»‘ lÆ°á»£ng neighbors cho KNN graph\nSIMILARITY_THRESHOLD = 0.5  # NgÆ°á»¡ng similarity Ä‘á»ƒ táº¡o edge\nGRAPH_TYPE = 'knn'  # 'knn' hoáº·c 'similarity'\nMAX_SAMPLES = 2000000  # Giá»›i háº¡n sá»‘ samples Ä‘á»ƒ training nhanh\n\n# ============================================================================\n# GRAPH CONSTRUCTION CLASS\n# ============================================================================\n\nclass NetworkTrafficGraphBuilder:\n    \"\"\"Class Ä‘á»ƒ xÃ¢y dá»±ng Ä‘á»“ thá»‹ tá»« network traffic data\"\"\"\n\n    def __init__(self, k_neighbors=10, similarity_threshold=0.5, graph_type='knn'):\n        self.k_neighbors = k_neighbors\n        self.similarity_threshold = similarity_threshold\n        self.graph_type = graph_type\n\n    def build_knn_graph(self, X, batch_size=10000):\n        \"\"\"\n        XÃ¢y dá»±ng KNN graph tá»« features\n\n        Args:\n            X: Feature matrix (n_samples, n_features)\n            batch_size: Batch size Ä‘á»ƒ xá»­ lÃ½ (trÃ¡nh memory overflow)\n\n        Returns:\n            edge_index: Edge indices (2, num_edges)\n        \"\"\"\n        print(f\"\\nXÃ¢y dá»±ng KNN graph vá»›i k={self.k_neighbors}...\")\n\n        n_samples = X.shape[0]\n\n        if n_samples <= batch_size:\n            # Xá»­ lÃ½ trá»±c tiáº¿p náº¿u dá»¯ liá»‡u nhá»\n            adjacency = kneighbors_graph(\n                X,\n                n_neighbors=self.k_neighbors,\n                mode='connectivity',\n                include_self=False\n            )\n            \n            # GC sau khi táº¡o adjacency matrix\n            gc.collect()\n            \n        else:\n            # Xá»­ lÃ½ theo batch náº¿u dá»¯ liá»‡u lá»›n - táº¡o edge list thay vÃ¬ adjacency matrix\n            print(f\"  Dá»¯ liá»‡u lá»›n, xá»­ lÃ½ theo batch ({batch_size} samples/batch)...\")\n            edges = []\n\n            # Fit NearestNeighbors má»™t láº§n cho toÃ n bá»™ dataset\n            from sklearn.neighbors import NearestNeighbors\n            print(\"  Fitting NearestNeighbors model...\")\n            nbrs = NearestNeighbors(n_neighbors=self.k_neighbors + 1, algorithm='auto').fit(X)\n            \n            # GC sau khi fit model\n            gc.collect()\n\n            for i in tqdm(range(0, n_samples, batch_size), desc=\"  Building KNN graph\"):\n                end_idx = min(i + batch_size, n_samples)\n                batch_X = X[i:end_idx]\n\n                # TÃ¬m k nearest neighbors\n                distances, indices = nbrs.kneighbors(batch_X)\n\n                # Táº¡o edges (bá» qua neighbor Ä‘áº§u tiÃªn vÃ¬ Ä‘Ã³ lÃ  chÃ­nh nÃ³)\n                for local_idx in range(len(batch_X)):\n                    global_idx = i + local_idx\n                    for neighbor_idx in indices[local_idx][1:]:  # Skip first (itself)\n                        edges.append([global_idx, neighbor_idx])\n\n                # GC sau má»—i 10 batches\n                if (i // batch_size) % 10 == 0:\n                    gc.collect()\n\n            # Chuyá»ƒn edge list sang tensor\n            print(\"  Converting edges to tensor...\")\n            edges = np.array(edges).T\n            edge_index = torch.tensor(edges, dtype=torch.long)\n            \n            # GC sau khi táº¡o tensor\n            del edges\n            gc.collect()\n            \n            print(f\"âœ“ Graph created: {n_samples} nodes, {edge_index.shape[1]} edges\")\n            return edge_index\n\n        # Chuyá»ƒn sang edge_index format (cho trÆ°á»ng há»£p khÃ´ng batch)\n        adjacency = adjacency.tocoo()\n        edge_index = torch.tensor(\n            np.vstack([adjacency.row, adjacency.col]),\n            dtype=torch.long\n        )\n        \n        # GC sau khi táº¡o edge_index\n        del adjacency\n        gc.collect()\n\n        print(f\"âœ“ Graph created: {n_samples} nodes, {edge_index.shape[1]} edges\")\n\n        return edge_index\n\n    def build_similarity_graph(self, X, batch_size=1000):\n        \"\"\"\n        XÃ¢y dá»±ng graph dá»±a trÃªn cosine similarity\n\n        Args:\n            X: Feature matrix\n            batch_size: Batch size\n\n        Returns:\n            edge_index: Edge indices\n        \"\"\"\n        print(f\"\\nXÃ¢y dá»±ng Similarity graph (threshold={self.similarity_threshold})...\")\n\n        n_samples = X.shape[0]\n        edges = []\n\n        # Xá»­ lÃ½ theo batch\n        for i in tqdm(range(0, n_samples, batch_size), desc=\"  Computing similarity\"):\n            end_idx = min(i + batch_size, n_samples)\n\n            # TÃ­nh similarity cho batch hiá»‡n táº¡i vá»›i táº¥t cáº£ samples\n            similarities = cosine_similarity(X[i:end_idx], X)\n\n            # TÃ¬m cÃ¡c edges cÃ³ similarity > threshold\n            for local_idx in range(end_idx - i):\n                global_idx = i + local_idx\n                # Láº¥y indices cÃ³ similarity > threshold (khÃ´ng bao gá»“m chÃ­nh nÃ³)\n                similar_indices = np.where(\n                    (similarities[local_idx] > self.similarity_threshold) &\n                    (np.arange(n_samples) != global_idx)\n                )[0]\n\n                # ThÃªm edges\n                for j in similar_indices:\n                    edges.append([global_idx, j])\n\n            # GC sau má»—i batch Ä‘á»ƒ giáº£i phÃ³ng similarity matrix\n            del similarities\n            if i % (batch_size * 10) == 0:\n                gc.collect()\n\n        if len(edges) == 0:\n            print(\"  âš  KhÃ´ng tÃ¬m tháº¥y edges, giáº£m threshold hoáº·c dÃ¹ng KNN graph\")\n            # Fallback to KNN\n            return self.build_knn_graph(X, batch_size)\n\n        # Chuyá»ƒn sang tensor\n        print(\"  Converting edges to tensor...\")\n        edge_index = torch.tensor(np.array(edges).T, dtype=torch.long)\n        \n        # GC sau khi táº¡o tensor\n        del edges\n        gc.collect()\n\n        print(f\"âœ“ Graph created: {n_samples} nodes, {edge_index.shape[1]} edges\")\n\n        return edge_index\n\n    def create_graph_data(self, X, y, edge_index):\n        \"\"\"\n        Táº¡o PyTorch Geometric Data object\n\n        Args:\n            X: Node features\n            y: Node labels\n            edge_index: Edge indices\n\n        Returns:\n            Data object\n        \"\"\"\n        # Chuyá»ƒn Ä‘á»•i sang tensor\n        x = torch.tensor(X, dtype=torch.float)\n        y = torch.tensor(y, dtype=torch.long)\n\n        # Táº¡o Data object\n        data = Data(x=x, edge_index=edge_index, y=y)\n        \n        # GC sau khi táº¡o Data object\n        gc.collect()\n\n        return data\n\n\nclass CICIDSGraphDataset(InMemoryDataset):\n    \"\"\"Custom Dataset cho CICIDS2018 Graph Data\"\"\"\n\n    def __init__(self, root, data_list=None, transform=None, pre_transform=None):\n        self.data_list = data_list\n        super().__init__(root, transform, pre_transform)\n        self.data, self.slices = torch.load(self.processed_paths[0])\n\n    @property\n    def raw_file_names(self):\n        return []\n\n    @property\n    def processed_file_names(self):\n        return ['data.pt']\n\n    def download(self):\n        pass\n\n    def process(self):\n        if self.data_list is not None:\n            data_list = self.data_list\n\n            if self.pre_filter is not None:\n                data_list = [d for d in data_list if self.pre_filter(d)]\n\n            if self.pre_transform is not None:\n                data_list = [self.pre_transform(d) for d in data_list]\n\n            data, slices = self.collate(data_list)\n            torch.save((data, slices), self.processed_paths[0])\n\n\n# ============================================================================\n# MAIN FUNCTION\n# ============================================================================\n\ndef build_graph_dataset():\n    \"\"\"XÃ¢y dá»±ng graph dataset tá»« processed data\"\"\"\n\n    print(\"=\" * 80)\n    print(\"BUILDING GRAPH DATASET FOR GNN\")\n    print(\"=\" * 80)\n\n    # Táº¡o output directory\n    os.makedirs(GRAPH_DATA_DIR, exist_ok=True)\n\n    # Load processed data\n    print(\"\\nLoading processed data...\")\n    X = np.load(os.path.join(PROCESSED_DATA_DIR, \"X_features.npy\"))\n    y_binary = np.load(os.path.join(PROCESSED_DATA_DIR, \"y_binary.npy\"))\n    y_multi = np.load(os.path.join(PROCESSED_DATA_DIR, \"y_multi.npy\"))\n\n    with open(os.path.join(PROCESSED_DATA_DIR, \"metadata.pkl\"), 'rb') as f:\n        metadata = pickle.load(f)\n\n    print(f\"âœ“ Loaded data: {X.shape[0]:,} samples, {X.shape[1]} features\")\n    print(f\"âœ“ Binary classes: {len(np.unique(y_binary))}\")\n    print(f\"âœ“ Multi classes: {len(np.unique(y_multi))}\")\n    \n    # GC sau khi load data\n    gc.collect()\n\n    # Giáº£m kÃ­ch thÆ°á»›c náº¿u quÃ¡ lá»›n (optional - Ä‘á»ƒ training nhanh hÆ¡n)\n    if X.shape[0] > MAX_SAMPLES:\n        print(f\"\\nâš  Dataset lá»›n ({X.shape[0]:,} samples), sampling {MAX_SAMPLES:,} samples...\")\n        indices = np.random.choice(X.shape[0], MAX_SAMPLES, replace=False)\n        X = X[indices]\n        y_binary = y_binary[indices]\n        y_multi = y_multi[indices]\n        print(f\"âœ“ Sampled data: {X.shape[0]:,} samples\")\n        \n        # GC sau khi sampling\n        del indices\n        gc.collect()\n\n    # Build graph\n    builder = NetworkTrafficGraphBuilder(\n        k_neighbors=K_NEIGHBORS,\n        similarity_threshold=SIMILARITY_THRESHOLD,\n        graph_type=GRAPH_TYPE\n    )\n\n    if GRAPH_TYPE == 'knn':\n        edge_index = builder.build_knn_graph(X)\n    else:\n        edge_index = builder.build_similarity_graph(X)\n    \n    # GC sau khi build graph\n    gc.collect()\n\n    # Create graph data objects\n    print(\"\\nCreating graph data objects...\")\n\n    # Binary classification graph\n    print(\"  Creating binary classification graph...\")\n    graph_binary = builder.create_graph_data(X, y_binary, edge_index)\n    \n    # GC sau khi táº¡o binary graph\n    gc.collect()\n\n    # Multi-class classification graph\n    print(\"  Creating multi-class classification graph...\")\n    graph_multi = builder.create_graph_data(X, y_multi, edge_index)\n    \n    # GC sau khi táº¡o multi graph\n    gc.collect()\n\n    # Save graphs\n    print(\"\\nSaving graph data...\")\n    torch.save(graph_binary, os.path.join(GRAPH_DATA_DIR, \"graph_binary.pt\"))\n    torch.save(graph_multi, os.path.join(GRAPH_DATA_DIR, \"graph_multi.pt\"))\n\n    # Save edge_index separately\n    torch.save(edge_index, os.path.join(GRAPH_DATA_DIR, \"edge_index.pt\"))\n\n    # Save graph metadata\n    graph_metadata = {\n        'n_nodes': X.shape[0],\n        'n_features': X.shape[1],\n        'n_edges': edge_index.shape[1],\n        'k_neighbors': K_NEIGHBORS,\n        'graph_type': GRAPH_TYPE,\n        'avg_degree': edge_index.shape[1] / X.shape[0]\n    }\n\n    with open(os.path.join(GRAPH_DATA_DIR, \"graph_metadata.pkl\"), 'wb') as f:\n        pickle.dump(graph_metadata, f)\n    \n    # GC cuá»‘i cÃ¹ng\n    gc.collect()\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"GRAPH CONSTRUCTION COMPLETED!\")\n    print(\"=\" * 80)\n    print(f\"Nodes: {graph_metadata['n_nodes']:,}\")\n    print(f\"Features per node: {graph_metadata['n_features']}\")\n    print(f\"Edges: {graph_metadata['n_edges']:,}\")\n    print(f\"Average degree: {graph_metadata['avg_degree']:.2f}\")\n    print(f\"Output directory: {GRAPH_DATA_DIR}\")\n    print(\"=\" * 80)\n\n    return graph_binary, graph_multi, graph_metadata\n\n\nif __name__ == \"__main__\":\n    graph_binary, graph_multi, metadata = build_graph_dataset()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nGNN Training Script for Kaggle - IoT Network Anomaly Detection\nScript hoÃ n chá»‰nh bao gá»“m cáº£ model definitions vÃ  training\nChá»‰ cáº§n cháº¡y file nÃ y trÃªn Kaggle\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.nn import GCNConv, GATConv, SAGEConv, BatchNorm\nimport numpy as np\nimport pickle\nimport os\nimport shutil\nfrom datetime import datetime\nfrom sklearn.metrics import (\n    precision_score, recall_score, f1_score,\n    confusion_matrix, classification_report, roc_auc_score\n)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ============================================================================\n# GNN MODEL DEFINITIONS\n# ============================================================================\n\nclass GCN(nn.Module):\n    \"\"\"Graph Convolutional Network\"\"\"\n\n    def __init__(self, in_channels, hidden_channels, num_classes, num_layers=3, dropout=0.5):\n        super(GCN, self).__init__()\n        self.num_layers = num_layers\n        self.dropout = dropout\n\n        self.convs = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n\n        # First layer\n        self.convs.append(GCNConv(in_channels, hidden_channels))\n        self.batch_norms.append(BatchNorm(hidden_channels))\n\n        # Hidden layers\n        for _ in range(num_layers - 2):\n            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n            self.batch_norms.append(BatchNorm(hidden_channels))\n\n        # Output layer\n        self.convs.append(GCNConv(hidden_channels, hidden_channels))\n        self.batch_norms.append(BatchNorm(hidden_channels))\n\n        # Classifier\n        self.classifier = nn.Linear(hidden_channels, num_classes)\n\n    def forward(self, x, edge_index):\n        for i in range(self.num_layers):\n            x = self.convs[i](x, edge_index)\n            x = self.batch_norms[i](x)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.classifier(x)\n        return x\n\n\nclass GAT(nn.Module):\n    \"\"\"Graph Attention Network\"\"\"\n\n    def __init__(self, in_channels, hidden_channels, num_classes,\n                 num_layers=3, heads=4, dropout=0.5):\n        super(GAT, self).__init__()\n        self.num_layers = num_layers\n        self.dropout = dropout\n\n        self.convs = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n\n        # First layer\n        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout))\n        self.batch_norms.append(BatchNorm(hidden_channels * heads))\n\n        # Hidden layers\n        for _ in range(num_layers - 2):\n            self.convs.append(GATConv(hidden_channels * heads, hidden_channels,\n                                     heads=heads, dropout=dropout))\n            self.batch_norms.append(BatchNorm(hidden_channels * heads))\n\n        # Output layer\n        self.convs.append(GATConv(hidden_channels * heads, hidden_channels,\n                                 heads=1, concat=False, dropout=dropout))\n        self.batch_norms.append(BatchNorm(hidden_channels))\n\n        # Classifier\n        self.classifier = nn.Linear(hidden_channels, num_classes)\n\n    def forward(self, x, edge_index):\n        for i in range(self.num_layers):\n            x = self.convs[i](x, edge_index)\n            x = self.batch_norms[i](x)\n            x = F.elu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.classifier(x)\n        return x\n\n\nclass GraphSAGE(nn.Module):\n    \"\"\"GraphSAGE\"\"\"\n\n    def __init__(self, in_channels, hidden_channels, num_classes,\n                 num_layers=3, dropout=0.5, aggregator='mean'):\n        super(GraphSAGE, self).__init__()\n        self.num_layers = num_layers\n        self.dropout = dropout\n\n        self.convs = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n\n        # First layer\n        self.convs.append(SAGEConv(in_channels, hidden_channels, aggr=aggregator))\n        self.batch_norms.append(BatchNorm(hidden_channels))\n\n        # Hidden layers\n        for _ in range(num_layers - 2):\n            self.convs.append(SAGEConv(hidden_channels, hidden_channels, aggr=aggregator))\n            self.batch_norms.append(BatchNorm(hidden_channels))\n\n        # Output layer\n        self.convs.append(SAGEConv(hidden_channels, hidden_channels, aggr=aggregator))\n        self.batch_norms.append(BatchNorm(hidden_channels))\n\n        # Classifier\n        self.classifier = nn.Linear(hidden_channels, num_classes)\n\n    def forward(self, x, edge_index):\n        for i in range(self.num_layers):\n            x = self.convs[i](x, edge_index)\n            x = self.batch_norms[i](x)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.classifier(x)\n        return x\n\n\nclass HybridGNN(nn.Module):\n    \"\"\"Hybrid GNN combining GCN and GAT\"\"\"\n\n    def __init__(self, in_channels, hidden_channels, num_classes,\n                 num_layers=3, heads=4, dropout=0.5):\n        super(HybridGNN, self).__init__()\n        self.num_layers = num_layers\n        self.dropout = dropout\n\n        # GCN branch\n        self.gcn_convs = nn.ModuleList()\n        self.gcn_bns = nn.ModuleList()\n\n        # GAT branch\n        self.gat_convs = nn.ModuleList()\n        self.gat_bns = nn.ModuleList()\n\n        # First layer\n        self.gcn_convs.append(GCNConv(in_channels, hidden_channels))\n        self.gcn_bns.append(BatchNorm(hidden_channels))\n        self.gat_convs.append(GATConv(in_channels, hidden_channels // heads, heads=heads))\n        self.gat_bns.append(BatchNorm(hidden_channels))\n\n        # Hidden layers\n        for _ in range(num_layers - 2):\n            self.gcn_convs.append(GCNConv(hidden_channels, hidden_channels))\n            self.gcn_bns.append(BatchNorm(hidden_channels))\n            self.gat_convs.append(GATConv(hidden_channels, hidden_channels // heads, heads=heads))\n            self.gat_bns.append(BatchNorm(hidden_channels))\n\n        # Fusion\n        self.fusion = nn.Linear(hidden_channels * 2, hidden_channels)\n        self.fusion_bn = BatchNorm(hidden_channels)\n\n        # Classifier\n        self.classifier = nn.Linear(hidden_channels, num_classes)\n\n    def forward(self, x, edge_index):\n        # GCN branch\n        x_gcn = x\n        for i in range(self.num_layers - 1):\n            x_gcn = self.gcn_convs[i](x_gcn, edge_index)\n            x_gcn = self.gcn_bns[i](x_gcn)\n            x_gcn = F.relu(x_gcn)\n            x_gcn = F.dropout(x_gcn, p=self.dropout, training=self.training)\n\n        # GAT branch\n        x_gat = x\n        for i in range(self.num_layers - 1):\n            x_gat = self.gat_convs[i](x_gat, edge_index)\n            x_gat = self.gat_bns[i](x_gat)\n            x_gat = F.elu(x_gat)\n            x_gat = F.dropout(x_gat, p=self.dropout, training=self.training)\n\n        # Fusion\n        x = torch.cat([x_gcn, x_gat], dim=1)\n        x = self.fusion(x)\n        x = self.fusion_bn(x)\n        x = F.relu(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.classifier(x)\n        return x\n\n\ndef create_model(model_name, in_channels, hidden_channels, num_classes, **kwargs):\n    \"\"\"Factory function Ä‘á»ƒ táº¡o model\"\"\"\n    models = {\n        'GCN': GCN,\n        'GAT': GAT,\n        'GraphSAGE': GraphSAGE,\n        'Hybrid': HybridGNN\n    }\n    if model_name not in models:\n        raise ValueError(f\"Model {model_name} khÃ´ng há»— trá»£. Chá»n: {list(models.keys())}\")\n    return models[model_name](in_channels, hidden_channels, num_classes, **kwargs)\n\n\ndef count_parameters(model):\n    \"\"\"Äáº¿m sá»‘ parameters\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\n# ============================================================================\n# KAGGLE CONFIGURATION\n# ============================================================================\nWORKING_DIR = \"/kaggle/working\"\nGRAPH_DATA_DIR = \"/kaggle/working\"\nMODEL_DIR = os.path.join(WORKING_DIR, \"models\")\nRESULTS_DIR = os.path.join(WORKING_DIR, \"results\")\n\n# Model config\nMODEL_NAME = 'GAT'  # 'GCN', 'GAT', 'GraphSAGE', 'Hybrid'\nHIDDEN_CHANNELS = 128\nNUM_LAYERS = 3\nHEADS = 4\nDROPOUT = 0.3\n\n# Training config\nLEARNING_RATE = 0.001\nWEIGHT_DECAY = 5e-4\nNUM_EPOCHS = 30\nPATIENCE = 15\nTASK = 'binary'  # 'binary' hoáº·c 'multi'\n\n# Data split\nTRAIN_RATIO = 0.7\nVAL_RATIO = 0.15\nTEST_RATIO = 0.15\n\n# Device\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"ğŸ”¥ Using device: {DEVICE}\")\n\n# ============================================================================\n# TRAINING CLASS\n# ============================================================================\n\nclass GNNTrainer:\n    \"\"\"Class Ä‘á»ƒ train GNN model\"\"\"\n\n    def __init__(self, model, device, task='binary'):\n        self.model = model.to(device)\n        self.device = device\n        self.task = task\n        self.history = {\n            'train_loss': [],\n            'train_acc': [],\n            'val_loss': [],\n            'val_acc': [],\n            'learning_rate': []\n        }\n        self.best_val_acc = 0\n        self.best_epoch = 0\n\n    def train_epoch(self, data, train_mask, optimizer):\n        \"\"\"Train má»™t epoch\"\"\"\n        self.model.train()\n        optimizer.zero_grad()\n\n        out = self.model(data.x, data.edge_index)\n        loss = F.cross_entropy(out[train_mask], data.y[train_mask])\n        loss.backward()\n        optimizer.step()\n\n        pred = out[train_mask].argmax(dim=1)\n        acc = (pred == data.y[train_mask]).float().mean()\n        return loss.item(), acc.item()\n\n    @torch.no_grad()\n    def evaluate(self, data, mask):\n        \"\"\"Evaluate model\"\"\"\n        self.model.eval()\n        out = self.model(data.x, data.edge_index)\n        loss = F.cross_entropy(out[mask], data.y[mask])\n        pred = out[mask].argmax(dim=1)\n        acc = (pred == data.y[mask]).float().mean()\n        return loss.item(), acc.item(), pred.cpu().numpy(), data.y[mask].cpu().numpy()\n\n    def train(self, data, train_mask, val_mask, optimizer, scheduler, num_epochs, patience):\n        \"\"\"Full training loop\"\"\"\n        print(\"\\n\" + \"=\" * 80)\n        print(\"ğŸš€ TRAINING GNN MODEL\")\n        print(\"=\" * 80)\n        print(f\"ğŸ“ Device: {self.device}\")\n        print(f\"ğŸ§  Model: {self.model.__class__.__name__}\")\n        print(f\"ğŸ“Š Parameters: {count_parameters(self.model):,}\")\n        print(f\"â° Epochs: {num_epochs}\")\n        print(\"=\" * 80 + \"\\n\")\n\n        patience_counter = 0\n\n        for epoch in range(1, num_epochs + 1):\n            train_loss, train_acc = self.train_epoch(data, train_mask, optimizer)\n            val_loss, val_acc, _, _ = self.evaluate(data, val_mask)\n\n            if scheduler is not None:\n                scheduler.step(val_loss)\n\n            self.history['train_loss'].append(train_loss)\n            self.history['train_acc'].append(train_acc)\n            self.history['val_loss'].append(val_loss)\n            self.history['val_acc'].append(val_acc)\n            self.history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n\n            if val_acc > self.best_val_acc:\n                self.best_val_acc = val_acc\n                self.best_epoch = epoch\n                patience_counter = 0\n                self.save_model(os.path.join(MODEL_DIR, f'best_model_{self.task}.pt'))\n            else:\n                patience_counter += 1\n\n            if epoch % 10 == 0 or epoch == 1:\n                print(f\"Epoch {epoch:3d}/{num_epochs} | \"\n                      f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n                      f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n\n            if patience_counter >= patience:\n                print(f\"\\nâ¹ï¸  Early stopping at epoch {epoch}\")\n                break\n\n        print(f\"\\nâœ… Best Val Acc: {self.best_val_acc:.4f} at epoch {self.best_epoch}\")\n        return self.history\n\n    def test(self, data, test_mask):\n        \"\"\"Test model\"\"\"\n        print(\"\\n\" + \"=\" * 80)\n        print(\"ğŸ§ª TESTING MODEL\")\n        print(\"=\" * 80)\n\n        test_loss, test_acc, pred, true = self.evaluate(data, test_mask)\n\n        print(f\"ğŸ“Š Test Accuracy: {test_acc:.4f}\")\n\n        precision = precision_score(true, pred, average='weighted', zero_division=0)\n        recall = recall_score(true, pred, average='weighted', zero_division=0)\n        f1 = f1_score(true, pred, average='weighted', zero_division=0)\n\n        print(f\"ğŸ“Š Precision: {precision:.4f}\")\n        print(f\"ğŸ“Š Recall: {recall:.4f}\")\n        print(f\"ğŸ“Š F1-Score: {f1:.4f}\")\n\n        # ROC-AUC for binary\n        roc_auc = None\n        if self.task == 'binary':\n            try:\n                self.model.eval()\n                with torch.no_grad():\n                    out = self.model(data.x, data.edge_index)\n                    probs = F.softmax(out[test_mask], dim=1)[:, 1].cpu().numpy()\n                roc_auc = roc_auc_score(true, probs)\n                print(f\"ğŸ“Š ROC-AUC: {roc_auc:.4f}\")\n            except:\n                pass\n\n        cm = confusion_matrix(true, pred)\n        print(\"\\n\" + \"-\" * 80)\n        print(\"ğŸ“‹ Classification Report:\")\n        print(\"-\" * 80)\n        print(classification_report(true, pred, zero_division=0))\n\n        return {\n            'test_loss': test_loss,\n            'test_acc': test_acc,\n            'precision': precision,\n            'recall': recall,\n            'f1': f1,\n            'roc_auc': roc_auc,\n            'confusion_matrix': cm,\n            'predictions': pred,\n            'true_labels': true\n        }\n\n    def save_model(self, path):\n        \"\"\"Save model\"\"\"\n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'best_val_acc': self.best_val_acc,\n            'best_epoch': self.best_epoch,\n            'history': self.history\n        }, path)\n\n    def load_model(self, path):\n        \"\"\"Load model\"\"\"\n        checkpoint = torch.load(path, map_location=self.device)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.best_val_acc = checkpoint['best_val_acc']\n        self.best_epoch = checkpoint['best_epoch']\n        self.history = checkpoint['history']\n\n\n# ============================================================================\n# VISUALIZATION\n# ============================================================================\n\ndef plot_training_history(history, save_path=None):\n    \"\"\"Plot training history\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss')\n    axes[0].set_title('Training and Validation Loss')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n\n    axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n    axes[1].plot(history['val_acc'], label='Val Accuracy', linewidth=2)\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].set_title('Training and Validation Accuracy')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.close()\n\n\ndef plot_confusion_matrix(cm, class_names=None, save_path=None):\n    \"\"\"Plot confusion matrix\"\"\"\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.tight_layout()\n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.close()\n\n\n# ============================================================================\n# MAIN\n# ============================================================================\n\ndef main():\n    \"\"\"Main training function\"\"\"\n    \n    os.makedirs(MODEL_DIR, exist_ok=True)\n    os.makedirs(RESULTS_DIR, exist_ok=True)\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"ğŸ¤– GNN TRAINING FOR IoT ANOMALY DETECTION - KAGGLE\")\n    print(\"=\" * 80 + \"\\n\")\n\n    # Load graph\n    print(\"ğŸ“‚ Loading graph data...\")\n    graph_file = f\"graph_{TASK}.pt\"\n    graph_path = os.path.join(GRAPH_DATA_DIR, graph_file)\n    \n    if not os.path.exists(graph_path):\n        print(f\"âŒ ERROR: Graph file not found: {graph_path}\")\n        print(\"ğŸ“ Available files:\")\n        for f in os.listdir(WORKING_DIR):\n            print(f\"  - {f}\")\n        return\n    \n    data = torch.load(graph_path, weights_only=False)\n    data = data.to(DEVICE)\n\n    print(f\"âœ… Graph: {data.num_nodes:,} nodes, {data.num_edges:,} edges\")\n    print(f\"âœ… Features: {data.num_features}\")\n    print(f\"âœ… Classes: {len(torch.unique(data.y))}\")\n\n    # Load metadata\n    metadata_path = os.path.join(GRAPH_DATA_DIR, \"graph_metadata.pkl\")\n    class_names = None\n    if os.path.exists(metadata_path):\n        with open(metadata_path, 'rb') as f:\n            metadata = pickle.load(f)\n            if TASK in metadata and 'class_names' in metadata[TASK]:\n                class_names = metadata[TASK]['class_names']\n    \n    if class_names is None:\n        class_names = ['Benign', 'Attack'] if TASK == 'binary' else [f'Class_{i}' for i in range(len(torch.unique(data.y)))]\n\n    # Create masks\n    print(\"\\nğŸ“Š Creating data splits...\")\n    num_nodes = data.num_nodes\n    indices = torch.randperm(num_nodes)\n\n    train_size = int(num_nodes * TRAIN_RATIO)\n    val_size = int(num_nodes * VAL_RATIO)\n\n    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n\n    train_mask[indices[:train_size]] = True\n    val_mask[indices[train_size:train_size + val_size]] = True\n    test_mask[indices[train_size + val_size:]] = True\n\n    print(f\"âœ… Train: {train_mask.sum():,} | Val: {val_mask.sum():,} | Test: {test_mask.sum():,}\")\n\n    # Create model\n    print(f\"\\nğŸ—ï¸  Creating {MODEL_NAME} model...\")\n    num_classes = len(torch.unique(data.y))\n\n    model_kwargs = {'num_layers': NUM_LAYERS, 'dropout': DROPOUT}\n    if MODEL_NAME in ['GAT', 'Hybrid']:\n        model_kwargs['heads'] = HEADS\n\n    model = create_model(\n        MODEL_NAME,\n        in_channels=data.num_features,\n        hidden_channels=HIDDEN_CHANNELS,\n        num_classes=num_classes,\n        **model_kwargs\n    )\n\n    print(f\"âœ… Model: {count_parameters(model):,} parameters\")\n\n    # Train\n    trainer = GNNTrainer(model, DEVICE, task=TASK)\n    optimizer = Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n\n    history = trainer.train(data, train_mask, val_mask, optimizer, scheduler, NUM_EPOCHS, PATIENCE)\n\n    # Test\n    print(\"\\nğŸ”„ Loading best model...\")\n    trainer.load_model(os.path.join(MODEL_DIR, f'best_model_{TASK}.pt'))\n    results = trainer.test(data, test_mask)\n\n    # Save results\n    print(\"\\nğŸ’¾ Saving results...\")\n    plot_training_history(history, os.path.join(RESULTS_DIR, f'training_history_{TASK}.png'))\n    plot_confusion_matrix(results['confusion_matrix'], class_names, os.path.join(RESULTS_DIR, f'confusion_matrix_{TASK}.png'))\n\n    with open(os.path.join(RESULTS_DIR, f'results_{TASK}.pkl'), 'wb') as f:\n        pickle.dump(results, f)\n\n    config = {\n        'model_name': MODEL_NAME,\n        'hidden_channels': HIDDEN_CHANNELS,\n        'num_layers': NUM_LAYERS,\n        'dropout': DROPOUT,\n        'best_epoch': trainer.best_epoch,\n        'best_val_acc': trainer.best_val_acc,\n        'test_results': {\n            'accuracy': results['test_acc'],\n            'precision': results['precision'],\n            'recall': results['recall'],\n            'f1': results['f1'],\n            'roc_auc': results['roc_auc']\n        }\n    }\n\n    with open(os.path.join(RESULTS_DIR, f'config_{TASK}.pkl'), 'wb') as f:\n        pickle.dump(config, f)\n\n    # Summary\n    with open(os.path.join(RESULTS_DIR, 'summary.txt'), 'w') as f:\n        f.write(\"=\" * 80 + \"\\n\")\n        f.write(\"TRAINING SUMMARY\\n\")\n        f.write(\"=\" * 80 + \"\\n\\n\")\n        f.write(f\"Model: {MODEL_NAME}\\n\")\n        f.write(f\"Task: {TASK}\\n\")\n        f.write(f\"Best Epoch: {trainer.best_epoch}\\n\")\n        f.write(f\"Best Val Acc: {trainer.best_val_acc:.4f}\\n\\n\")\n        f.write(f\"Test Results:\\n\")\n        f.write(f\"  Accuracy: {results['test_acc']:.4f}\\n\")\n        f.write(f\"  Precision: {results['precision']:.4f}\\n\")\n        f.write(f\"  Recall: {results['recall']:.4f}\\n\")\n        f.write(f\"  F1-Score: {results['f1']:.4f}\\n\")\n        if results['roc_auc']:\n            f.write(f\"  ROC-AUC: {results['roc_auc']:.4f}\\n\")\n\n    # ZIP for download\n    print(\"\\n\" + \"=\" * 80)\n    print(\"ğŸ“¦ CREATING ZIP FILES\")\n    print(\"=\" * 80)\n\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    zip_results = f'gnn_results_{TASK}_{timestamp}'\n    zip_models = f'gnn_models_{TASK}_{timestamp}'\n\n    shutil.make_archive(os.path.join(WORKING_DIR, zip_results), 'zip', RESULTS_DIR)\n    shutil.make_archive(os.path.join(WORKING_DIR, zip_models), 'zip', MODEL_DIR)\n\n    print(f\"âœ… Results: {zip_results}.zip\")\n    print(f\"âœ… Models: {zip_models}.zip\")\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"ğŸ‰ TRAINING COMPLETED!\")\n    print(\"=\" * 80)\n    print(f\"ğŸ“Š Test Accuracy: {results['test_acc']:.4f}\")\n    print(f\"ğŸ“Š F1-Score: {results['f1']:.4f}\")\n    print(f\"\\nğŸ“¥ Download: {zip_results}.zip & {zip_models}.zip\")\n    print(\"=\" * 80 + \"\\n\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.1.0+cu121.html","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install PyTorch Geometric\n!pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n!pip install torch-geometric\n\n# Upgrade scikit-learn (náº¿u cáº§n)\n!pip install --upgrade scikit-learn\n\n# Install tqdm\n!pip install tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nGNN Training Script - FIXED VERSION with Mini-Batch Support\nKey fixes:\n1. Added NeighborLoader for mini-batch training\n2. Reduced memory footprint significantly\n3. Added gradient accumulation option\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch_geometric.nn import GCNConv, GATConv, SAGEConv, BatchNorm\n# Removed: from torch_geometric.loader import NeighborLoader\nimport numpy as np\nimport pickle\nimport os\nimport shutil\nfrom datetime import datetime\nfrom sklearn.metrics import (\n    precision_score, recall_score, f1_score,\n    confusion_matrix, classification_report, roc_auc_score\n)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# ============================================================================\n# MODEL DEFINITIONS (Same as before - these are fine)\n# ============================================================================\n\nclass GCN(nn.Module):\n    def __init__(self, in_channels, hidden_channels, num_classes, num_layers=3, dropout=0.5):\n        super(GCN, self).__init__()\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.convs = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n\n        self.convs.append(GCNConv(in_channels, hidden_channels))\n        self.batch_norms.append(BatchNorm(hidden_channels))\n\n        for _ in range(num_layers - 2):\n            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n            self.batch_norms.append(BatchNorm(hidden_channels))\n\n        self.convs.append(GCNConv(hidden_channels, hidden_channels))\n        self.batch_norms.append(BatchNorm(hidden_channels))\n        self.classifier = nn.Linear(hidden_channels, num_classes)\n\n    def forward(self, x, edge_index):\n        for i in range(self.num_layers):\n            x = self.convs[i](x, edge_index)\n            x = self.batch_norms[i](x)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.classifier(x)\n        return x\n\n\nclass GAT(nn.Module):\n    def __init__(self, in_channels, hidden_channels, num_classes,\n                 num_layers=3, heads=4, dropout=0.5):\n        super(GAT, self).__init__()\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.convs = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n\n        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout))\n        self.batch_norms.append(BatchNorm(hidden_channels * heads))\n\n        for _ in range(num_layers - 2):\n            self.convs.append(GATConv(hidden_channels * heads, hidden_channels,\n                                     heads=heads, dropout=dropout))\n            self.batch_norms.append(BatchNorm(hidden_channels * heads))\n\n        self.convs.append(GATConv(hidden_channels * heads, hidden_channels,\n                                 heads=1, concat=False, dropout=dropout))\n        self.batch_norms.append(BatchNorm(hidden_channels))\n        self.classifier = nn.Linear(hidden_channels, num_classes)\n\n    def forward(self, x, edge_index):\n        for i in range(self.num_layers):\n            x = self.convs[i](x, edge_index)\n            x = self.batch_norms[i](x)\n            x = F.elu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.classifier(x)\n        return x\n\n\nclass GraphSAGE(nn.Module):\n    def __init__(self, in_channels, hidden_channels, num_classes,\n                 num_layers=3, dropout=0.5, aggregator='mean'):\n        super(GraphSAGE, self).__init__()\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.convs = nn.ModuleList()\n        self.batch_norms = nn.ModuleList()\n\n        self.convs.append(SAGEConv(in_channels, hidden_channels, aggr=aggregator))\n        self.batch_norms.append(BatchNorm(hidden_channels))\n\n        for _ in range(num_layers - 2):\n            self.convs.append(SAGEConv(hidden_channels, hidden_channels, aggr=aggregator))\n            self.batch_norms.append(BatchNorm(hidden_channels))\n\n        self.convs.append(SAGEConv(hidden_channels, hidden_channels, aggr=aggregator))\n        self.batch_norms.append(BatchNorm(hidden_channels))\n        self.classifier = nn.Linear(hidden_channels, num_classes)\n\n    def forward(self, x, edge_index):\n        for i in range(self.num_layers):\n            x = self.convs[i](x, edge_index)\n            x = self.batch_norms[i](x)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.classifier(x)\n        return x\n\n\ndef create_model(model_name, in_channels, hidden_channels, num_classes, **kwargs):\n    models = {'GCN': GCN, 'GAT': GAT, 'GraphSAGE': GraphSAGE}\n    if model_name not in models:\n        raise ValueError(f\"Model {model_name} not supported. Choose: {list(models.keys())}\")\n    return models[model_name](in_channels, hidden_channels, num_classes, **kwargs)\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\n# ============================================================================\n# CONFIGURATION - OPTIMIZED FOR MEMORY\n# ============================================================================\nWORKING_DIR = \"/kaggle/working\"\nGRAPH_DATA_DIR = \"/kaggle/input/model/pytorch/default/1\"\nMODEL_DIR = os.path.join(WORKING_DIR, \"models\")\nRESULTS_DIR = os.path.join(WORKING_DIR, \"results\")\n\n# Model config - Reduced for memory\nMODEL_NAME = 'GraphSAGE'  # GraphSAGE is most memory-efficient\nHIDDEN_CHANNELS = 64      # Reduced from 128\nNUM_LAYERS = 2            # Reduced from 3\nHEADS = 2                 # Reduced from 4 (for GAT)\nDROPOUT = 0.3\n\n# Training config\nLEARNING_RATE = 0.001\nWEIGHT_DECAY = 5e-4\nNUM_EPOCHS = 30\nPATIENCE = 15\nTASK = 'binary'\n\n# MINI-BATCH CONFIG\nBATCH_SIZE = 2048         # Process nodes in batches\nNUM_WORKERS = 0           # Not used in simple batching\n\n# Gradient accumulation (simulate larger batch)\nACCUMULATION_STEPS = 4\n\n# Data split\nTRAIN_RATIO = 0.7\nVAL_RATIO = 0.15\nTEST_RATIO = 0.15\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"ğŸ”¥ Using device: {DEVICE}\")\n\n# ============================================================================\n# MINI-BATCH TRAINER CLASS - COMPLETELY REWRITTEN\n# ============================================================================\n\nclass MiniBatchGNNTrainer:\n    \"\"\"Memory-efficient mini-batch trainer\"\"\"\n\n    def __init__(self, model, device, task='binary'):\n        self.model = model.to(device)\n        self.device = device\n        self.task = task\n        self.history = {\n            'train_loss': [], 'train_acc': [],\n            'val_loss': [], 'val_acc': [], 'learning_rate': []\n        }\n        self.best_val_acc = 0\n        self.best_epoch = 0\n\n    def train_epoch(self, train_batches, data, optimizer, accumulation_steps=1):\n        \"\"\"Train one epoch with simple batches\"\"\"\n        self.model.train()\n        total_loss = 0\n        total_correct = 0\n        total_samples = 0\n        \n        optimizer.zero_grad()\n        \n        # Move full graph to device once\n        data = data.to(self.device)\n        \n        for i, batch_idx in enumerate(train_batches):\n            batch_idx = batch_idx.to(self.device)\n            \n            # Forward pass on full graph\n            out = self.model(data.x, data.edge_index)\n            \n            # Compute loss only on batch nodes\n            loss = F.cross_entropy(out[batch_idx], data.y[batch_idx])\n            loss = loss / accumulation_steps\n            \n            # Backward pass\n            loss.backward()\n            \n            # Update weights\n            if (i + 1) % accumulation_steps == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n            \n            # Metrics\n            with torch.no_grad():\n                pred = out[batch_idx].argmax(dim=1)\n                total_correct += (pred == data.y[batch_idx]).sum().item()\n                total_loss += loss.item() * accumulation_steps * len(batch_idx)\n                total_samples += len(batch_idx)\n        \n        # Final update\n        if (i + 1) % accumulation_steps != 0:\n            optimizer.step()\n            optimizer.zero_grad()\n        \n        return total_loss / total_samples, total_correct / total_samples\n\n    @torch.no_grad()\n    def evaluate(self, batches, data):\n        \"\"\"Evaluate with simple batches\"\"\"\n        self.model.eval()\n        total_loss = 0\n        total_correct = 0\n        total_samples = 0\n        all_preds = []\n        all_labels = []\n        \n        data = data.to(self.device)\n        \n        for batch_idx in batches:\n            batch_idx = batch_idx.to(self.device)\n            \n            # Forward on full graph\n            out = self.model(data.x, data.edge_index)\n            \n            # Loss and predictions on batch\n            loss = F.cross_entropy(out[batch_idx], data.y[batch_idx])\n            pred = out[batch_idx].argmax(dim=1)\n            \n            total_correct += (pred == data.y[batch_idx]).sum().item()\n            total_loss += loss.item() * len(batch_idx)\n            total_samples += len(batch_idx)\n            \n            all_preds.append(pred.cpu())\n            all_labels.append(data.y[batch_idx].cpu())\n        \n        all_preds = torch.cat(all_preds).numpy()\n        all_labels = torch.cat(all_labels).numpy()\n        \n        return total_loss / total_samples, total_correct / total_samples, all_preds, all_labels\n\n    def train(self, train_batches, val_batches, data, optimizer, scheduler, num_epochs, patience):\n        \"\"\"Full training loop\"\"\"\n        print(\"\\n\" + \"=\" * 80)\n        print(\"ğŸš€ TRAINING GNN MODEL (BATCHED)\")\n        print(\"=\" * 80)\n        print(f\"ğŸ“ Device: {self.device}\")\n        print(f\"ğŸ§  Model: {self.model.__class__.__name__}\")\n        print(f\"ğŸ“Š Parameters: {count_parameters(self.model):,}\")\n        print(f\"ğŸ¯ Batch size: {BATCH_SIZE}\")\n        print(f\"â° Epochs: {num_epochs}\")\n        print(\"=\" * 80 + \"\\n\")\n\n        patience_counter = 0\n\n        for epoch in range(1, num_epochs + 1):\n            train_loss, train_acc = self.train_epoch(train_batches, data, optimizer, ACCUMULATION_STEPS)\n            val_loss, val_acc, _, _ = self.evaluate(val_batches, data)\n\n            if scheduler is not None:\n                scheduler.step(val_loss)\n\n            self.history['train_loss'].append(train_loss)\n            self.history['train_acc'].append(train_acc)\n            self.history['val_loss'].append(val_loss)\n            self.history['val_acc'].append(val_acc)\n            self.history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n\n            if val_acc > self.best_val_acc:\n                self.best_val_acc = val_acc\n                self.best_epoch = epoch\n                patience_counter = 0\n                self.save_model(os.path.join(MODEL_DIR, f'best_model_{self.task}.pt'))\n            else:\n                patience_counter += 1\n\n            if epoch % 5 == 0 or epoch == 1:\n                print(f\"Epoch {epoch:3d}/{num_epochs} | \"\n                      f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n                      f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n\n            if patience_counter >= patience:\n                print(f\"\\nâ¹ï¸  Early stopping at epoch {epoch}\")\n                break\n\n        print(f\"\\nâœ… Best Val Acc: {self.best_val_acc:.4f} at epoch {self.best_epoch}\")\n        return self.history\n\n    def test(self, test_batches, data):\n        \"\"\"Test model\"\"\"\n        print(\"\\n\" + \"=\" * 80)\n        print(\"ğŸ§ª TESTING MODEL\")\n        print(\"=\" * 80)\n\n        test_loss, test_acc, pred, true = self.evaluate(test_batches, data)\n\n        print(f\"ğŸ“Š Test Accuracy: {test_acc:.4f}\")\n\n        precision = precision_score(true, pred, average='weighted', zero_division=0)\n        recall = recall_score(true, pred, average='weighted', zero_division=0)\n        f1 = f1_score(true, pred, average='weighted', zero_division=0)\n\n        print(f\"ğŸ“Š Precision: {precision:.4f}\")\n        print(f\"ğŸ“Š Recall: {recall:.4f}\")\n        print(f\"ğŸ“Š F1-Score: {f1:.4f}\")\n\n        cm = confusion_matrix(true, pred)\n        print(\"\\n\" + \"-\" * 80)\n        print(\"ğŸ“‹ Classification Report:\")\n        print(\"-\" * 80)\n        print(classification_report(true, pred, zero_division=0))\n\n        return {\n            'test_loss': test_loss, 'test_acc': test_acc,\n            'precision': precision, 'recall': recall, 'f1': f1,\n            'confusion_matrix': cm, 'predictions': pred, 'true_labels': true\n        }\n\n    def save_model(self, path):\n        torch.save({\n            'model_state_dict': self.model.state_dict(),\n            'best_val_acc': self.best_val_acc,\n            'best_epoch': self.best_epoch,\n            'history': self.history\n        }, path)\n\n    def load_model(self, path):\n        checkpoint = torch.load(path, map_location=self.device)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.best_val_acc = checkpoint['best_val_acc']\n        self.best_epoch = checkpoint['best_epoch']\n        self.history = checkpoint['history']\n\n\n# ============================================================================\n# VISUALIZATION (Same as before)\n# ============================================================================\n\ndef plot_training_history(history, save_path=None):\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n    axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss')\n    axes[0].set_title('Training and Validation Loss')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n\n    axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n    axes[1].plot(history['val_acc'], label='Val Accuracy', linewidth=2)\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].set_title('Training and Validation Accuracy')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.close()\n\n\ndef plot_confusion_matrix(cm, class_names=None, save_path=None):\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.tight_layout()\n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    plt.close()\n\n\n# ============================================================================\n# MAIN - REWRITTEN WITH MINI-BATCH LOADERS\n# ============================================================================\n\ndef main():\n    os.makedirs(MODEL_DIR, exist_ok=True)\n    os.makedirs(RESULTS_DIR, exist_ok=True)\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"ğŸ¤– GNN TRAINING (MEMORY-OPTIMIZED)\")\n    print(\"=\" * 80 + \"\\n\")\n\n    # Load graph\n    print(\"ğŸ“‚ Loading graph data...\")\n    graph_file = f\"graph_{TASK}.pt\"\n    graph_path = os.path.join(GRAPH_DATA_DIR, graph_file)\n    \n    if not os.path.exists(graph_path):\n        print(f\"âŒ ERROR: Graph file not found: {graph_path}\")\n        return\n    \n    data = torch.load(graph_path, weights_only=False)\n\n    print(f\"âœ… Graph: {data.num_nodes:,} nodes, {data.num_edges:,} edges\")\n\n    # Create masks\n    print(\"\\nğŸ“Š Creating data splits...\")\n    num_nodes = data.num_nodes\n    indices = torch.randperm(num_nodes)\n\n    train_size = int(num_nodes * TRAIN_RATIO)\n    val_size = int(num_nodes * VAL_RATIO)\n\n    train_idx = indices[:train_size]\n    val_idx = indices[train_size:train_size + val_size]\n    test_idx = indices[train_size + val_size:]\n\n    print(f\"âœ… Train: {len(train_idx):,} | Val: {len(val_idx):,} | Test: {len(test_idx):,}\")\n\n    # Create simple batched indices (no neighbor sampling)\n    print(f\"\\nğŸ”§ Creating simple batched processing...\")\n    \n    def create_simple_batches(indices, batch_size):\n        \"\"\"Create simple batches without neighbor sampling\"\"\"\n        return [indices[i:i+batch_size] for i in range(0, len(indices), batch_size)]\n    \n    train_batches = create_simple_batches(train_idx, BATCH_SIZE)\n    val_batches = create_simple_batches(val_idx, BATCH_SIZE)\n    test_batches = create_simple_batches(test_idx, BATCH_SIZE)\n    \n    print(f\"âœ… Train batches: {len(train_batches)}\")\n    print(f\"âœ… Val batches: {len(val_batches)}\")\n    print(f\"âœ… Test batches: {len(test_batches)}\")\n\n    # Create model\n    print(f\"\\nğŸ—ï¸  Creating {MODEL_NAME} model...\")\n    num_classes = len(torch.unique(data.y))\n\n    model_kwargs = {'num_layers': NUM_LAYERS, 'dropout': DROPOUT}\n    if MODEL_NAME == 'GAT':\n        model_kwargs['heads'] = HEADS\n\n    model = create_model(\n        MODEL_NAME,\n        in_channels=data.num_features,\n        hidden_channels=HIDDEN_CHANNELS,\n        num_classes=num_classes,\n        **model_kwargs\n    )\n\n    print(f\"âœ… Model: {count_parameters(model):,} parameters\")\n\n    # Train\n    trainer = MiniBatchGNNTrainer(model, DEVICE, task=TASK)\n    optimizer = Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n\n    history = trainer.train(train_batches, val_batches, data, optimizer, scheduler, NUM_EPOCHS, PATIENCE)\n\n    # Test\n    print(\"\\nğŸ”„ Loading best model...\")\n    trainer.load_model(os.path.join(MODEL_DIR, f'best_model_{TASK}.pt'))\n    results = trainer.test(test_batches, data)\n\n    # Save results\n    print(\"\\nğŸ’¾ Saving results...\")\n    plot_training_history(history, os.path.join(RESULTS_DIR, f'training_history_{TASK}.png'))\n    plot_confusion_matrix(results['confusion_matrix'], ['Benign', 'Attack'], \n                          os.path.join(RESULTS_DIR, f'confusion_matrix_{TASK}.png'))\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"ğŸ‰ TRAINING COMPLETED!\")\n    print(\"=\" * 80)\n    print(f\"ğŸ“Š Test Accuracy: {results['test_acc']:.4f}\")\n    print(f\"ğŸ“Š F1-Score: {results['f1']:.4f}\")\n    print(\"=\" * 80 + \"\\n\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q torch-sparse","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
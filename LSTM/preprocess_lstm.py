import pandas as pd
import numpy as np
import os
import glob
import pickle
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import gc

# ============================================================================
# CONFIGURATION (AUTO-DETECT KAGGLE VS LOCAL)
# ============================================================================
IS_KAGGLE = os.path.exists('/kaggle/input')

if IS_KAGGLE:
    print("üåç ENVIRONMENT: KAGGLE DETECTED")
    # T·ª± ƒë·ªông t√¨m dataset CICIDS2018 trong input
    # ∆Øu ti√™n c√°c folder th√¥ng d·ª•ng
    possible_dirs = [
        "/kaggle/input/cicids2018/CICIDS2018_CSV", 
        "/kaggle/input/cse-cic-ids2018",
        "/kaggle/input/cicids2018"
    ]
    RAW_DATA_DIR = None
    for d in possible_dirs:
        if os.path.exists(d):
            RAW_DATA_DIR = d
            break
    
    # Fallback n·∫øu kh√¥ng t√¨m th·∫•y ƒë√∫ng t√™n, l·∫•y folder ƒë·∫ßu ti√™n trong input
    if RAW_DATA_DIR is None:
        try:
            subdirs = glob.glob("/kaggle/input/*")
            if subdirs:
                RAW_DATA_DIR = subdirs[0]
        except:
            pass
            
    if RAW_DATA_DIR is None:
        RAW_DATA_DIR = "/kaggle/input" # Hy v·ªçng user mount ƒë√∫ng
        
    OUTPUT_DIR = "/kaggle/working/processed_lstm"
    TARGET_ROWS = 2000000 # Kaggle RAM m·∫°nh, l·∫•y 2 tri·ªáu d√≤ng
    print(f"   - Raw Data: {RAW_DATA_DIR}")
    print(f"   - Output:   {OUTPUT_DIR}")
    print(f"   - Target:   {TARGET_ROWS:,} rows")

else:
    print("üíª ENVIRONMENT: LOCAL DESKTOP DETECTED")
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    # Local: Data n·∫±m ·ªü th∆∞ m·ª•c cha ../data_IOT
    RAW_DATA_DIR = os.path.join(os.path.dirname(BASE_DIR), "data_IOT")
    OUTPUT_DIR = os.path.join(BASE_DIR, 'processed_lstm')
    TARGET_ROWS = 2000000  # Local RAM h·∫°n ch·∫ø, l·∫•y 500k d√≤ng
    print(f"   - Raw Data: {RAW_DATA_DIR}")
    print(f"   - Output:   {OUTPUT_DIR}")
    print(f"   - Target:   {TARGET_ROWS:,} rows")

# Common Params
WINDOW_SIZE = 10
BALANCE_DATA = True
TEST_SIZE = 0.2
VAL_SIZE = 0.1
RANDOM_STATE = 42

def load_and_clean_raw_data():
    """ƒê·ªçc tr·ª±c ti·∫øp t·ª´ CSV v√† l√†m s·∫°ch s∆° b·ªô"""
    print(f"Checking data dir: {RAW_DATA_DIR}")
    
    # T√¨m t·∫•t c·∫£ c√°c file c√≥ ƒëu√¥i .csv
    all_paths = glob.glob(os.path.join(RAW_DATA_DIR, "**", "*.csv"), recursive=True)
    csv_files = []
    
    for p in all_paths:
        if os.path.isfile(p):
            csv_files.append(p)
    
    if not csv_files:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file CSV th·ª±c s·ª±!")
        exit(1)
        
    print(f"Found {len(csv_files)} actual CSV files.")
    
    dfs = []
    total_rows = 0
    
    # S·∫Øp x·∫øp ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh th·ª© t·ª± th·ªùi gian (quan tr·ªçng cho LSTM)
    csv_files = sorted(csv_files)
    
    pbar = tqdm(csv_files, desc="Loading CSVs")
    for f in pbar:
        if total_rows >= TARGET_ROWS:
            break
            
        try:
            # ƒê·ªçc file th·ª±c s·ª±
            df = pd.read_csv(f, low_memory=False, nrows=200000) 
            
            # Clean Headers l·∫∑p l·∫°i (quan tr·ªçng v√¨ dataset CICIDS2018 hay b·ªã l·ªói n√†y)
            if 'Label' in df.columns:
                # Lo·∫°i b·ªè d√≤ng m√† c·ªôt Label ch·ª©a ch·ªØ 'Label' (header b·ªã l·∫∑p)
                df = df[df['Label'].astype(str).str.lower() != 'label']
            
            # 2. Drop columns kh√¥ng d√πng cho LSTM
            cols_to_drop = ['Timestamp', 'Flow ID', 'Src IP', 'Dst IP', 'Src Port']
            cols_exist = [c for c in cols_to_drop if c in df.columns]
            df.drop(columns=cols_exist, inplace=True)
            
            # 3. Convert Numeric & Optimize Memory
            if 'Label' in df.columns:
                label_col = df['Label']
                df = df.drop(columns=['Label'])
                df = df.apply(pd.to_numeric, errors='coerce')
                df = df.astype(np.float32)
                df['Label'] = label_col
            else:
                df = df.apply(pd.to_numeric, errors='coerce')
                df = df.astype(np.float32)
            
            # 4. Fill NaNs
            df = df.fillna(0)
            
            if len(df) > 0:
                dfs.append(df)
                total_rows += len(df)
                pbar.set_postfix({'rows': f"{total_rows:,}"})
            
        except Exception as e:
            # print(f"Error reading {f}: {e}")
            pass

    print("\nMerging data...")
    if not dfs:
        print("‚ùå Kh√¥ng load ƒë∆∞·ª£c d·ªØ li·ªáu n√†o. H√£y ki·ªÉm tra l·∫°i file CSV b√™n trong data_IOT.")
        exit(1)
        
    full_df = pd.concat(dfs, ignore_index=True)
    
    # Handle Infinity
    print("Handling Infinity values...")
    numeric_cols = full_df.select_dtypes(include=[np.number]).columns
    full_df[numeric_cols] = full_df[numeric_cols].replace([np.inf, -np.inf], 0)
    
    del dfs
    gc.collect()
    
    return full_df

def create_sequences_and_balance(df):
    """
    T·∫°o sequences v√† c√¢n b·∫±ng d·ªØ li·ªáu ·ªû C·∫§P ƒê·ªò SEQUENCE.
    ƒêi·ªÅu n√†y gi·ªØ nguy√™n t√≠nh to√†n v·∫πn c·ªßa sliding window.
    """
    # 1. Prepare Features & Label
    labels = df['Label'].values
    # Encode Label: Benign=0, Attack=1
    y_binary = (labels != 'Benign').astype(int)
    
    # Drop label col for features
    X_data = df.drop(columns=['Label']).values
    feature_names = df.drop(columns=['Label']).columns.tolist()
    
    # 2. Normalize (StandardScaler)
    print("Normalizing features...")
    scaler = StandardScaler()
    X_data = scaler.fit_transform(X_data).astype(np.float32) # Keep float32
    
    # SAVE SCALER & FEATURE NAMES for Inference
    print("Saving scaler and feature names...")
    with open(os.path.join(OUTPUT_DIR, 'scaler.pkl'), 'wb') as f:
        pickle.dump(scaler, f)
    with open(os.path.join(OUTPUT_DIR, 'feature_names.json'), 'w') as f:
        import json
        json.dump(feature_names, f)
    
    print(f"\nCreating Sliding Windows (Size: {WINDOW_SIZE})...")
    # S·ª≠ d·ª•ng logic sliding window
    # X_seq: (N, Window, Features)
    # y_seq: (N,) - Label c·ªßa d√≤ng cu·ªëi c√πng trong window
    
    Xs, ys = [], []
    
    # ƒê·ªÉ tr√°nh loop Python ch·∫≠m, ta d√πng loop ƒë∆°n gi·∫£n nh∆∞ng hi·ªáu qu·∫£
    # N·∫øu dataset qu√° l·ªõn, c√≥ th·ªÉ d√πng stride_tricks (nh∆∞ng c·∫©n th·∫≠n RAM copy)
    
    # Ch·ªâ l·∫•y sequence h·ª£p l·ªá (kh√¥ng b·ªã ng·∫Øt qu√£ng gi·ªØa c√°c file n·∫øu gh√©p)
    # ·ªû ƒë√¢y ta ch·∫•p nh·∫≠n r·ªßi ro nh·ªè ·ªü ƒëi·ªÉm n·ªëi c√°c file ƒë·ªÉ code ƒë∆°n gi·∫£n
    
    # Optimization: Ch·ªâ loop qua index
    total_len = len(X_data) - WINDOW_SIZE
    
    # C·∫£nh b√°o n·∫øu d·ªØ li·ªáu qu√° l·ªõn
    if total_len > 1000000:
        print("Data l·ªõn, qu√° tr√¨nh t·∫°o sequence c√≥ th·ªÉ m·∫•t v√†i ph√∫t...")

    # T·∫°o sequences
    # C√°ch nhanh h∆°n: List comprehension
    indices = np.arange(total_len)
    
    # ƒê·ªÉ ti·∫øt ki·ªám RAM, ta kh√¥ng t·∫°o ngay numpy array kh·ªïng l·ªì
    # Ta s·∫Ω t·∫°o index cho Attack v√† Benign tr∆∞·ªõc
    
    # X√°c ƒë·ªãnh label cho m·ªói window (l√† label c·ªßa d√≤ng cu·ªëi c√πng)
    # y_binary[WINDOW_SIZE:] kh·ªõp v·ªõi index c·ªßa window
    window_labels = y_binary[WINDOW_SIZE:]
    
    attack_indices = np.where(window_labels == 1)[0]
    benign_indices = np.where(window_labels == 0)[0]
    
    print(f"Found {len(attack_indices):,} attack sequences")
    print(f"Found {len(benign_indices):,} benign sequences")
    
    if BALANCE_DATA:
        print("\nBalancing Data (Undersampling Benign Sequences)...")
        # L·∫•y t·∫•t c·∫£ Attack
        # L·∫•y Benign b·∫±ng s·ªë l∆∞·ª£ng Attack (ho·∫∑c t·ªëi ƒëa n·∫øu √≠t h∆°n)
        n_samples = min(len(attack_indices), len(benign_indices))
        
        if n_samples == 0:
            print("‚ö†Ô∏è Warning: Kh√¥ng t√¨m th·∫•y Attack n√†o! S·∫Ω d√πng to√†n b·ªô data.")
            final_indices = indices
        else:
            # Random ch·ªçn Benign sequences
            np.random.seed(RANDOM_STATE)
            chosen_benign = np.random.choice(benign_indices, n_samples, replace=False)
            
            # G·ªôp l·∫°i
            final_indices = np.concatenate([attack_indices, chosen_benign])
            # Shuffle sequence order (Quan tr·ªçng: Shuffle C√ÅC SEQUENCE, kh√¥ng ph·∫£i shuffle b√™n trong sequence)
            np.random.shuffle(final_indices)
            
            print(f"‚úì Balanced Total Sequences: {len(final_indices):,} (50% Attack / 50% Benign)")
    else:
        final_indices = indices
        
    # B√¢y gi·ªù m·ªõi build m·∫£ng X th·∫≠t s·ª± d·ª±a tr√™n index ƒë√£ l·ªçc
    # ƒêi·ªÅu n√†y ti·∫øt ki·ªám c·ª±c nhi·ªÅu RAM so v·ªõi t·∫°o h·∫øt r·ªìi m·ªõi l·ªçc
    print("Building final sequence array...")
    
    X_final = []
    y_final = []
    
    for idx in tqdm(final_indices, desc="Constructing 3D Array"):
        # idx l√† v·ªã tr√≠ b·∫Øt ƒë·∫ßu c·ªßa window (trong logic c≈© l√† 0..len-window)
        # window: X[idx : idx + WINDOW]
        # label: y[idx + WINDOW] -> ch√≠nh l√† window_labels[idx]
        
        # S·ª≠a l·∫°i logic index m·ªôt ch√∫t cho kh·ªõp window_labels
        # window_labels[i] ·ª©ng v·ªõi window k·∫øt th√∫c t·∫°i i+WINDOW
        # start index = i
        
        X_final.append(X_data[idx : idx + WINDOW_SIZE])
        y_final.append(window_labels[idx])
        
    return np.array(X_final, dtype=np.float32), np.array(y_final, dtype=np.float32)

def main():
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        
    print("üöÄ STARTED LSTM INDEPENDENT PREPROCESSING")
    
    # 1. Load Raw
    df = load_and_clean_raw_data()
    print(f"Raw Data Loaded: {df.shape}")
    
    # 2. Create Sequences & Balance
    X_seq, y_seq = create_sequences_and_balance(df)
    
    print(f"\nFinal Shape:")
    print(f"X: {X_seq.shape}")
    print(f"y: {y_seq.shape}")
    
    # 3. Split
    print("Splitting Train/Val/Test...")
    X_train, X_test, y_train, y_test = train_test_split(
        X_seq, y_seq, test_size=TEST_SIZE, shuffle=False # Gi·ªØ th·ª© t·ª± th·ªùi gian c·ªßa c√°c sequence ƒë√£ shuffle (ho·∫∑c kh√¥ng)
    )
    # L∆∞u √Ω: ·ªû b∆∞·ªõc Balance tr√™n ta ƒë√£ shuffle th·ª© t·ª± c√°c sequence. 
    # V·ªõi LSTM Anomaly detection, th∆∞·ªùng ta split theo th·ªùi gian TR∆Ø·ªöC khi balance.
    # Nh∆∞ng v√¨ ƒë√¢y l√† b√†i to√°n Classification (Attack vs Benign) d·ª±a tr√™n window,
    # vi·ªác shuffle c√°c window l√† ch·∫•p nh·∫≠n ƒë∆∞·ª£c.
    
    X_train, X_val, y_train, y_val = train_test_split(
        X_train, y_train, test_size=VAL_SIZE, shuffle=False
    )
    
    # 4. Save
    print(f"Saving to {OUTPUT_DIR}...")
    np.save(os.path.join(OUTPUT_DIR, 'X_train.npy'), X_train)
    np.save(os.path.join(OUTPUT_DIR, 'X_val.npy'), X_val)
    np.save(os.path.join(OUTPUT_DIR, 'X_test.npy'), X_test)
    np.save(os.path.join(OUTPUT_DIR, 'y_train.npy'), y_train)
    np.save(os.path.join(OUTPUT_DIR, 'y_val.npy'), y_val)
    np.save(os.path.join(OUTPUT_DIR, 'y_test.npy'), y_test)
    
    print("\n‚úÖ DONE! Ready for training.")

if __name__ == "__main__":
    main()